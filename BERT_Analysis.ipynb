{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ee16350569a47b08cf81b384dd88279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fb883f3d0280437d9ae480ef22144207",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d345ea71540946368901fb423beb95c7",
              "IPY_MODEL_11989d1e4cab49148e901a67a1da4f5f"
            ]
          }
        },
        "fb883f3d0280437d9ae480ef22144207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d345ea71540946368901fb423beb95c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8fa39f52445447eb80ae07aeb7ac0495",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61f7acdc1cbf4840b1f45da3bc7c3b82"
          }
        },
        "11989d1e4cab49148e901a67a1da4f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7aa290f3ba254038950e44209799884f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 10.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a17ed805a3b74ff5a13ad928678d76f6"
          }
        },
        "8fa39f52445447eb80ae07aeb7ac0495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61f7acdc1cbf4840b1f45da3bc7c3b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7aa290f3ba254038950e44209799884f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a17ed805a3b74ff5a13ad928678d76f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "375c2bfd19c648ceb2ab69f75a7e72bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1b5b7c13d05844959aec52fba51b8586",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_477c12db3d2a44528e0401ee12266a69",
              "IPY_MODEL_f6bacae83315436c9ad1879b389b5d51"
            ]
          }
        },
        "1b5b7c13d05844959aec52fba51b8586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "477c12db3d2a44528e0401ee12266a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db6b1e77ac5a4d21a7787d347dac3892",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70d04d81526647959009c7ad9df5da8a"
          }
        },
        "f6bacae83315436c9ad1879b389b5d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9782b23db0974ced8a7c57b480d0b843",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:11&lt;00:00, 39.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e41afc3802b45d08be98f06c7c607d6"
          }
        },
        "db6b1e77ac5a4d21a7787d347dac3892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70d04d81526647959009c7ad9df5da8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9782b23db0974ced8a7c57b480d0b843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e41afc3802b45d08be98f06c7c607d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachel-ker/refugeediscourse/blob/master/BERT_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFkME77f1Zh",
        "colab_type": "text"
      },
      "source": [
        "## Final Project - Deep Neural Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTCP80HqWCZs",
        "colab_type": "code",
        "outputId": "e36e7294-a2b1-4ced-fc0d-d0d32e0d549d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkVY2HIWRPC",
        "colab_type": "code",
        "outputId": "a9f6beed-65e0-4219-8973-40ac951c99ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzt0EKCqWVsN",
        "colab_type": "code",
        "outputId": "440f81d7-e6c3-4e89-a938-e82b96c6fa9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwp7W3BWWERu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d1e58b6-6818-4899-a46f-041847dbd64c"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onaOGdjxgiyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"./refugee_coca_foranalysis.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNr_QIjt-GCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#5-year periods\n",
        "def get_period(year, startyr, endyr, n=5):\n",
        "    period_start = []\n",
        "    for i in range(startyr, endyr+1, n):\n",
        "        period_start.append(i)\n",
        "    for index, p in enumerate(period_start):\n",
        "        if year >= p:\n",
        "            period = index\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    return period  \n",
        "    \n",
        "data['period'] = data['year'].apply(lambda x: get_period(x, 1991, 2015, n=5) if x>=1991 else 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhwKZFME9dkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# Add label\n",
        "unique_sources = list(data.source.apply(lambda x: re.split(\"_|: | \", x)[0]).unique())\n",
        "len(unique_sources)\n",
        "\n",
        "# Write a function that let us conveniently label ideology: Left, Neutral, Right\n",
        "media_ideology = {}\n",
        "\n",
        "def label_ideology(media_title, ideology, media_ideology = media_ideology):\n",
        "    media_ideology[media_title] = ideology\n",
        "    \n",
        "label_ideology('Money', 'Neutral')\n",
        "label_ideology('MotherEarth', 'Neutral')\n",
        "label_ideology('MotherJones', 'Left')\n",
        "label_ideology('AmHeritage', 'Neutral')\n",
        "label_ideology('AmSpect', 'Right')\n",
        "label_ideology('Forbes', 'Right')\n",
        "label_ideology('NatlReview', 'Right')\n",
        "label_ideology('Newsweek', 'Left')\n",
        "label_ideology('ScienceNews', 'Neutral')\n",
        "label_ideology('Smithsonian', 'Neutral')\n",
        "label_ideology('USNWR', 'Left')\n",
        "label_ideology('WashMonth', 'Left')\n",
        "label_ideology('ChangingTimes', 'Right')\n",
        "label_ideology('HistoryToday', 'Neutral')\n",
        "label_ideology('Omni', 'Neutral')\n",
        "label_ideology('Wilderness', 'Neutral')\n",
        "label_ideology('TIME', 'Left')\n",
        "label_ideology('NatlParks', 'Neutral')\n",
        "label_ideology('AmerArtist', 'Neutral')\n",
        "label_ideology('RollingStone', 'Left')\n",
        "label_ideology('Americas', 'Neutral')\n",
        "label_ideology('SportsIll', 'Neutral')\n",
        "label_ideology('Ms', 'Left')\n",
        "label_ideology('PopScience', 'Neutral')\n",
        "label_ideology('Futurist', 'Neutral')\n",
        "label_ideology('HarpersMag', 'Left')\n",
        "label_ideology('Fortune', 'Right')\n",
        "label_ideology('USAToday', 'Left')\n",
        "label_ideology('America', 'Left')\n",
        "label_ideology('ChristCentury', 'Right')\n",
        "label_ideology('People', 'Left')\n",
        "label_ideology('Jet', 'Left')\n",
        "label_ideology('Aging', 'Neutral')\n",
        "label_ideology('Horticulture', 'Neutral')\n",
        "label_ideology('NewRepublic', 'Left')\n",
        "label_ideology('Conservation', 'Left')\n",
        "label_ideology('NaturalHist', 'Neutral')\n",
        "label_ideology('Atlantic', 'Left')\n",
        "label_ideology('Inc.', 'Neutral')\n",
        "label_ideology('ChildrenToday', 'Neutral')\n",
        "label_ideology('Ebony', 'Left')\n",
        "label_ideology('ConsumResrch', 'Neutral')\n",
        "label_ideology('SatEvenPost', 'Neutral')\n",
        "label_ideology('ChristToday', 'Right')\n",
        "label_ideology('Backpacker', 'Neutral')\n",
        "label_ideology('AmericanCraft', 'Neutral')\n",
        "label_ideology('ArtAmerica', 'Neutral')\n",
        "label_ideology('SportingNews', 'Neutral')\n",
        "label_ideology('MensHealth', 'Neutral')\n",
        "label_ideology('Antiques', 'Neutral')\n",
        "label_ideology('Parenting', 'Neutral')\n",
        "label_ideology('Essence', 'Neutral')\n",
        "label_ideology('Environmental', 'Neutral')\n",
        "label_ideology('USCatholic', 'Right')\n",
        "label_ideology('MilitaryHist', 'Neutral')\n",
        "label_ideology('PsychToday', 'Neutral')\n",
        "label_ideology('Cosmopolitan', 'Left')\n",
        "label_ideology('Redbook', 'Neutral')\n",
        "label_ideology('Bazaar', 'Left')\n",
        "label_ideology('ChildDigest', 'Neutral')\n",
        "label_ideology('Bicycling', 'Neutral')\n",
        "label_ideology('Shape', 'Neutral')\n",
        "label_ideology('NatGeog', 'Neutral')\n",
        "label_ideology('Entertainment', 'Neutral')\n",
        "label_ideology('Astronomy', 'Neutral')\n",
        "label_ideology('TownCountry', 'Neutral')\n",
        "label_ideology('TotalHealth', 'Neutral')\n",
        "label_ideology('Esquire', 'Left')\n",
        "label_ideology('FieldStream', 'Neutral')\n",
        "label_ideology('TechReview', 'Neutral')\n",
        "label_ideology('CountryLiving', 'Neutral')\n",
        "label_ideology('VegTimes', 'Neutral')\n",
        "label_ideology('SouthernLiv', 'Neutral')\n",
        "label_ideology('Skiing', 'Neutral')\n",
        "label_ideology('ConsumRep', 'Neutral')\n",
        "label_ideology('Sunset', 'Neutral')\n",
        "label_ideology('HarpersBazaar', 'Neutral')\n",
        "label_ideology('AmericanSpectator', 'Right')\n",
        "label_ideology('GoodHousekeeping', 'Neutral')\n",
        "label_ideology('PopMech', 'Neutral')\n",
        "label_ideology('MHQTheQuarterly', 'Neutral')\n",
        "label_ideology('TodaysParent', 'Neutral')\n",
        "label_ideology('NationalGeographic', 'Neutral')\n",
        "label_ideology('EEnvironmental', 'Neutral')\n",
        "label_ideology('ParentingEarly', 'Neutral')\n",
        "label_ideology('ABC', 'Left')\n",
        "label_ideology('CNN', 'Left')\n",
        "label_ideology('PBS', 'Left')\n",
        "label_ideology('CBS', 'Left')\n",
        "label_ideology('Ind', 'Left')\n",
        "label_ideology('NPR', 'Left')\n",
        "label_ideology('NBC', 'Left')\n",
        "label_ideology('Fox', 'Right')\n",
        "label_ideology('MSNBC', 'Left')\n",
        "label_ideology('NYTimes', 'Left')\n",
        "label_ideology('CSMonitor', 'Neutral')\n",
        "label_ideology('AssocPress', 'Neutral')\n",
        "label_ideology('WashPost', 'Left')\n",
        "label_ideology('SanFranChron', 'Left')\n",
        "label_ideology('Atlanta', 'Left') #Atlanta Journal Constitution\n",
        "label_ideology('Houston', 'Left') #Houston Chronicle\n",
        "label_ideology('Chicago', 'Left') #Chicago Sun-Times\n",
        "label_ideology('Denver', 'Left') #Denver Post\n",
        "label_ideology('GolfMag', 'Neutral')\n",
        "label_ideology('NewStatesman', 'Left')\n",
        "label_ideology('Austin', 'Left') #Austin American Statesman\n",
        "label_ideology('STLouis', 'Left') #St Louis Post_Dispatch\n",
        "label_ideology('Pittsburgh', 'Right') #Pittsburgh Post-Gazette\n",
        "label_ideology('OrangeCR', 'Right') #Orange County Register\n",
        "\n",
        "#add political leaning label\n",
        "def add_ideology(x, media_ideology=media_ideology):\n",
        "    try_split = re.split(\"_|: | \",x)\n",
        "    if len(try_split)>1:\n",
        "        x = try_split[0]\n",
        "    if x in media_ideology:\n",
        "        return media_ideology[x]\n",
        "    else:\n",
        "        print('{} does not exists'.format(x))\n",
        "\n",
        "data[\"ideology\"] = data['source'].apply(add_ideology)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT_bV6Nm9hhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[data['ideology']!='Neutral']\n",
        "data['label'] = data['ideology'].mask(data['ideology']=='Right',1)\n",
        "data['label'] = data['label'].mask(data['ideology']=='Left',0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjoz66ZUTmcK",
        "colab_type": "code",
        "outputId": "5fa10517-039e-45dc-e5e1-5339839230a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "data.groupby(['ideology','label']).size()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ideology  label\n",
              "Left      0        2728\n",
              "Right     1         271\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nE4RKCo9lCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0ace04de-fb1b-4002-df73-bc8d1223d19c"
      },
      "source": [
        "data = data[['text','label','period']]\n",
        "data.rename(columns={'text':'sentence'}, inplace=True)\n",
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>section : movements from socialist to republi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>inside a dusty cement-block house with worn ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>even if saddam hussein fails to get his way ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>section : spectator 's journal  dateline : li...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>soviet jewry is giving israel new life -- bu...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence label  period\n",
              "3   section : movements from socialist to republi...     0       0\n",
              "4    inside a dusty cement-block house with worn ...     0       0\n",
              "6    even if saddam hussein fails to get his way ...     1       0\n",
              "7   section : spectator 's journal  dateline : li...     1       0\n",
              "8    soviet jewry is giving israel new life -- bu...     1       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOAd1sJU90uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = data[data.period==4]\n",
        "df_train = data[data.period!=4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUhJf_oPDCNV",
        "colab_type": "code",
        "outputId": "9a4d08fd-c129-45ec-8c79-cc6ef817aea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df_train.sample(10)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3001</th>\n",
              "      <td>a snicket ticket could have been a people pl...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1587</th>\n",
              "      <td>host : oprah winfrey !executive-producer : di...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1866</th>\n",
              "      <td>!robert-siegel-hos : this is all things consi...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>11:00 am-12:00 noon , many afghans living in ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3219</th>\n",
              "      <td>what is the origin of road movies ? a year a...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>the company has been walloped by terror and r...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>colleen mcedwards , world news ( voice-over )...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2732</th>\n",
              "      <td>istanbul , turkey - terrified earthquake sur...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1260</th>\n",
              "      <td>peter jennings , abc news : voice-over in los...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>in july , lebanon 's leadership finally forme...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "3001    a snicket ticket could have been a people pl...     0       2\n",
              "1587   host : oprah winfrey !executive-producer : di...     0       2\n",
              "1866   !robert-siegel-hos : this is all things consi...     0       3\n",
              "1532   11:00 am-12:00 noon , many afghans living in ...     0       2\n",
              "3219    what is the origin of road movies ? a year a...     0       3\n",
              "453    the company has been walloped by terror and r...     1       2\n",
              "1490   colleen mcedwards , world news ( voice-over )...     0       1\n",
              "2732    istanbul , turkey - terrified earthquake sur...     0       1\n",
              "1260   peter jennings , abc news : voice-over in los...     0       1\n",
              "670    in july , lebanon 's leadership finally forme...     0       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HXdqkq2grt",
        "colab_type": "code",
        "outputId": "9e541aab-872f-442c-980b-9251b20f7a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df_test.sample(10)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3914</th>\n",
              "      <td>!george-stephanopou# (off-camera) let 's brin...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3786</th>\n",
              "      <td>ethical failure # basic ethics should n't hav...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>he 's not here in \" welcome to the cafeteria ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3575</th>\n",
              "      <td>: views what are we to make of the horrific e...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3984</th>\n",
              "      <td>!gwen-ifill# president obama said today gun c...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3697</th>\n",
              "      <td>!chuck-todd , -nbc-ne# host : this sunday on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>!jeffrey-brown# police and protesters clashed...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3434</th>\n",
              "      <td>seoul , south korea # in august 1996 , the ve...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3401</th>\n",
              "      <td>on tuesday in a ceremony in rome , the united...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3882</th>\n",
              "      <td>!donald-trump-(r) , -# we will have so much w...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "3914   !george-stephanopou# (off-camera) let 's brin...     0       4\n",
              "3786   ethical failure # basic ethics should n't hav...     1       4\n",
              "739    he 's not here in \" welcome to the cafeteria ...     0       4\n",
              "3575   : views what are we to make of the horrific e...     1       4\n",
              "3984   !gwen-ifill# president obama said today gun c...     0       4\n",
              "3697   !chuck-todd , -nbc-ne# host : this sunday on ...     0       4\n",
              "3979   !jeffrey-brown# police and protesters clashed...     0       4\n",
              "3434   seoul , south korea # in august 1996 , the ve...     0       4\n",
              "3401   on tuesday in a ceremony in rome , the united...     0       4\n",
              "3882   !donald-trump-(r) , -# we will have so much w...     1       4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYMBQJB5K0Vl",
        "colab_type": "text"
      },
      "source": [
        "### LSTM (First 4 periods predicting the last period)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saNk183jgjVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df_train.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df_train.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoH-oU6iOEGd",
        "colab_type": "code",
        "outputId": "8c4364d1-0113-4413-d62d-c0c23742d081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "sentences[:5]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]  section : movements from socialist to republicans , a generation of student activists test the limits of volunteerism .  university of houston student lloyd jacobson was organizing for a sleep-in outside a local homeless shelter and invited the head of the campus college republicans to join in . \" do n\\'t you want to be part of bush \\'s thousand points of light ? \" jacobson asked , and quickly got the student to agree . others in the republican group hesitated , but reluctantly came along . jacobson next enlisted the college democrats : \" they \\'re participating . are n\\'t you going to as well ? \"  the houston students initially regarded homeless people as faceless and disposable . but they were unnerved by what they witnessed while trying to sleep on cardboard sheets on the hard , cold ground . huddled together , they gingerly began talking with shelter residents . the young republican coordinator returned to write an editorial for the campus daily paper , in which he said he was amazed to find the homeless had comprehensible thoughts , and that           relying on \" worthless government bureaucrats , \" something clearly needed to be done . \" you can not possibly know the situation , \" he concluded , \" until you open your eyes . \"  everywhere , formerly apolitical students have begun to stream into neighborhood soup kitchens , shelters , and tutoring campaigns . at some campuses , volunteer service programs have grown tenfold in the past few years , placing as many as a thousand students a week in community projects .  it \\'s easy to dismiss these students as gushy candy stripers , or to chide them for not re-creating the acts of those who rebelled twenty , years ago at berkeley or columbia . but by breaking students out of insulated worlds , setting no bars on who may participate , and demanding that individuals take seriously the lives of those they hope to assist , the community-service movement has the potential to create the broadest base for campus activism in years . if the past is a guide , these seemingly innocuous charitable involvements may even produce later commitments that could shift the           service movements began by making an end run around prevailing barriers to political involvement . most students mistrusted campus activists , believed solutions to society \\'s major problems were individual , not structural , and felt they could have little influence on key public issues . service participants instead performed immediately useful tasks that did not force them to challenge entrenched institutional authority . these efforts still remained a marginal campus presence in 1984 , when a group of recent college graduates launched the campus outreach opportunity league ( cool ) as a national vehicle to promote student community involvement . angered by media stereotypes that called his generation uncaring , cool cofounder wayne meisel undertook a fifteen-hundred-mile walk from maine to washington , d.c. , where he stopped at campus after campus to spread the message of commitment . cool grew , by 1989 , to a six-hundred-school network .  when i first talked to cool \\'s current executive director , twenty-six-year-old julia scatliff , she said the movement \\'s task was \" to help the person lying bleeding in the street , to put our hands over the           larger issues , scatliff said , up to the nation \\'s leaders , who would respond when they heard sufficiently eloquent testaments of need .  scatliff recalled how author and homeless-rights activist jonathan kozol had praised the students cool worked with by equating them with the founding generation of vietnam-era sds activists , and how she \\'d bridled at the comparison . \" we \\'re different , \" she said . when l asked how , she answered , smiling , \" we have macintoshes and modems , \" then added , \" we live our beliefs . if we say something , we back it up . if we talk about housing , we get involved with housing . \" scatliff stressed cool \\'s authentic commitment , as opposed to the radical posturing that she associated both with her 1960s predecessors and today \\'s campus politicos .  commitment and authenticity , however , do not always create social change . at a recent conference , one stanford student explained how he \\'d learned more from volunteering than from all his courses in school , then concluded , \"           have the same experience working in the same homeless shelter that i did . \"  from its beginning , movement participants have alternated between a fear of taking on explicitly political challenges , and a sense that they can not allow the conditions they address to continue . the resulting tension expresses itself in a variety of ways . one group of dartmouth students volunteered eagerly in the soup kitchens , but wanted nothing to do with an oxfam fast for hunger , considering it \" too ideological . \" a young woman at a small north carolina college began an innovative mentor program for pregnant teens , yet feared that if she worked to shape larger policy , she would get burned out . \" i do community service for myself , \" she concluded , \" because i have a passion for it . i ca n\\'t save the world . \"  this vision is limited not only by the students \\' own wariness , but by praise from the nation \\'s dominant elite . the movement can seem a neat fit with the bush administration \\'s           volunteer action award , and a congratulatory telegram from barbara bush . following lengthy internal debate , cool also accepted a 1988 grant from coors , whose owners are major funders of the new right agenda .  yet distinctions between political and service efforts continue to blur . in march 1988 , twelve university of minnesota students drove down to brownsville , texas , to spend a week as participants in one of the alternative spring breaks that have recently proliferated across the country . they visited the gulf coast beaches , but also tutored kids , rebuilt an old woman \\'s burned house , and stopped at a shantytown . later , they met three teenage refugees from honduras , the last surviving males from their village , who described seeing friends killed by the military or ground down by poverty . the students concluded their week with a visit to the nearby sanctuary center , casa oscar romero . when the refugees brought out guitars , the students led off with the only songs they could think of , the themes from gilligan \\'s island and the brady           with loss , offered their own ballads , about homes left behind and their hopes for justice .  a few days after the students arrived back home , minneapolis residents demonstrated in opposition to governor rudy perpich \\'s decision to send the minnesota national guard to central america . nearly all the students who had visited brownsville were there .  at a cool-sponsored leadership summit this past summer , some students wrote a series of questions for the ad hoc newsletter : \" is community service an end itself , \" they asked , \" or is it a means for social change ? \" \" how do we reconcile the tension between those who do n\\'t want to be forced into politics \\' ... and those who believe that community service without commitments to long-term structural change does more harm than good ? \" \" given our diverse backgrounds and motives for doing what we are doing , what is it that brings us all together ? \"  they are questions that go to the heart of the student service movement \\'s growing pains . during cool           \\'d worked in surrounding bronx neighborhoods sparked bitter debate when they insisted that service activists must begin taking political stands . they influenced the organization to later endorse its first national rally , the october 1989 washington , d.c. , march against hunger and homelessness . at a recent cool gathering , alex byrd , a black junior at rice , said he was tired of all the brochures showing middle-class white kids cradling little black children in their arms , and initiated discussions on race .  responding to these concerns , cool recently presented workshops where black students suggested that white volunteers might have to do things they \\'ve never done : \" approach black colleges , the alpha fraternities , or the urban league . relinquish power and work on someone else \\'s project where you \\'re totally outnumbered . put yourself in the situation a lot of minorities put themselves in every day . \"  by 1989 , cool director julia scatliff explained how her involvement with a racially and culturally diverse group of movement participants had given her a new circle of friends and introduced           a lot in the year since our initial conversation and now stressed her discomfort with the image of \" a bunch of florence nightingales running out to bandage people \\'s wounds . \"  historically , involvement in service efforts has laid the foundation for some of the country \\'s most significant social movements . at the turn of the century , key future new dealers -- like secretary of labor frances perkins , works project administration head harry hopkins , and first lady eleanor roosevelt -- first encountered political issues in their youth , while working in the settlement houses that sought to assist new immigrants . similarly , in the middle 1960s , the young organizers of the sds economic research and action project ( erap ) lived and worked in the ghettos of northern cities , striving to build \" an interracial movement of the poor \" and find what former organizer sharon jeffrey described as \" a meaning of life \" that would be \" personally authentic . \"  if today \\'s student volunteers have a distinct vision , it is that social change requires broad-based           , in the words of one cool staffer , \" you do n\\'t have to believe any eight specific things to be part of the club . \" the volunteers are proud that their centers draw in everyone from republicans to socialists .  many of these students have only begun to deal with critical questions of power , conflict , and privilege . some still stake too much on influencing the future elite , and thus downplay any hint of radical dissent . they still need to learn how to organize , how to pressure entrenched bureaucratic institutions , and how to articulate a shared political vision . yet the rest of us would do well to treat their movement as a common challenge , and to take the service movements \\' goal of inclusion seriously .  some students may treat their experiences with the poor as little more than exotic tourism . but , over time , many student participants have come to acknowledge the necessity for political involvements that would have terrified them before . as the service movement gains strength , members can heed the lessons           build a society where such entities will not be needed .  illustration  by paul rogat loeb   paul rogat loeb is the author of nuclear culture ( new society publishers ) and hope in hard times ( lexington books ) . he is writing a book on the political sensibility of today \\'s college students .   [SEP]',\n",
              " '[CLS]   inside a dusty cement-block house with worn linoleum floors , miqueas mishari sat at a long , wooden conference table , his broad , bronzed face distended , drawn-down cheeks pinched into a frown . patting the pocket of a pressed , white , short-sleeved shirt , he seemed oblivious to the bustle oil , organizers and clerks working in the head . quarters of peru \\'s amazon indians . they hurriedly photocopied and passed around news stories-under headlines like the indian war -- that were worrying mishari so . \" es compleja , \" he murmured , staring at the wall . it \\'s complicated reflexive peruvian saying , commonly delivered with a shrug , was about to become the mantra of our journey together into the amazon basin . mishari was divining the odds , trying to figure what the latest news from the jungle would mean for trip . \" this could be tricky , this could be difficult . everybody \\'s going to be on edge . the trip little dangerous . \" which was not what he had had in mind .           way a year ago into the presidency of aidesep ( the inter-ethnic association for the development of the peruvian jungle ) , had expected to play guide for a nuanced story about the amazon , a slice deeper than the usual grim surface trees falling , cultures dying , gold mines bustling . against a backdrop of environmental destruction , mishari wanted to show off his organization \\'s remarkable accomplishments . even in the midst of widespread economic collapse , aidesep is using $1.2 million in aid from the danish government to identify , mark , and gain legal title to thousands of square miles of traditional indian land throughout the amazon basin . indians there are protecting the forest by defending their own rights , he says . mishari hoped a story about the project might educate outsiders about the lives of an estimated 1.2 million indians who live in the amazon , a region widely misunderstood so far only as the \" lungs of the world , \" home to rare animals and exotic plant life . a mass , armed rebellion by mishari \\'s own tribe , the           track , derailing us into the details of a nasty guerrilla war .  trouble had begun last december , when guerrillas kidnapped , and then killed , the ashaninkas \\' grand chief . even during an insurgency that has cost eighteen thousand lives in the last decade , the ashaninka response was so unusual that it drew national fascination . thousands of ashaninka warriors gathered up bows and arrows , &dared; war on the guerrillas , and rousted them from a large section of indian territory . among the ashaninka , including mishari \\'s coworkers , stories with the aura of mythology had already sprung up to explain how indians with bows and hunting rifles had vanquished soldiers with ak-47s . tales of witchcraft , including whispered asides about the use of traditional hallucinogenic herbs against the guerrillas , spiced the accounts of men who claimed to be eyewitnesses .  the sudden antiguerilla offensive initially came as a pleasant surprise to the peruvian army , which has been steadily losing ground . but what came next welcome news : in parts of ashaninka territory , the indians had not           they had gone on it chase out mixed-blood mestizo settlers as well . \" we never thought we would take up arms to kill , but the civilized ones have taught us this savagery , \" the tribe \\'s new leader , or pinkatsiri , announced . \" we have returned to a savage state to defend what \\'s our . \" and then , the ashaninkas took over the city of puerto bermudez , jailed the district commissioner , and began shooting some prisoners to death with their bows and arrows . this journey was getting even more compleja .  from lima , a gray and fetid city where the skies hardly ever open for the rain , the specter , of long-suffering natives , suddenly rising up in frightening bloody fury against all outsiders , seemed fantastical , something from a novel by mario vargas llosa . ( in fact , stories about the ashaninka rebellion crowded vargas llosa , the presidential candidate , off the front pages for several days running . ) the news seemed sure to add thousands more peasants and mestizo settlers to the           peruvian countryside for lima , the capital already crammed to bursting with nearly 30 percent of the country \\'s twenty million people . what the immigrants find here , of course , is a society no longer disintegrating ; it can fairly be said to have already disintegrated .  in taxicabs , cafes , even in government offices , two questions -- from people across the political spectrum-emerged in any conversation : is there a job for someone like me in the united states ? where can i send my kids to get them away from the bloodbath sure to come ? at the ministry of agriculture , one official interrupted her discourse on the law , the environment , and indigenous rights to say she could not bear to leave peru herself , but wanted to send her fifteen-year-old daughter to the united states . would i be willing to take the teenager in ?  even miqueas mishari , so committed to his work as an indian leader , sometimes fell victim to the prevailing desperation . after living in a small made-over garage in lima with his           mishari recently sent his son to school in mexico , and he was thinking about taking a leave from aidesep to look for work in the united states . his information , like that of many other peruvian immigrants , about opportunities in the united states was a bit jumbled he \\'d heard there was good money to be made planting pineapples in chicago .  a few minutes \\' drive from mishari \\'s one-room home , in miraflores , lima \\'s most upscale shopping district , well-dressed young men elbowed one an . other out of the way to trade peruvian intis for dollars , struggling to earn enough to buy a ticket out of the country . this widespread money changing is an informal laundry for the estimated $2 billion in peruvian coca exports each year-an estimated 70 percent of the u.s. supply , and the one remaining strut in the peruvian economy .  henry mancini was in mid-serenade as dinner-time arrived . a group of indigenous women , some in traditional clothes hawkea batman posters , mickey mouse sunglasses , and records , including an album of           a benefit for the people of kampuchea . couples weaved through a maze of restaurants the haiti , don corleone , super parking , and super parking ii . beyond a line of those waiting to see honey , i shrunk the kids , mementos of the amazon were for sale , stretched across the sidewalk in see-through boxes . giant black tarantulas , mahogany letter openers , oversized brown flying beetles , stuffed baby alligators , preserved piranhas with mouths agape , and a display of butterflies , turquoise , magenta , and golden yellow , glinting in the reflection of the neon movie marquee-nature transformed into exotic , tamed trinkets under glass .  \" we \\'re trying to make the world understand , and we can only hope we \\'re in time . all you have to do is see what \\'s happening in the cities and frontier towns-and in the parts of the amazon where indigenous rights have not been respected to see what the tremendous stakes are . but how many can see this yet ? \" haroldo salazar , garrulous and open faced , hoists           calluses of his hands together . he \\'s just left off shellacking mishari with frank criticism the project is behind schedule , why have n\\'t organizers been paid for the past two months , where are the supplies , why has n\\'t mishari been around to help ? but now his face softens as we walk across a narrow , suspended wooden footbridge on our way to salazar \\'s pride twenty acres etched into the jungle a few miles outside the wild frontier town of pucallpa .  these acres make up a demonstration project-sponsored by aidesep , run on a shoestring , and staffed by volunteers from indigenous organizations-that is an ecologist \\'s dream . the curriculum has been taught by salazar , a taut ashaninka farmer , for the past few years , and it includes organic methods ( no use of chemical fertilizers or pesticides ) , an aggressive program of reforestation , and reinforcement for the use of traditional crops and medicines . each year , six indian couples from throughout peru spend a few months meeting together to discuss these issues and to help work the           and what we had , \" explains salazar .  mishari follows behind salazar , relishing the contrast between the rich , composted soil on the project side of a drooping barbed-wire fence , and the adjacent stunted landscape belonging to a large landowner , where five mangy cows graze on scattered patches of grass . \" this border tells the story of our battle , \" mishari says . \" on our side , the land supports us and we care for the land . on the other side , a large patron saps the land of its future , all for the sake of a few ugly cows like those , and then moves on to clear more trees somewhere else . \"  salazar leads us to a large gazebo where colorful wall , sized posters of indian families subjected to a barrage of outside influences are hung across the enclosure . the paintings depict the terrors of communal lands broken up , cultural ties fractured , dependence on the outside world setting in . \" we teach what a lie it is that/surrounded by all the resources           tin , \" salazar says . \" we start needing to pay for things we used to grow , and in place of the things we had -- pure air and clean water-we get a jungle destroyed . \"  at this point , salazar is speaking so quickly , in the style of a testimonial , that it \\'s difficult to/keep up , but it \\'s easy to see why he \\'s so agitated . just as the biological diversity of the amazon environment can not be sustained in tattered patches , so too are the prospects for survival of the indigenous world threatened by fight physical constraints on indian communities . in orbits around us , large-scale timber development , cattle ranching , coca growing , and mining are destroying the amazon basin -- in peru alone , at the of 700,000 acres rate each year .  just an hour by dugout canoe from aidesep \\'s demonstration project , the village of san francisco is surrounded by colonists and a factory , one thousand people squeezing into just a few acres per person , far too little to           francisco know their fates probably lie in joining the great tide of immigration , first to frontier towns , then on to lima , and perhaps even out of the country . \" we are at the end of the line . there is no room for the children to grow up and stay here , \" complains the village chief .  gaining communal titles to larger indian communities would slow massive migration to the cities , where widespread unemployment , prostitution , and drug abuse await peru \\'s next generation of displaced indians . titling larger communities may even be the key to survival of the indigenous world , according to salazar . bouncing along a rutted dirt road near the demonstration project , in the back of a pickup , he is bearing down on this subject . \" i grew listening to the stories of my mother , \" salazar says , staring straight ahead now , as if in a trance . \" she used to tell me my tribe \\'s ritual myths . about the land when it was pure ashaninka . i remember her           e were . how jaguars roamed peacefully through the territory . how no invaders came or , when they came , they were sent away . so growing up i heard these stories . when i entered the world , it had changed , of course . that world was gone . lots of invaders . no more territory that was pure ashaninka , and not much calm . no jaguars .  \" but still , for me those things are real . what keeps them alive ? those stories , and my people \\'s history , are alive in these trees and in those birds crying in the treetops . this jungle keeps the stories , and everything i was told about , alive . \"  near salazar \\'s demonstration project , though , an army base , lumber mills , factories , and the city \\'s streets -- the products of thousands of people pressing into a frontier town -- slash into the vermillion soil and leave jagged scars . from the air in mid-afternoon , in a small plane streaking away from pucallpa south toward atalaya           jungle has been tamed , used up , and discarded . gradually , the scars give way to unbroken forest . deep , blackish greens are threaded by rivers that wind back on themselves in elegant curlicues . the sensation of color suddenly intensifies , as if a gray film has been lifted from the land . clouds at eye level loom like spun confections : tigers , toucans , and old bearded men .  back on the ground in searing heat , the sputtering whir of the plane \\'s engine is replaced by a deafening cacophony of insects . along rutted dirt trails , we trudge past clapboard storefronts , the shopkeepers and lumbermen glaring at mishari , murmuring curses and threats . sweat is dripping in sheets off mishari \\'s back as he totters under the weight of his bag and a large cardboard box holding a gift for his nephew : a television set , as it turns out . there are no telephones in atalaya , no automobiles , neon , or faxes ; but after the sun go , is down , televisions glow from           on our first night in town to watch tv , clustering in the cafes to watch peru \\'s favorite shows . one is peru eres -- fantastico , a cross between wheel of fortune and let \\'s make a deal , in which contestants are swept off their feet while trying to gob money blown at them through vacuum tubes . grand-prize winners from another humiliation can walk away with $150 -- and trips for two to miami . ( on this night a woman from cuzco comes close to clinching the trip . )  next up is a strap opera about the smart ways of city-wise mestizos , attributing unbelievable stupidities to darker , rural people . in the shows dramatic denouement , a mestizo doctor rushes in to save the life of a young man on the verge of death , to the audible relief of the town \\'s audience , slapping at mosquitoes and sitting on edge . but as the show fades , to a political commercial promising a \" great change \" with marlo vargas llosa , an indian lumber worker can be heard moaning           the tv , seeming not  julio is a teenager , having returned from harvesting mahogany trees a few days \\' hike away . he says no food was provided by the lumber foreman , who made meals for himself but forced his indian workers to go hungry . julio had survived by scavenging and eating worms . but something had gone dreadfully wrong . julio \\'s stomach has ballooned , as though he had a termite colony inside his belly , and he is in agonizing pain . but without a doctor , or even a curandera ( healer ) , in atalaya , julio does n\\'t stand a chance . along with bottles of rum , curious onlookers grace his bedside , sharing the scorching pain of being powerless to comfort or save his life , as julio calls out a few times for his mother , lapses into unconsciousness-and dies . outside , a whir of cicadas covers the shuffling sounds of preparing the body for burial .  on a patio across the way , a coffee grower is complaining loudly about the backwardness of the local           for the natives . and them not knowing what to do with it ! \" he says , turning his palms up . he talks about the tremendous contributions of the colonizers to the town , the bravery they \\'d demonstrated in moving to the frontier . \" people warned me against coming here , \" he points out . \" they said there were jaguars and alligators . but , look , i \\'ve been here a dozen years and i \\'ve never seen a jaguar . \"  a local lumberman , tapping his forefinger on the table , adds that the world has recently been turned upside down . with this crazy land-tiffing program , he says , indians are now reluctant to work for others , demand high wages , require payment before they work , and often do n\\'t complete their jobs . he mutters that peruvians would n\\'t \" even be on horseback yet if the indians had their way , \" and that indians are responsible for the insurgency . \" it \\'s like a dog with rabies , \" he says firmly .           cure a dog with rabies ? \"  this kind of talk makes the coffee grower nervous , particularly with an outsider present . \" let \\'s acknowledge that everyone is in a squeeze , \" the grower counters , jovially . \" conflict is n\\'t good for anyone . we \\'re all in quite a fix . without a road , we ca n\\'t get our crops to lima . so while rice and corn are stacked up all over atalaya and in every hacienda around , the government is importing rice from china . buying rice ! from china ! and not from us . \" the coffee grower smiles and lays his arms across the table , defenseless . he pivots toward the outsider . \" once i turned down a chance to go work in europe . do you think there is work for me now in the united states ? \"  in a hotel room within hearing , just past an adobe wall surrounding the coffee grower \\'s garden , mishari is splayed out on his bed , his hands beneath his chin and a           a letter accusing him of being a drug trafficker and a guerrilla has been received by the local army commander , but he greets this news placidly , as if it \\'s just another day at the office . when he first trekked into atalaya four years ago , mishari discovered hundreds of ashaninkas working in slavelike conditions for large local landowners , and he helped organize a national campaign against the practice . then , too , he \\'d been threatened on the street . \" at the time i did n\\'t walk through the town , even in daylight , \" mishari says , smiling and shaking his head as if marveling at the mischief of a child . \" now it \\'s better , because over the years these people have seen how many of us there are and how we stand together ... it \\'s still possible that the patrones could hurt us , or even kill . but i doubt they will . \"  mishari muses about his own home village and begins recalling , in a murmuring singsong , a series of stories told           stories about ashaninka lore , which he first heard as a child laboring in harsh conditions on a plantation with armed guards , come back to him in a wave of nostalgia . it \\'s odd but true , he says , that the ashaninkas have no creation myth . in traditional stories , the ashaninkas have always been , and always will be . their history of survival , of successfully repelling intruders for the past five hundred years , is a common theme .  mishari says :  in ancient times , there was a being named aviriri . he was old and bent , and be carried his grandson on his shoulder wherever be went . he never looked up . but when his grandson would ask him , \" granddad , what \\'s that ? \" be answered : \" that \\'s a heron . \" \" that \\'s a jaguar . .... that \\'s a mahogany tree . \" this is bow the animals and things of the world were named .  then one day , the spanish invaded . from across the cerro           . so be said , \" granddad , there are men there , it looks like they are armed . what are they ? \"  since aviriri was like a god , he could bring something into being by imagining it . so he said , \" those are soldiers . they \\'re invading our land . so we \\'re going to change them . . . into black rocks . \"  in my country , near the perene river , there is a big black rock . when i was growing up , people would place offerings there , sometimes just chewed coca leaves and sometimes more . that was a tribute to aviriri . if something bad happened to you , people would say , \" ah , ha ! you have forgotten to pay tribute to aviriri . \" then , in the 1950s , the evangelists came and convinced people that this was just an idol , that aviriri was superstition . and that \\'s bow the belief in aviriri died out .  near wehre the urubamba meets the tambo , two chocolate-colored rivers           loaded so heavily that each time the sputtering engine chokes , water spills over the sides and soaks our feet . as two of our guides shout at one another , shifting their weight for emphasis -- \" it \\'s that way . .... wrong , it \\'s this way ! \" -the streams become a minor flood and several men begin to bail . the canoe \\'s owner remains impassive , setting a third course , tucking into a hidden tributary and accelerating flat out for a remote indian village called tahuanti .  mishari , perched on the bow of the canoe , peers anxiously over his shoulder and squints into the dense , turquoise foliage along the riverbank . he \\'s on edge , his head swiveling like an owl \\'s , on the lookout for townspeople , army patrols , guerrillas . as we pass a family of refugee indians from the highlands , dressed in traditional clothes and paddling downstream , mishari puts a sharp point on all the talk we \\'ve been hearing from local indians about the threat posed by intruders . these highlanders           townspeople , and mishari says they will be ejected from ashaninka territory as well . but surely they are as needy and desperate as anyone . what will happen to them ? \" we hope they will learn from our example , form a community somewhere else , and file for title from the government , \" mishari says .  after we dock , mishari heads up a beaten trail to tahuanti , trees and shrubs dwarfing him , his shoulders beginning to droop , relaxing now . as he passes through communal cropland-acres of cassava , rice , corn , and sugarcane-his surface submissiveness vanishes in expansive gestures , arms akimbo , his mouth puckered to issue an insistent , shrill whistle , which echoes deep into the jungle . he stops to knock down a few papayas from trees in a lush grove , splitting them with a machete and passing slices around to share .  beyond the papaya grove , small canoes traverse a creek loaded with river shad , men and boys spearing their catch ; women and girls gather the fish and carry them           arrow from his bodyguard and jogs to the water , playful as a five-year-old . in his excitement , he slips and falls in , shoes and all , and takes aim , striking and missing , striking and missing again . \" i \\'m a good marksman . it \\'s only these bad arrows that are to blame , \" he jokes .  that comment draws a laugh , as it is translated from spanish to ashaninka and passed from one villager to another , gathered in a row at creekside . tahuanti \\'s leader , a young , sleek man named seminario marinero camaitei , stands a few feet from the others , shifting his intent gaze from one visitor to the next , as if trying to sense whether anyone brings danger to the village .  underneath a hut on stilts , fish laid beneath palm fronds smoke on an oblong grill . in the open-air kitchen above , women are stirring masato , a brew made from boiled , fermented cassava . mishari sits at the table in a place of honor , while seminario           on the floor between the common room and the kitchen , a mentally retarded girl rolls from side to side-clothed , tended , and emitting periodic squeaks . bowls of fish soup , plates of boiled yucca , and gourds full of masato crowd the table as seminario delivers brief news flashes , as if he were a town crier : \" the colonists are getting out of hand ! they \\'re invading our land ! we \\'ve got to figure out a way to get them out ! \"  lima now seems placed on the wrong end of a telescope , the capital \\'s intertwined problems so remote . no one in tahuanti expresses the slightest interest in the capital or anyplace else beyond the village ; there are no questions about where we \\'ve been or what we \\'ve seen . having visited the nearest town , full of drunks , prostitutes , and angry men , seems contact enough with civilization for the people of tahuanti . \" lots of noise , lots of tin , lots of trouble , \" seminario says of atalaya . here           him in the outside world . it \\'s the only conversation i \\'ll have in peru that does n\\'t end with the anxious question , \" do you think there is a job for me in the united states ? \" or the frightened whisper , \" i \\'m scared . what \\'s going to happen to my kids ? \"  after lunch , seminario mentions tentatively that his father \\'s older brother ( \" we do n\\'t know how old he is , he \\'s probably ninety or so \" ) lives a few hours \\' hike away , near the hacienda where he \\'d worked for more than seventy years . sebastian knows more about the history of the village , and the tribe , than anyone else . he was born on the land where he now lives . would we like to meet him ?  by the time we set off into the jungle , we \\'re followed , and flanked , by twelve escorts , barefooted boys and young men wearing cloth briefs , their chests bare , several with bright red achiote daubed           over us , we trudge for an hour , the thin film of swamp water seeping through our shoes , vines ripping at our legs , twisting and turning , leaping over fallen logs , as the trail grows less and less distinct in perpetual twilight beneath the jungle canopy .  after a two-hour trek and canoe ride , with seminario and his brother poling through a creek that winds around massive tree trunks , uncle sebastian , the elder of tahuanti , is waiting . sebastian \\'s skin has slipped from the bones and he falters twice as he shuffles out of his hut to greet us . he \\'s welcoming , but not eager to be interviewed , especially since mishari has to translate into -- and out of -- ashaninka . but sebastian wants it known that he worked for a big landowner as a virtual slave for many decades . slashing his arm toward the thousands of acres of land adjoining his small plot , sebasti5n says : \" that man did nothing but sit all these years . i did the work . and now           title to the land that should be ours . \"  sebastian \\'s point punctuates a subject now being forcefully debated among indian advocates in the united states and europe . preservation of the amazon is often seen as a fight to resist change , stop development , and save the \" lungs of the world . \" but , as mishari pointed out on our way to sebastian \\'s hut , preservation of the amazon is not as simple as preserving the status quo . for many indians , maintaining the status quo , even fighting to simply conserve as much of the amazon basin as possible , could actually mean maintaining the oppression of centuries .  those who work most closely with aidesep complain that this idea seems beyond the grasp of even the most well-meaning outsiders . soren hvalkof , the danish anthropologist who oversees his government \\'s support for the land-titling project , says : \" it frustrates me that the same bastards who conquered the amazon , and dominated it , now call for conserving it , for keeping the status quo .... it frustrates           , should accept the status quo . if the indians stand up , as at tahuanti and in most of the amazon basin today , make political demands , wear manufactured clothes , sell surplus food for cash , they risk losing their cachet with outsiders . the indians are up against a philosophy , the classic division between god and satan , object and subject , humans and nature . we tend to accept them only as long as they remain savages . \"  \" we need room , \" sebastian is saying , insistently . he looks around at the men and boys who have accompanied us and points at them for emphasis . we need to protect these children , and their children , and the children after that -- or our whole world comes to an end . \"  asked to add to mishari \\'s collection of ritual ashaninka myths ( does he remember the traditional stories of his childhood , about great eagles and wild jaguars ? ) , sebastian chuckles , setting off a round of laughter , which echoes through the encampment           . \" we heard many of those things over the years . from my own people , my parents , i heard that gods were inside the trees and birds , in the sun and moon . those were our gods . and then came intruders with different ideas . they said there was only one god , one man , way up there , all by himself . they said we should worship him . but i did n\\'t believe it . and i do n\\'t believe any of what i learned before . did the birds protect us ? did their god protect us ?  \" no , i do n\\'t believe any of that anymore . i do n\\'t believe in spirits , in gods in trees or birds . i do n\\'t believe in god . i only believe in these young ones , and in what they will do with this land . \"  sebastian stops abruptly . just as suddenly , seminario swivels toward the patron \\'s grazing land , which will soon be part of his village if mishari has his way           . great gobs of words tumble out , bitterness so sharp and anger so pungent that the other men back away from him as he begins growling . seminario \\'s tempo picks up and he punches the air , shouting at the missing landowner . his uncle sits with a slight smile , nodding approvingly . seminario calls on the power of the forest to help expel all invaders , demanding the power to do what \\'s just and right . it \\'s a glimpse of the kind of fury that drove fellow ashaninkas in puerto bermudez to take up their bows and face down soldiers with machine guns . \" the colonists are out of hand ! they \\'re invading our land ! we \\'ve got to get them out ! it \\'s ours ! we want them out ! what we want is this : the land must be pure indigenous . pure ashaninka . pure ! that is all ! es todo ! \"  seminario \\'s fierce tirade , full of grievance and venom , has gone on for several minutes . but having reached his conclusion           a switch , turning back and smiling sheepishly , appearing slightly embarrassed by the passion he has just unleashed . among some amazon tribes , anger like this is avoided as a dangerous thing ; set loose in the world , it could make the skies tumble and the sun fall . seminario steps forward , bending in a courtly way and quietly , politely , asks , \" are you ready to go back now ? \" pablo cabado was selected 1989 young photographer of the year by the international center for photography .  photo ( black &; white ) : surrounded by massive environmental destruction , amazon indians have a new plan to save their river and themselves .  photos ( black &; white ) : six million people are crammed into lima ; millions are unemployed in frontier towns .  photos ( black &; white ) : mishari on his way to tahuanti ( left ) ; village elder sebastian resting in his hammock .  photos ( black &; white ) : swimming in the urubamba river near tahuanti ; an ecology project near           is the editor of mother jones . special thanks to richard smith of oxfam america , in lima , for his assistance .  debt for nature : dirty deals ?  evaristo nugkuag has been standing up to intruders in the amazon basin ever since 1979 , when his tribe ran filmmaker werner herzog out of the area and burned his encampment to the ground .  \" he wanted to make this movie about fitzcarraldo , \" nugkuag remembers . \" but we knew that this project would bring outsiders in-delinquents , prostitutes , liars .... also , fitzcarraldo was a landowner who victimized indians to bring his boat across a mountain , and we did n\\'t want to glamorize him . \"  nugkuag is now president of coica , coordinating body for the indigenous peoples organizations of the amazon basin , a federation that represents indigenous amazonians from five countries . it \\'s not widely understood that while most of the amazon land mass is in brazil , only 300,000 of an estimated 1.2 million indigenous amazonians live there . the rest live scattered along the headwaters           , peru , and bolivia .  from coica headquarters on a quiet residential street in lima , peru , nugkuag monitors the federation \\'s work , traveling often to stitch together traditionally separate tribal cultures , spreading news about widespread efforts to protect indian culture by safeguarding indian land . he is most concerned now about a new wave of well-meaning intruders : large environmental organizations out to preserve the amazon . nugkuag sharply criticizes groups involved in \" debt-for-nature swaps , \" arrangements in which organizations buy a portion of a country \\'s debt from international lenders , and then negotiate to create parks or environmental preserves .  \" the environmentalists talk a lot about butterflies , fish , animals , and trees , \" nugkuag says . \" but in their view of the amazon biosphere , they do n\\'t take human beings , indigenous peoples , into account . we are part of the ecosystem , and our ancestors are the ones who protect its resources . and if we \\'re thrown out , who is going to defend the amazon ? national parks are not           a park created by law can also be done away with by law . this has nothing to do with indigenous peoples-just like the debt-for-nature idea .  \" we think that instead of debt for nature , we should be talking about debt for indigenous control -- creating large new extensions where indians can live , to protect our culture while also protecting the land . \"  gaining support from environmentalists is particularly important as coica \\'s battles cross national borders . \" the common problem we all have is controlling our land , \" says nugkuag . \" we need to safeguard the land and develop in our own way . we do n\\'t want sophisticated technology imposed from the outside . we know how to develop with appropriate technology , in our own way , at our own speed , using minimal resources and causing the least damage to the environment . \"  nugkuag \\'s program for the year ahead includes :  -- in brazil , supporting the efforts of the yanomami tribe to remove gold miners who have wreaked havoc on the environment ;           of one-half of the country \\'s amazon basin , recently ceded to them by the government ;  -- in ecuador , supporting the effort to stop the construction of a conoco oil pipeline through indian land ;  -- in bolivia , supporting efforts to secure government recognition of indigenous territorial rights to the chimanes forest , where five thousand indians are fighting lumber companies for control of the land ;  -- in peru , fighting for the land titling program to continue after the inauguration of a new president this month .  in werner herzog \\'s film , eventually shot in 1981 near the headwaters of the amazon river , klaus kinski played fitzcarraldo , a pioneer who risks his life in efforts to build a railroad , produce ice , and stage an opera to uplift the natives who populate a jungle \" full of lies , demons , and illusions . \" while fitzcarraldo \\'s motivations-symbolized by his attempts to drown out the sounds of indigenous drums by blasting a caruso recording of aida into the jungle-are made clear in the film , the indians           , the backdrop for an adventure story , but not the main event .  nugkuag says outsiders have always treated amazonians that way , alternately exploiting them and romanticizing their experience-sometimes doing both simultaneously . now that there is a federation to speak directly for amazon indians , he hopes there will be more indigenous control over the ways in which outsiders attempt to save the amazon . nugkuag says : \" they \\'ve treated us as if we did n\\'t have a brain , a mouth , eyes , ears . but the world is about to find out that we can see , hear , and that we can speak . outsiders are going to have to deal with us directly from now on . \"  for more information about coica , contact : evaristo nugkuag , president , coordinadora de las organizaciones indigenas de la cuenca amaz6nica , jiron almagro 614 , lima 11 , peru ; fax 51-14-423572 .  office in the united states : 1011 orleans street , new orleans , louisiana 70116 ; fax ( 504 ) 522-7454 .  photo (           up to intruders in the amazon basin ever since 1979 , when his tribe ran filmmaker werner herzog out of the area and burned his encampment to the ground .   [SEP]',\n",
              " '[CLS]   even if saddam hussein fails to get his way , kuwait as it existed before august 2 is lost forever . widespread torture and executions have sent as much as two-thirds of its population fleeing abroad , and a disorganized resistance leaves saddam some interesting options .  after a few days , she summoned up her courage and went to the iraqi military post , kuwait city \\'s former salymah police station , looking for her son . the rape stories frightened her . the rumors of people never returning from the building frightened her . but her son was missing , and the reports of children being taken to concentration camps in iraq were torturing her , so she went to the station to inquire . the iraqi soldiers there knew nothing of him . all inquiries should be forwarded to baghdad , she was told . almost as an afterthought , the soldier at the desk waved her on to a room at the end of the hall . she walked down the long hall and opened the door . none of the three           now they were hanging from a beam , their testicles cut off .  like many of the stories emerging from kuwait these days , this one is difficult to corroborate , though it has a tragic ring of truth . witnesses insist on anonymity to protect family and friends still in the country , but as the exiles spread out around the globe and fragmentary bulletins emerge from the resistance inside kuwait , testimony of vast iraqi human-rights abuses is emerging . in october , after interviewing scores of refugees in the region , amnesty international charged that \" a horrifying picture of widespread arrests , torture under interrogation , summary executions and mass extrajudicial killings \" is now the reality in province no. 19 , formerly known as kuwait . my interviews with kuwaiti exiles , u.s. government officials , and mideast analysts confirm amnesty \\'s charges .  the iraqi dictator is a blood-soaked big brother . both the amnesty document and my interviews contain allegations that people have been arrested and even murdered for failing to replace photographs of the emir of kuwait with those of saddam           identity , carrying off public records and renaming streets , hospitals , police stations , schools , museums . public buildings have been turned into detention centers . possession of opposition literature or the kuwaiti flag is treated as a capital offense . the amnesty report notes , \" iraqi military and intelligence routinely torture detainees . some have been given electric shocks or suffered prolonged beatings to sensitive parts of their bodies . others have had limbs broken , their hair plucked out with pincers , their finger- and toenails pulled out , and were threatened with sexual assault or execution . \"  executions seem widespread . doctors who have fled kuwait report iraqi soldiers bringing in dozens of bodies of young men , many shot at close range in the head or heart . the doctors were forced to issue death certificates saying the boys had died after being admitted to the hospital . mr. l. , a kuwaiti citizen with contacts in the resistance , told me of a teenage boy he knew well . the boy was seized by the iraqi military and tortured , not           boy then revealed the names of fifteen of his young allies , many of whom did no more than paint out street signs and tear down portraits of saddam . within a week , all fifteen had been captured and executed .  \" they destroy everything , everything , \" said mrs. q. , a kuwaiti exile . \" saddam hussein is stripping our country . he kills and kills . children . women . he has stolen medical equipment from our hospitals , even our street lights and school desks . why are they doing this ? iraq is not a poor country .... \" she pauses and her mind goes back to those she left behind in kuwait . her back straightens and her eyes glitter with pride and sorrow . \" but still he destroys -- the zoo , our islamic cultural museum with its priceless artifacts , even our children \\'s entertainment garden -- like your disneyland . it is all destroyed and taken away . they say that kuwait now , it is a house of ghosts . \"  some of the ghosts are           iraqi surge across the border on august 2 , but made a stand around kuwait city . fighting was fierce , though brief . according to mr. l. , as the city fell all military planes were ordered out of the country and the army was told to go underground . some mideast analysts and u.s. officials are skeptical about reports of widespread resistance , however . \" the kuwaitis were sort of a playboy army , \" a state department analyst said . \" they never had a reputation for toughness , like the iraqis and the syrians , or for professionalism , like the israelis and jordanians . most of them probably headed for saudi arabia and are attempting to regroup there . \" unconfirmed reports of u.s. special forces teams working with kuwaitis in saudi arabia support this line of thinking , as does the lack of much direct evidence of resistance activities -- photographs , videotapes , eyewitness reports -- beyond the first few weeks of the occupation .  still , a few facts may be pieced together . the size of the resistance is not           the source of much western information -- in the interest of keeping up morale . yet what resistance there is in kuwait has exhibited a jaunty savoir-faire in the face of brutal suppression . they are said to communicate over cellular telephones . in one story making the rounds , the resistance bought a tank from an iraqi deserter for about $500 . unable to drive the thing off and hide it , they blew it up . largely made up of scattered bands of teenagers , and led by a few individuals with military training , the resistance initially drove around running over lone iraqi soldiers and removing street signs to confuse the occupying army . firebomb and sniper attacks also have been reported . the attacks usually come at night and are directed at iraqi outposts in former police stations . to counter the attacks , the iraqis arrest a hundred people each night , place them in jails as shields , and release them in the morning .  the resistance appears to be protecting foreigners hiding in kuwait . when saddam hussein announced that anyone sheltering a           that anyone turning in a foreigner would be shot . early in the occupation , the iraqis marked the houses of foreigners with red paint and painted those of kuwaitis black . that night , the resistance painted over all the red paint with black . such actions are not without cost . mr. l. has firsthand knowledge of a young kuwaiti who was stopped at a checkpoint . a can of black spray paint was found in his car . he was shot on the spot and his body was dumped in his parents \\' front yard .  in the early days of the invasion , the iraqis behaved \" politely \" in kuwait city , said mr. l. but as the resistance disrupted the occupation , the army began to take out its wrath on the people , chiefly by street executions , looting , and raping the egyptian , bangladeshi , and filipino women still working in kuwait as domestic servants . refugees who have survived iraqi torture and made it out of the country report that their inquisitors demand the names of kuwaiti police officers ,           the leaders of the resistance . other refugees say the resistance is led by civilians , and that the bands are not in contact with each other .  my favorite resistance story , perhaps apocryphal , concerns a young kuwaiti with a trunk full of machine guns . stopped at an iraqi checkpoint , he began to complain bitterly about \" being stopped again \" so soon . what , asked the iraqi officer , was he talking about ? there was no other checkpoint in the area . not true , the kuwaiti replied . only half a mile down the road he had just been stopped by an american patrol . the iraqi barked out some orders and the entire checkpoint jumped into their jeeps and drove off in the opposite direction .  it seems everyone in kuwait is waiting for the americans . that is , everyone who is left . the iraqi invasion has displaced over 800,000 people , creating a massive and complex refugee problem . half a million saudis , egyptians , jordanians , palestinians , yemenis , and syrians fled iraq and           new pressure on already-strained local economies ( except for the saudi economy , which can handle the burden ) . two hundred thousand native-born kuwaitis , roughly one-third of the native-born population , escaped the country and are now in saudi arabia , the gulf states , cairo , and london . the deep pockets of the exiled kuwaiti government will ensure that these people will not suffer harsh deprivation . less fortunate are the roughly 150,000 asian refugees -- from bangladesh , india , pakistan , sri lanka , thailand , the philippines , and elsewhere -- who worked as domestic servants and laborers in kuwait . driven from their homes with no money and few possessions , fleeing mainly to barren camps in jordan , they became \" hostages of the desert , \" as one relief official put it .  the u.s. committee for refugees reported that two months after the invasion there were still more than 75,000 non-native kuwaiti refugees , most of them asians , in ill-staffed camps in jordan . approximately 60,000 refugees crossed into syria , 40,000 into turkey , and 20,000 into           pace . the iranian situation is of particular concern to refugee organizations . if baghdad succeeds in patching relations with tehran , the new refugees , as well as the half-million refugees from the iran-iraq war , including 90,000 iraqi kurds , may be forcibly repatriated .  the palestinian factor adds another element of uncertainty to the standoff over kuwait . prior to the iraqi invasion about 300,000 palestinians worked in kuwait , and many are thought to have decamped for other parts of the arab world . although the israelis have been asked by washington to keep a low profile in the crisis , and generally have complied , in october they gave an unusual briefing on the kuwaiti refugee dilemma and the palestinian aspect . israeli intelligence claims that roughly two-thirds of the native-born kuwaiti population -- not the one-third of western estimates -- have fled the country , and that hundreds of thousands of jordanians and palestinians are being brought , or invited back , to kuwait . in other words , there are only about 200,000 kuwaitis left in kuwait . independent reports from journalists and           flocking into iraq and kuwait . some observers are calling this \" the palestinian scenario . \" having driven out the majority of kuwaitis and looted the country , saddam would repopulate it with iraqis and palestinians , declare kuwait a \" temporary homeland \" for the palestinians -- driving a wedge between america and its arab allies -- and call for a \" democratic election \" to consolidate his gains . saddam would retain his army , along with its chemical , biological , and fast-developing nuclear weaponry . the rape of kuwait would he complete .  illustration :  by micah morrison   micah morrison , tas \\'s roving correspondent , will report next month from saudi arabia    [SEP]',\n",
              " '[CLS]  section : spectator \\'s journal  dateline : lilongwe , malawi  i t \\'s almost noon on a saturday at the u.s. information center here , where a two-week-old video of abc news is drawing a big crowd . news of the outside world is hard to come by in malawi , a landlocked country in southern africa that \\'s about as remote as it \\'s possible to be in the jet age . when i told my friends in the states i was going to tour the country by bicycle , most said , \" gee that \\'s great ! \" then asked where it was . but why should they know ? plenty of folks among the 7.5 million people here gave me the same polite , never-heard-of-the-place smile when i told them i come from new york .  a hush fills the room as the audience , more than a hundred locals with a smattering of expatriates-mostly aid workers-take their seats . the bearer of evil tidings , in this case peter jennings , begins in solemn tones with a report on the           . the screen shows a montage of broken buildings and weeping survivors . the audience titters .  another story flashes on the screen . back in the u.s. , a mass murderer has been put to death . as the screen shows the hearse containing his coffin passing through prison gates a wave of chuckling ripples through the room . the next item , a special report on a california sniper who shot several children , draws big belly laughs . now , i \\'m a culturally sensitive person . i remove my shoes at mosques , do n\\'t pat asian kids on the head , and never , never butter my baguettes in paris . but i must admit that i find the native optimism in the face of the world \\'s catastrophes disturbingly upbeat . tomorrow i \\'m to set out alone on a mountain bike through a land where people smile at disaster and laugh at death . as the audience sobs with mirth i reflect , not for the first time , on how wrong walt disney was : it \\'s not such a small world           song and laugh too .  \" when i tell people i \\'ve never seen anyone shot on the street back home they simply do n\\'t believe me , \" my host , an american aid worker who \\'s lived here for nearly two years , tells me after the news . except for the privileged few who \\'ve traveled : abroad , most malawians simply do n\\'t have the frame of reference to relate to the strange happenings of the mzungu world . mzungu means \" foreigner \" in chichewa , the dominant language of the tribes in southern malawi . not that azungu , the plural form , are all that rare here . but they generally keep to the cities , game parks , and beaches of lake malawi , the continent \\'s third largest lake . still , azungu behavior is n\\'t predictable . some say the word itself is related to the verbs kuzungulira , which means \" to go around , \" and kuzungumutu , \" to be crazy . \" my plan to bicycle 100 miles from the capital city of lilongwe to senga           these images .  \" malawians often laugh when they are embarrassed or puzzled , \" my host explains . \" out in the bush , they \\'ll probably laugh at you too , wo n\\'t they , lyman ? \"  \" oh no , sir , \" says lyman , his chief servant . \" maybe in the old days . but now the people see many foreigners . they will take no notice . \"  \" mzungu ! mzungu ! \" the children cry , running from mud huts to point and gape as i bicycle past their villages . many shout the only english phrase they appear to know . \" mzungu , mzungu , give me ten tambala ! \"  on the road to the great lake you see many women carrying 50-pound bags of maize upon their heads , sorceresses hawking love potions guaranteed to bring back an errant husband , and witch doctors exhorting against evil spirits . but nothing draws a crowd like a close encounter of the mzungu kind . the road , a one-lane strip of ripped up tarmac           same loping , somnambulant gait . the pace seldom changes , even in the face of the occasional careening truck . but as i pedal by everyone perks up . in some places people line the road as if watching a parade , the older children calling for money , the toddlers cowering behind their parents . mothers tell their children that if they misbehave , azungu will come and gobble them up .  all countries have their ideas about foreigners and this one , which the world bank ranks among the world \\'s ten poorest , is no different . four centuries after portuguese explorers first came here , westerners are still regarded with amusement , awe , and incredulity . many malawians think of them as carefree , eccentric , and not quite human . they also think most azungu are ugly and all of them are rich .  \" the average malawian does n\\'t think europeans feel hunger or pain , or have any problems , \" a catholic priest who \\'s lived here for twenty-three years told me . \" he thinks money and the           european does n\\'t have to work for them . \"  a chewa legend tells of a woman who commits some unspecified crime and gives birth to a deformed child- born with a head and no body . she nurtures it under the watchful eye of her brother , who notices that at night the : mouth opens and a european emerges , complete with azungu accessories -- car , refrigerator , and tv .  perhaps it \\'s because azungu have all these marvelous possessions that they are considered weak and lazy . after all , they ca n\\'t walk for miles with heavy loads on their or perform the simplest household chores without the aid of fantastic devices .  i certainly fit the stereotype . even with the aid of a sturdy mountain bike , i \\'m bushed after six hours on the road . fifty miles from lilongwe , i struggle up the last long hill toward-a government grammar school where i plan to spend the night . my hosts , two ; american teachers , are n\\'t expecting me . but hospitality in malawi tends to           bags of m &ms,; a pleasant alternative to fried termites-a crispy regional snack . chocolate here is much more costly than bottled beer or an entire evening with a bar girl . but that is n\\'t surprising in a country where kids ask passing azungu if they \\'d like to buy field mice kebabs .  the two teachers , women in their early twenties , had heard through the grapevine that i might stop by . they live in a house on a hill at the edge of the school grounds , overlooking a broad valley . there \\'s no running water or electricity , but they \\'ve each got their own room .  the m &ms; are a smashing success . so is dinner -- beans and rice served in tin plates in their small living room . one of the women , joanna , is from chicago and has only been in the country a couple of months . she says she \\'s beginning to get used to being one of the few azangu for miles around . it \\'s well past eight o\\'clock before they say           spread my sleeping bag on the couch , get inside , and blow out the candles .  a scream rips the night like a howl from hell . joanna bursts from her room , shouting something about the wall . the other girl rushes in and tries to comfort her . \" i ca n\\'t stand it , \" joanna sobs . \" i just ca n\\'t stand it . \"  the bedroom door is ajar . i take a flashlight and peer inside . at first i do n\\'t see anything . then i notice something odd about the wall . it seems to be undulating , as though there were a living thing inside it . i move closer with the flashlight .  eeeek ! bugs ! bugs flow down the wall like a waterfall , fanning out across joanna \\'s desk and spreading onto the floor . they \\'re munching on the rafters and on a pile of papers on the desk . i step outside and close the door .  \" termites , they \\'re termites , \" joanna says between sobs .           . \"  i take a can of pesticide and spray the room with thick bursts . for the termites , it \\'s apocalypse now .  and so to bed .  in the morning a servant came and swept the dead bugs away . outside it was raining pretty hard . i wanted all day but it did n\\'t let up . the next morning it was still raining , but i decided to push on for the lake anyway . the rainy season had begun in earnest , and it turned out to be one of the worst in years , ruining crops and flooding roads . five days later i put aside the bike to travel by thumb , bus , and rented car . further south things were much worse . later that year there was an earthquake ; then a cyclone struck , leaving 80,000 , people homeless .  i would \\' have said it was bad luck . but a catholic priest who put me up for a night at a mission hospital deep in the bush told me that many malawians do           stock in witchcraft . not long ago , he said , a man was accused by fellow villagers of deliberately delaying the start of the rainy season in order to finish repairing the thatched roof on his home . there was a drought and he was blamed . in addition , several bones , said to be those of children , were unearthed near his house . charged with witchcraft and cannibalism , he was tried by being forced to eat a plate of hot piri-piri peppers . he died , thus proving his guilt .  the bones probably belonged to a goat . in all likelihood , said the missionary , the man \\'s real crime was that he had somehow failed to fit in with the community . in malawian society group harmony counts far more than personal happiness and individualism is frowned upon . it is considered illbred to draw attention to oneself by committing such vulgarities as , say , doing a better job than one \\'s co-workers . women keep an even lower profile than men , but among the southern tribes they make the           even a village chief often ca n\\'t act without consulting his mother and sisters , and frequently serves only as their mouthpiece . when a man marries he goes to live in his wife \\'s village where , as an outsider , he must be careful . he ca n\\'t be too forward in malting suggestions , yet \" he faces the dilemma of trying to prove himself because he is a stranger , \" the missionary said .  given this attitude toward outsiders , it \\'s not surprising azungu are often treated with polite , affable mistrust . indeed much of what foreigners say here is taken with a grain of salt . some villagers believe , for example , that a government campaign against aids is really just an azungu ruse to encourage birth control and to keep people from enjoying sex . others acknowledge that the disease exists but it can be prevented by wearing the bark of a mango tree .  azungu may give faulty medical advice , but they \\'re an excellent source of names . malawians change their names as often as you           one american volunteer who left an automobile manual in a rural village while there on a job . returning a year later , he met infants named master cylinder kanyama and brake shoes komoni . the story may be apocryphal , but another foreign teacher compiled a list of names his teenage students chose for themselves : master pencil chibonga , mary engines soko , kidney chimenya , happy holiday simwaba , snow mzoma , texas hardwack fusilani , and nature study nsunza .  young people add to their status by having many nicknames . so do older malawians . the country \\'s leader , his excellency , life president hastings kamuzu banda , is also known as the ngwazi ( powerful one ) , or just plain \" h. e. \" for short . h. e. , who is believed to be about ninety years old , was educated in the u.s. and practiced medicine in london . after more than three decades abroad he returned to what was then the british colony of nyasaland to lead the battle for independence . he \\'s run malawi -- the land           providing relative prosperity stability . but no criticism of government is permitted , and news about the country is very difficult get , especially when you \\'re within its borders . dr. banda permits fewjournalists to enter the country most of the people i talked to asked me not to use their names .  the ngwazi has crushed all opposition and has n\\'t named a successor . now , with his term of office drawing toa close , the big political question is who will succeed him . foreigners here are hesitant to speak openly about malawi \\'s future , but most everyone agrees that even harder times lie ahead . the average life expectancy is forty-five for women , forty-three for men . infant is 151 out of every 1,000 births and 33 percent of all children die before the age of five . the past few harvests were poor and food supplies have been further depleted by the arrival of thousands of refugees from the civil war in neighboring mozambique . but despite the poverty , malawians are most generous to strangers in need .  i experience           of a farmer named daniel moyo . the villagers ca n\\'t take their eyes off me as mr. moyo puts the two chairs he owns in the middle of the courtyard formed by the family \\'s three mud huts . his wife , six children , and a dozen neighbors sit on the porch staring at me , laughing . the chief arrives , dressed in a polo shirt and dark blazer . he chuckles as we shake hands , then sits in the other chair , legs crossed , staring at his shoes . no one speaks . although the younger adults seem to know some english , etiquette does n\\'t require talk.there is no electricity , but in my honor a radio is played throughout the afternoon , though batteries cost roughly half average monthly salary of forty kwacha ( about $15 ) . among other things , the station broadcasts one of the ngwazi \\'s innumerable a song by freddy and dreamers , and a muzak rendition of \" born free . \" dusk , dinner , and bedtime are hours away .  to liven things up           emergencies , assemble it before the curious children and astound everyone as i throw it into the sky . they stand in awe as it loops over the cornfield and swoops down to a perfect landing beside the thatched three-room hut where the moyos sleep . one boy picks it up but mr. moyo \\'s son , seven or eight years old , grabs it , pushes him away , and launches the plane tail-first . it crashes at his feet . after a while , they get the hang of it but then the plane cracks up . for my next act , i juggle three stones . the crowd loves it .  before dinner , we share a carton of chebuku , the local beer brewed from maize that tastes like sour milk and leaves chunks of matter in your teeth . then i \\'m served a piece of chicken and a massive portion of ensema -- steamed cornflour and water that looks and tastes like a giant lump of cream of wheat mr. moyo places the chicken \\'s trunk , containing some internal organs , my tin           he should have it . he puts it back on one insisting i take it . i return it , explaining that i have n\\'t been working as he has am not very hungry . \" but you are so big , \" he says , \" you must eat . \" most azungu tower over the malawi men , whose average height is about 5 feet 7 inches . i get a flash of inspiration . \" can i have it for lunch tomorrow ? \" he seems pleased as his wife wraps it in brown paper . the children gobble the remains of the meal .  at bedtime the moyos unroll a straw mat on the dirt floor of my room and bid me goodnight . i roll out my sleeping bag , light a slow-burning mosquito repellent and read by candlelight . a bug stings my neck . i crush it to death in my fingers , not because bugs spread disease but because i hate bugs . i blow out the candle , lay on my back , and hear something scurry across the floor .           rat . i wonder if black mamba snakes ever crawl inside the hut .  from this vantage point it \\'s easy to see the appeal of the chewa legend about the european who emerges from the mysterious head each night with all his marvelous possessions . according to that story , the stranger disappears each dawn . but one night , the woman and her brother resolve to catch him . trapped , he kisses them both , turning them into azungu too . the woman marries stranger and they live happily ever after .  illustrations  by joseph r. gregory   joseph r. gregory is a senior editor at the foreign policy association in new york    [SEP]',\n",
              " '[CLS]   soviet jewry is giving israel new life -- but what kind of life will it be ?  a witty rabbi once said that the trotskys make the revolutions , and the bronsteins pay the price .  in may , more than 10,000 fleeing bronsteins arrived in the jewish state . it wo n\\'t be long , according to israeli officials , before the monthly figure reaches 20,000 . no one knows how many soviet jewish refugee-immigrants will finally come , in part because no one knows how many jews there are in the teetering russian empire -- guesses range from 1.4 million to twice that . it \\'s possible that in the next five to seven years , a tidal wave of 700,000 newcomers , mainly from the soviet union but also from hungary , argentina , ethiopia , and south africa , will hit israel . putting it in american terms , this means the equivalent of 42 million new citizens in less than a decade .  the stampede out of the soviet union is a spinoff of glasnost and perestroika . what           nationalists can now be shouted and published -- that the jews collectively are the cause of the country \\'s woes , economic , political , social , and cultural . this self-pitying canard was born even before trotsky-the-demon burst onto the scene . it has lived underground for generations , and now that free speech has been decreed , the old jew-hatred is in the open . it \\'s frightening .  to be sure , may 5 , which is marx \\'s birthday and was unofficially heralded this year as pogrom day , passed non-violently . this did little to dampen the panic many jews are taking the chance to get out to wherever they can , as quickly as they can . it \\'s unlikely , though not impossible , that if and when pogroms break out , they \\'ll be officially inspired . more likely , they \\'ll be just one aspect of the chaos if perestroika fails , the soviet union collapses , and government of any kind becomes a memory .  the stampede is something of a race against time . there \\'s no telling           open . gorbachev was the one who opened them , he made visas available almost on demand , even though he explicitly acknowledged that a jewish exodus would mean a brain drain for his backward country . he decided to let the jews go nevertheless not because he was converted to zionism , but because he hoped to win trade concessions from the u.s. by living up to basic human rights standards , of which freedom to emigrate is one . his successor could well have different ideas . that is also concentrating the minds of soviet jews .  the door to the u.s. having meanwhile been nearly shut by a new quota , they go to israel , where the door is wide open -- for a zionist , what is being granted the soviet jews , what is taking place , is nothing more or less than repatriation . they \\'re fleeing , but compared with russians who have to stay behind , those jews who have gotten out are privileged , the luckiest casualties of the simultaneous washout of marxism-leninism and russian imperialism . which of course           of milk and honey , \" the smooth kremlin spokesman , gennady gerasimov , calls israel . it certainly is , especially in contrast to the land of empty shelves . yet it \\'s also a country beset by a palestinian uprising , threats of poison gas from iraq , an unemployment rate of almost 10 percent , a permanent political crisis , strikes , and 25 percent annual inflation . a great soviet exodus at this time is , and is n\\'t , exactly what the jewish state needed . on the one hand , it demonstrates that the obituaries for zionism were premature . on the other hand , it may swamp the so-called \" absorption \" apparatus .  where are hundreds of thousands of newcomers to be housed ? how will jobs for them be created ? generally , big waves of immigration have triggered boom times , but a wave this big has n\\'t been seen since israel \\'s first years , which were years of idealism , austerity , and rationing . the situation bears some comparison to that in west germany when the wall           with their better-off brethren . no one has yet suggested that the door be shut -- that would be the repeal of zionism . but there is some grumbling and alarm undercutting the satisfaction of rescuing all these jews , receiving all these reinforcements .  blessing or curse ? israel is a tiny , not very well-off or efficiently run country , that is going to have a hard time integrating , much less turning to advantage , this windfall of trained brains and great genes -- this year , for example , 1,000 soviet jewish physicists are expected . such \" material , \" as it is referred to , could be the country \\'s salvation , but only with a lot of luck and intelligent planning on the part of a not exactly brilliant bureaucracy . already , some of the immigrants who have been here long enough to catch their breaths have staged protests against the red tape . if things go badly , ph.d.s from moscow university who find themselves on the dole in tel aviv are liable to head for the us. , quota or           be leftists and doves who fear that the soviet jews will vote right-wing-hope in private that somehow the influx will turn out to be not so great . and in public , there is already talk across the political spectrum of declaring a state of emergency , bringing the army in to take over from the bureaucrats and setting up reception camps of pre-fabs , even tents . the incessant arrival of chartered jumbos from budapest and bucharest , soon from warsaw and helsinki , too , and maybe even straight from moscow , probably makes such talk all too realistic .  meanwhile , israel remains very much a democracy . the soviet jews represent new voters , so almost all of the country \\'s dozen parties , including the communists , are earnestly wooing them . only the two arab parties are leaving them alone . the rush starts even before the flights land at ben-gurion airport . unfazed by the absence of full diplomatic ties between israel and the soviet union , many parties have opened branches in moscow , leningrad , and other cities , where tea           the israelis-to-be during the months they must anxiously wait before they can leave . once in israel , they are bombarded with everything from dinner invitations to offers of furniture by ward heelers .  it \\'s fairly safe to predict that the communists wo n\\'t get much backing from the newcomers . the romance of soviet jews with communism , which was a minority phenomenon even when it was hot , long ago burned out . nor will the anti-zionist , rabbinical parties in israel appeal much to them . though something of a religious revival has been noted among soviet jews of late , most are secular , not to say profoundly ignorant of all things jewish -- most of the men arrive uncircumcised . between the two extremes of communists and rabbis , however , all the other parties have a chance to win votes .  the question is whether any one party , or bloc of parties , will pick up the lion \\'s share , breaking the deadlock between \" doves \" and \" hawks , \" depriving the rabbinical parties of the balance of           so-called peace process in the middle east .  \" they \\'re ours , \" a likud back-roomer said recently . his confidence is based on experience with an earlier , smaller exodus of soviet jews , which brought some 150,000 to israel between 1969 and 1979 . at exit polls during knesset elections in those years , respondents were n\\'t asked their country of origin . nevertheless , it \\'s the conventional wisdom that the majority of the previous soviet immigrants voted for the likud , and that it was in part thanks to their support that menachem begin was able to come in from the wilderness in 1977 and become prime minister .  the conventional wisdom fits well with impressionistic evidence gathered in the company of ex-soviet israelis . furthermore , it stands to reason . a very high percentage of the jews from the soviet union -- especially those from the baltic , russian , and ukrainian republics -- are doctors , engineers , inventors , teachers , musicians , linguists . they constitute a class , a type . were they israeli-born , most would hold           for labor and parties to its left . but their natural inclinations are smothered or corrected , at least temporarily and often forever , by what they learned in their previous lives , that is , by the socialism and anti-semitism they knew in the soviet union .  the red flags at the labor party \\'s may day parade in tel aviv evoked laughter at best and fear at worst among the new arrivals . it did n\\'t matter that labor has ditched almost all of its ideology , that the flags are vestigial -- it rubbed the wrong way anyway . so did the dovish propaganda of some laborites and many of those to labor \\'s left , decrying the israeli occupation of the west bank and plumping for talks with the plo . all that smacked of the brezhnev-era izvestia . though the left and center-left in israel is where you find most , if by no means all , of the local academics , scientists , and bohemians , nevertheless the same sort of people who had come from the soviet union had good reason not to           counterparts in zion .  to make things worse for itself , until 1977 , and even for a while afterward until the political earthquake of that year which brought begin and the right to the top sunk in , labor behaved like the establishment and either patronized or rejected most of those ex-soviets who tried to join up . the likud , however , the underdog party that lambasted socialism and all its bureaucratic works and exhibited no guilt about the occupation , welcomed the newcomers , as it did the moroccans a decade before . the results were immediate and some have been long-lasting -- today sixty-five ax-soviets sit on the likud \\'s central committee , to only twenty-five on labor \\'s . the likud is counting on the current , potentially much bigger wave of soviets to do as its predecessor did . according to this scenario , every jumbo that lands improves the likud \\'s odds to form a government without labor after the next general elections , which may be called soon .  labor has n\\'t written the newcomers off . prof. mikhail agursky ,           become a laborite soon after , says that the support of these latest arrivals is up for grabs . for one thing , he says , they \\'re not zionists like those who came in 1969-79 . they \\'re refugees -- most would be going to brooklyn if the u.s. had n\\'t imposed a quota . the only things they have to declare in their ideological baggage are a loathing for socialism and a rather idealized love of its opposite , which they believe prevails everywhere in the west , of which israel is a part . motivated much more by fear and a desire for the good , safe life than by jewish nationalism , they \\'ll be less susceptible to the likud and more open to labor , which advertises itself as the party of peace . so hopes agursky , a zionist himself .  besides , labor has mended some of its ways . it long ago expunged \" socialism \" from its vocabulary , and the first ex-soviet member of the knesset is a laborite , the georgian-born ephraim gur . today \\'s refugees may finally           who came in the seventies , a good many of whom by now have become acclimated , put their previous lives and experiences in perspective , and spread out across the spectrum . the trouble for labor is that as they became more knowledgeable about the not-very-edifying workings of israeli democracy , those ax-soviets who abandoned the likud in the eighties were probably as likely to move farther to the right as they were to switch to labor . you often hear hebrew spoken with a juicy russian accent by activists for parties whose ideas about the arabs make the likud \\'s sound conciliatory .  things are fluid . everyone is hopeful , everyone is watching his back . likud and labor are both keeping a wary eye on natan ( anatoli ) sharansky , the celebrated zionist who spent years in the gulag and was released to come to israel in 1986 . both big parties have courted him in vain . he \\'s probably the most popular figure among the ax-soviets , and probably could deliver a couple of added knesset seats to the party he blessed --           alternatively , he could field a list of his own . he already heads something called zionist forum , a nonpolitical pressure group seeking to cut red tape in the immigration and absorption process . rumors circulate that zionist forum will go political . but it probably wo n\\'t , sharansky obviously being aware that as things stand , it \\'s easier to keep your integrity in the gulag than the knesset .  meanwhile , the jumbos touch down , and if another \" national unity \" government does n\\'t emerge from the current political torture , there will probably be early elections .  labor \\'s campaign heading into an election will stress the opportunity to compromise with the palestinians and the folly of defying the world by spurning that chance . benefits will be foreseen even before peace is made . why , for example , not do everything possible to free up money being budgeted for the army , and for containing the intifada , and use it instead to absorb the immigration , an immigration apt to grow even larger if israel unbends ? shimon peres           party , has already said that as soon as israel gets on track and accepts us . secretary of state james baker \\'s ideas for talks , the ussr will renew full diplomatic relations , direct flights from moscow will double or triple the number of soviet jews arriving , the intifada will halt , and american aid will actually increase . indeed , in what might be collusion , the state department and soviet foreign minister eduard shevardnadze have also let it be understood that if israel relaxes its conditions for meeting the palestinians ( read the plo ) and pledges not to settle soviet jews in the occupied territories , it would become possible to honor the el al-aeroflot agreement on a direct moscow-tel aviv route .  this is the agreement that the arab league persuaded the kremlin to put on ice following yitzhak shamir \\'s remark last january about a big immigration necessitating a big israel . right-wing zionists are especially tempted by such an airlift . an expanded , accelerated exodus coupled with the start of talks would , for opposite reasons , be both their           would also appeal to the soviet newcomers in israel . they have scant attachment to or interest in the occupied territories . fewer than a thousand so far have chosen to go to the west bank . but many do have family and friends back in the ussr sitting on their own suitcases as the rumors of imminent pogrom multiply . labor , which is in sad shape after failing to form a government , might have success too in selling itself , both to veteran israelis and to greenhorns voting for the first time , as the peace party . ex-soviet jews who \\'d rather have gone to the states are n\\'t eager to get into uniform and help suppress the intifada , as the boys and men among them up to the age of fifty-two will have to do . and it is n\\'t any relief to be instructed in the use of gas masks .  the likud will riposte that those who sound dovish notes are n\\'t necessarily the ones best suited to make peace with the arabs . after all , the peace treaty with egypt           such breakthroughs is blocked , according to this argument , only so long as the men ruling the arabs can believe that they have more to gain , and less to lose , by threatening war than making peace . to be sure , the likud is riven at least as seriously as labor is by tactical disputes and personal feuds . but when voting time comes its hard-nosed outlook on negotiations has so far always made more sense to more israeli jews . certainly , if in the next elections the conventional wisdom is borne out , and the majority of the new voters from the ussr help to return the likud to power , this of itself wo n\\'t mean that the \" peace process \" is doomed .  the great exodus is a rabbit that history has pulled out of its hat . from being a country where the majority of jews have grandfathers born in moslem countries , israel is about to become a place where european backgrounds are more common . if it can hold on to most of the newcomers and their children ,           it will become a stronger , more secure , less nervous country . other things may change too , but israel \\'s brand of democratic politics is n\\'t necessarily going to be one of them . no one is betting that there is going to be an electoral reform soon to curb the small parties . nor can the arrival of hosts of soviet jews be expected to answer the eternal questions behind the nonstop political uncertainty : in what ways should the jewish state be jewish ? and how should peace with the arabs be pursued ?  illustration :  by edward norden   edward norden is a writer living in jerusalem    [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOLRljp3gr9T",
        "colab_type": "code",
        "outputId": "bcf05a12-0ec5-4e22-cec1-08f94cbe25e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'section', ':', 'movements', 'from', 'socialist', 'to', 'republicans', ',', 'a', 'generation', 'of', 'student', 'activists', 'test', 'the', 'limits', 'of', 'volunteer', '##ism', '.', 'university', 'of', 'houston', 'student', 'lloyd', 'jacobs', '##on', 'was', 'organizing', 'for', 'a', 'sleep', '-', 'in', 'outside', 'a', 'local', 'homeless', 'shelter', 'and', 'invited', 'the', 'head', 'of', 'the', 'campus', 'college', 'republicans', 'to', 'join', 'in', '.', '\"', 'do', 'n', \"'\", 't', 'you', 'want', 'to', 'be', 'part', 'of', 'bush', \"'\", 's', 'thousand', 'points', 'of', 'light', '?', '\"', 'jacobs', '##on', 'asked', ',', 'and', 'quickly', 'got', 'the', 'student', 'to', 'agree', '.', 'others', 'in', 'the', 'republican', 'group', 'hesitated', ',', 'but', 'reluctantly', 'came', 'along', '.', 'jacobs', '##on', 'next', 'enlisted', 'the', 'college', 'democrats', ':', '\"', 'they', \"'\", 're', 'participating', '.', 'are', 'n', \"'\", 't', 'you', 'going', 'to', 'as', 'well', '?', '\"', 'the', 'houston', 'students', 'initially', 'regarded', 'homeless', 'people', 'as', 'face', '##less', 'and', 'di', '##sp', '##osa', '##ble', '.', 'but', 'they', 'were', 'un', '##ner', '##ved', 'by', 'what', 'they', 'witnessed', 'while', 'trying', 'to', 'sleep', 'on', 'cardboard', 'sheets', 'on', 'the', 'hard', ',', 'cold', 'ground', '.', 'huddled', 'together', ',', 'they', 'gingerly', 'began', 'talking', 'with', 'shelter', 'residents', '.', 'the', 'young', 'republican', 'coordinator', 'returned', 'to', 'write', 'an', 'editorial', 'for', 'the', 'campus', 'daily', 'paper', ',', 'in', 'which', 'he', 'said', 'he', 'was', 'amazed', 'to', 'find', 'the', 'homeless', 'had', 'com', '##pre', '##hen', '##sible', 'thoughts', ',', 'and', 'that', 'relying', 'on', '\"', 'worthless', 'government', 'bureau', '##crats', ',', '\"', 'something', 'clearly', 'needed', 'to', 'be', 'done', '.', '\"', 'you', 'can', 'not', 'possibly', 'know', 'the', 'situation', ',', '\"', 'he', 'concluded', ',', '\"', 'until', 'you', 'open', 'your', 'eyes', '.', '\"', 'everywhere', ',', 'formerly', 'ap', '##oli', '##tical', 'students', 'have', 'begun', 'to', 'stream', 'into', 'neighborhood', 'soup', 'kitchens', ',', 'shelters', ',', 'and', 'tutor', '##ing', 'campaigns', '.', 'at', 'some', 'campuses', ',', 'volunteer', 'service', 'programs', 'have', 'grown', 'ten', '##fold', 'in', 'the', 'past', 'few', 'years', ',', 'placing', 'as', 'many', 'as', 'a', 'thousand', 'students', 'a', 'week', 'in', 'community', 'projects', '.', 'it', \"'\", 's', 'easy', 'to', 'dismiss', 'these', 'students', 'as', 'gus', '##hy', 'candy', 'stripe', '##rs', ',', 'or', 'to', 'chi', '##de', 'them', 'for', 'not', 're', '-', 'creating', 'the', 'acts', 'of', 'those', 'who', 'rebelled', 'twenty', ',', 'years', 'ago', 'at', 'berkeley', 'or', 'columbia', '.', 'but', 'by', 'breaking', 'students', 'out', 'of', 'ins', '##ulated', 'worlds', ',', 'setting', 'no', 'bars', 'on', 'who', 'may', 'participate', ',', 'and', 'demanding', 'that', 'individuals', 'take', 'seriously', 'the', 'lives', 'of', 'those', 'they', 'hope', 'to', 'assist', ',', 'the', 'community', '-', 'service', 'movement', 'has', 'the', 'potential', 'to', 'create', 'the', 'broad', '##est', 'base', 'for', 'campus', 'activism', 'in', 'years', '.', 'if', 'the', 'past', 'is', 'a', 'guide', ',', 'these', 'seemingly', 'inn', '##oc', '##uous', 'charitable', 'involvement', '##s', 'may', 'even', 'produce', 'later', 'commitments', 'that', 'could', 'shift', 'the', 'service', 'movements', 'began', 'by', 'making', 'an', 'end', 'run', 'around', 'prevailing', 'barriers', 'to', 'political', 'involvement', '.', 'most', 'students', 'mist', '##rus', '##ted', 'campus', 'activists', ',', 'believed', 'solutions', 'to', 'society', \"'\", 's', 'major', 'problems', 'were', 'individual', ',', 'not', 'structural', ',', 'and', 'felt', 'they', 'could', 'have', 'little', 'influence', 'on', 'key', 'public', 'issues', '.', 'service', 'participants', 'instead', 'performed', 'immediately', 'useful', 'tasks', 'that', 'did', 'not', 'force', 'them', 'to', 'challenge', 'en', '##tre', '##nched', 'institutional', 'authority', '.', 'these', 'efforts', 'still', 'remained', 'a', 'marginal', 'campus', 'presence', 'in', '1984', ',', 'when', 'a', 'group', 'of', 'recent', 'college', 'graduates', 'launched', 'the', 'campus', 'outreach', 'opportunity', 'league', '(', 'cool', ')', 'as', 'a', 'national', 'vehicle', 'to', 'promote', 'student', 'community', 'involvement', '.', 'angered', 'by', 'media', 'stereotypes', 'that', 'called', 'his', 'generation', 'un', '##car', '##ing', ',', 'cool', 'co', '##fo', '##under', 'wayne', 'mei', '##sel', 'undertook', 'a', 'fifteen', '-', 'hundred', '-', 'mile', 'walk', 'from', 'maine', 'to', 'washington', ',', 'd', '.', 'c', '.', ',', 'where', 'he', 'stopped', 'at', 'campus', 'after', 'campus', 'to', 'spread', 'the', 'message', 'of', 'commitment', '.', 'cool', 'grew', ',', 'by', '1989', ',', 'to', 'a', 'six', '-', 'hundred', '-', 'school', 'network', '.', 'when', 'i', 'first', 'talked', 'to', 'cool', \"'\", 's', 'current', 'executive', 'director', ',', 'twenty', '-', 'six', '-', 'year', '-', 'old', 'julia', 'sc', '##at', '##li', '##ff', ',', 'she', 'said', 'the', 'movement', \"'\", 's', 'task', 'was', '\"', 'to', 'help', 'the', 'person', 'lying', 'bleeding', 'in', 'the', 'street', ',', 'to', 'put', 'our', 'hands', 'over', 'the', 'larger', 'issues', ',', 'sc', '##at', '##li', '##ff', 'said', ',', 'up', 'to', 'the', 'nation', \"'\", 's', 'leaders', ',', 'who', 'would', 'respond', 'when', 'they', 'heard', 'sufficiently', 'el', '##o', '##quent', 'testament', '##s', 'of', 'need', '.', 'sc', '##at', '##li', '##ff', 'recalled', 'how', 'author', 'and', 'homeless', '-', 'rights', 'activist', 'jonathan', 'ko', '##zo', '##l', 'had', 'praised', 'the', 'students', 'cool', 'worked', 'with', 'by', 'e', '##qua', '##ting', 'them', 'with', 'the', 'founding', 'generation', 'of', 'vietnam', '-', 'era', 'sd', '##s', 'activists', ',', 'and', 'how', 'she', \"'\", 'd', 'br', '##id', '##led', 'at', 'the', 'comparison', '.', '\"', 'we', \"'\", 're', 'different', ',', '\"', 'she', 'said', '.', 'when', 'l', 'asked', 'how', ',', 'she', 'answered', ',', 'smiling', ',', '\"', 'we', 'have', 'macintosh', '##es', 'and', 'mode', '##ms', ',', '\"', 'then', 'added', ',', '\"', 'we', 'live', 'our', 'beliefs', '.', 'if', 'we', 'say', 'something', ',', 'we', 'back', 'it', 'up', '.', 'if', 'we', 'talk', 'about', 'housing', ',', 'we', 'get', 'involved', 'with', 'housing', '.', '\"', 'sc', '##at', '##li', '##ff', 'stressed', 'cool', \"'\", 's', 'authentic', 'commitment', ',', 'as', 'opposed', 'to', 'the', 'radical', 'post', '##uring', 'that', 'she', 'associated', 'both', 'with', 'her', '1960s', 'predecessors', 'and', 'today', \"'\", 's', 'campus', 'pol', '##itic', '##os', '.', 'commitment', 'and', 'authenticity', ',', 'however', ',', 'do', 'not', 'always', 'create', 'social', 'change', '.', 'at', 'a', 'recent', 'conference', ',', 'one', 'stanford', 'student', 'explained', 'how', 'he', \"'\", 'd', 'learned', 'more', 'from', 'volunteer', '##ing', 'than', 'from', 'all', 'his', 'courses', 'in', 'school', ',', 'then', 'concluded', ',', '\"', 'have', 'the', 'same', 'experience', 'working', 'in', 'the', 'same', 'homeless', 'shelter', 'that', 'i', 'did', '.', '\"', 'from', 'its', 'beginning', ',', 'movement', 'participants', 'have', 'alternate', '##d', 'between', 'a', 'fear', 'of', 'taking', 'on', 'explicitly', 'political', 'challenges', ',', 'and', 'a', 'sense', 'that', 'they', 'can', 'not', 'allow', 'the', 'conditions', 'they', 'address', 'to', 'continue', '.', 'the', 'resulting', 'tension', 'expresses', 'itself', 'in', 'a', 'variety', 'of', 'ways', '.', 'one', 'group', 'of', 'dartmouth', 'students', 'volunteered', 'eagerly', 'in', 'the', 'soup', 'kitchens', ',', 'but', 'wanted', 'nothing', 'to', 'do', 'with', 'an', 'ox', '##fa', '##m', 'fast', 'for', 'hunger', ',', 'considering', 'it', '\"', 'too', 'ideological', '.', '\"', 'a', 'young', 'woman', 'at', 'a', 'small', 'north', 'carolina', 'college', 'began', 'an', 'innovative', 'mentor', 'program', 'for', 'pregnant', 'teens', ',', 'yet', 'feared', 'that', 'if', 'she', 'worked', 'to', 'shape', 'larger', 'policy', ',', 'she', 'would', 'get', 'burned', 'out', '.', '\"', 'i', 'do', 'community', 'service', 'for', 'myself', ',', '\"', 'she', 'concluded', ',', '\"', 'because', 'i', 'have', 'a', 'passion', 'for', 'it', '.', 'i', 'ca', 'n', \"'\", 't', 'save', 'the', 'world', '.', '\"', 'this', 'vision', 'is', 'limited', 'not', 'only', 'by', 'the', 'students', \"'\", 'own', 'war', '##iness', ',', 'but', 'by', 'praise', 'from', 'the', 'nation', \"'\", 's', 'dominant', 'elite', '.', 'the', 'movement', 'can', 'seem', 'a', 'neat', 'fit', 'with', 'the', 'bush', 'administration', \"'\", 's', 'volunteer', 'action', 'award', ',', 'and', 'a', 'cong', '##rat', '##ulator', '##y', 'telegram', 'from', 'barbara', 'bush', '.', 'following', 'lengthy', 'internal', 'debate', ',', 'cool', 'also', 'accepted', 'a', '1988', 'grant', 'from', 'co', '##ors', ',', 'whose', 'owners', 'are', 'major', 'fund', '##ers', 'of', 'the', 'new', 'right', 'agenda', '.', 'yet', 'distinctions', 'between', 'political', 'and', 'service', 'efforts', 'continue', 'to', 'blur', '.', 'in', 'march', '1988', ',', 'twelve', 'university', 'of', 'minnesota', 'students', 'drove', 'down', 'to', 'browns', '##ville', ',', 'texas', ',', 'to', 'spend', 'a', 'week', 'as', 'participants', 'in', 'one', 'of', 'the', 'alternative', 'spring', 'breaks', 'that', 'have', 'recently', 'pro', '##life', '##rated', 'across', 'the', 'country', '.', 'they', 'visited', 'the', 'gulf', 'coast', 'beaches', ',', 'but', 'also', 'tutor', '##ed', 'kids', ',', 'rebuilt', 'an', 'old', 'woman', \"'\", 's', 'burned', 'house', ',', 'and', 'stopped', 'at', 'a', 'shan', '##ty', '##town', '.', 'later', ',', 'they', 'met', 'three', 'teenage', 'refugees', 'from', 'honduras', ',', 'the', 'last', 'surviving', 'males', 'from', 'their', 'village', ',', 'who', 'described', 'seeing', 'friends', 'killed', 'by', 'the', 'military', 'or', 'ground', 'down', 'by', 'poverty', '.', 'the', 'students', 'concluded', 'their', 'week', 'with', 'a', 'visit', 'to', 'the', 'nearby', 'sanctuary', 'center', ',', 'casa', 'oscar', 'romero', '.', 'when', 'the', 'refugees', 'brought', 'out', 'guitars', ',', 'the', 'students', 'led', 'off', 'with', 'the', 'only', 'songs', 'they', 'could', 'think', 'of', ',', 'the', 'themes', 'from', 'gill', '##igan', \"'\", 's', 'island', 'and', 'the', 'brady', 'with', 'loss', ',', 'offered', 'their', 'own', 'ballads', ',', 'about', 'homes', 'left', 'behind', 'and', 'their', 'hopes', 'for', 'justice', '.', 'a', 'few', 'days', 'after', 'the', 'students', 'arrived', 'back', 'home', ',', 'minneapolis', 'residents', 'demonstrated', 'in', 'opposition', 'to', 'governor', 'rudy', 'per', '##pic', '##h', \"'\", 's', 'decision', 'to', 'send', 'the', 'minnesota', 'national', 'guard', 'to', 'central', 'america', '.', 'nearly', 'all', 'the', 'students', 'who', 'had', 'visited', 'browns', '##ville', 'were', 'there', '.', 'at', 'a', 'cool', '-', 'sponsored', 'leadership', 'summit', 'this', 'past', 'summer', ',', 'some', 'students', 'wrote', 'a', 'series', 'of', 'questions', 'for', 'the', 'ad', 'hoc', 'newsletter', ':', '\"', 'is', 'community', 'service', 'an', 'end', 'itself', ',', '\"', 'they', 'asked', ',', '\"', 'or', 'is', 'it', 'a', 'means', 'for', 'social', 'change', '?', '\"', '\"', 'how', 'do', 'we', 'reconcile', 'the', 'tension', 'between', 'those', 'who', 'do', 'n', \"'\", 't', 'want', 'to', 'be', 'forced', 'into', 'politics', \"'\", '.', '.', '.', 'and', 'those', 'who', 'believe', 'that', 'community', 'service', 'without', 'commitments', 'to', 'long', '-', 'term', 'structural', 'change', 'does', 'more', 'harm', 'than', 'good', '?', '\"', '\"', 'given', 'our', 'diverse', 'backgrounds', 'and', 'motives', 'for', 'doing', 'what', 'we', 'are', 'doing', ',', 'what', 'is', 'it', 'that', 'brings', 'us', 'all', 'together', '?', '\"', 'they', 'are', 'questions', 'that', 'go', 'to', 'the', 'heart', 'of', 'the', 'student', 'service', 'movement', \"'\", 's', 'growing', 'pains', '.', 'during', 'cool', \"'\", 'd', 'worked', 'in', 'surrounding', 'bronx', 'neighborhoods', 'sparked', 'bitter', 'debate', 'when', 'they', 'insisted', 'that', 'service', 'activists', 'must', 'begin', 'taking', 'political', 'stands', '.', 'they', 'influenced', 'the', 'organization', 'to', 'later', 'end', '##ors', '##e', 'its', 'first', 'national', 'rally', ',', 'the', 'october', '1989', 'washington', ',', 'd', '.', 'c', '.', ',', 'march', 'against', 'hunger', 'and', 'homeless', '##ness', '.', 'at', 'a', 'recent', 'cool', 'gathering', ',', 'alex', 'byrd', ',', 'a', 'black', 'junior', 'at', 'rice', ',', 'said', 'he', 'was', 'tired', 'of', 'all', 'the', 'bro', '##chu', '##res', 'showing', 'middle', '-', 'class', 'white', 'kids', 'cr', '##ad', '##ling', 'little', 'black', 'children', 'in', 'their', 'arms', ',', 'and', 'initiated', 'discussions', 'on', 'race', '.', 'responding', 'to', 'these', 'concerns', ',', 'cool', 'recently', 'presented', 'workshops', 'where', 'black', 'students', 'suggested', 'that', 'white', 'volunteers', 'might', 'have', 'to', 'do', 'things', 'they', \"'\", 've', 'never', 'done', ':', '\"', 'approach', 'black', 'colleges', ',', 'the', 'alpha', 'fra', '##tern', '##ities', ',', 'or', 'the', 'urban', 'league', '.', 're', '##lin', '##qui', '##sh', 'power', 'and', 'work', 'on', 'someone', 'else', \"'\", 's', 'project', 'where', 'you', \"'\", 're', 'totally', 'outnumbered', '.', 'put', 'yourself', 'in', 'the', 'situation', 'a', 'lot', 'of', 'minorities', 'put', 'themselves', 'in', 'every', 'day', '.', '\"', 'by', '1989', ',', 'cool', 'director', 'julia', 'sc', '##at', '##li', '##ff', 'explained', 'how', 'her', 'involvement', 'with', 'a', 'racial', '##ly', 'and', 'culturally', 'diverse', 'group', 'of', 'movement', 'participants', 'had', 'given', 'her', 'a', 'new', 'circle', 'of', 'friends', 'and', 'introduced', 'a', 'lot', 'in', 'the', 'year', 'since', 'our', 'initial', 'conversation', 'and', 'now', 'stressed', 'her', 'discomfort', 'with', 'the', 'image', 'of', '\"', 'a', 'bunch', 'of', 'florence', 'nightingale', '##s', 'running', 'out', 'to', 'bandage', 'people', \"'\", 's', 'wounds', '.', '\"', 'historically', ',', 'involvement', 'in', 'service', 'efforts', 'has', 'laid', 'the', 'foundation', 'for', 'some', 'of', 'the', 'country', \"'\", 's', 'most', 'significant', 'social', 'movements', '.', 'at', 'the', 'turn', 'of', 'the', 'century', ',', 'key', 'future', 'new', 'dealers', '-', '-', 'like', 'secretary', 'of', 'labor', 'frances', 'perkins', ',', 'works', 'project', 'administration', 'head', 'harry', 'hopkins', ',', 'and', 'first', 'lady', 'eleanor', 'roosevelt', '-', '-', 'first', 'encountered', 'political', 'issues', 'in', 'their', 'youth', ',', 'while', 'working', 'in', 'the', 'settlement', 'houses', 'that', 'sought', 'to', 'assist', 'new', 'immigrants', '.', 'similarly', ',', 'in', 'the', 'middle', '1960s', ',', 'the', 'young', 'organizers', 'of', 'the', 'sd', '##s', 'economic', 'research', 'and', 'action', 'project', '(', 'era', '##p', ')', 'lived', 'and', 'worked', 'in', 'the', 'ghetto', '##s', 'of', 'northern', 'cities', ',', 'st', '##ri', '##ving', 'to', 'build', '\"', 'an', 'inter', '##rac', '##ial', 'movement', 'of', 'the', 'poor', '\"', 'and', 'find', 'what', 'former', 'organizer', 'sharon', 'jeffrey', 'described', 'as', '\"', 'a', 'meaning', 'of', 'life', '\"', 'that', 'would', 'be', '\"', 'personally', 'authentic', '.', '\"', 'if', 'today', \"'\", 's', 'student', 'volunteers', 'have', 'a', 'distinct', 'vision', ',', 'it', 'is', 'that', 'social', 'change', 'requires', 'broad', '-', 'based', ',', 'in', 'the', 'words', 'of', 'one', 'cool', 'staff', '##er', ',', '\"', 'you', 'do', 'n', \"'\", 't', 'have', 'to', 'believe', 'any', 'eight', 'specific', 'things', 'to', 'be', 'part', 'of', 'the', 'club', '.', '\"', 'the', 'volunteers', 'are', 'proud', 'that', 'their', 'centers', 'draw', 'in', 'everyone', 'from', 'republicans', 'to', 'socialists', '.', 'many', 'of', 'these', 'students', 'have', 'only', 'begun', 'to', 'deal', 'with', 'critical', 'questions', 'of', 'power', ',', 'conflict', ',', 'and', 'privilege', '.', 'some', 'still', 'stake', 'too', 'much', 'on', 'influencing', 'the', 'future', 'elite', ',', 'and', 'thus', 'down', '##play', 'any', 'hint', 'of', 'radical', 'dissent', '.', 'they', 'still', 'need', 'to', 'learn', 'how', 'to', 'organize', ',', 'how', 'to', 'pressure', 'en', '##tre', '##nched', 'bureau', '##cratic', 'institutions', ',', 'and', 'how', 'to', 'art', '##iculate', 'a', 'shared', 'political', 'vision', '.', 'yet', 'the', 'rest', 'of', 'us', 'would', 'do', 'well', 'to', 'treat', 'their', 'movement', 'as', 'a', 'common', 'challenge', ',', 'and', 'to', 'take', 'the', 'service', 'movements', \"'\", 'goal', 'of', 'inclusion', 'seriously', '.', 'some', 'students', 'may', 'treat', 'their', 'experiences', 'with', 'the', 'poor', 'as', 'little', 'more', 'than', 'exotic', 'tourism', '.', 'but', ',', 'over', 'time', ',', 'many', 'student', 'participants', 'have', 'come', 'to', 'acknowledge', 'the', 'necessity', 'for', 'political', 'involvement', '##s', 'that', 'would', 'have', 'terrified', 'them', 'before', '.', 'as', 'the', 'service', 'movement', 'gains', 'strength', ',', 'members', 'can', 'hee', '##d', 'the', 'lessons', 'build', 'a', 'society', 'where', 'such', 'entities', 'will', 'not', 'be', 'needed', '.', 'illustration', 'by', 'paul', 'ro', '##gat', 'lo', '##eb', 'paul', 'ro', '##gat', 'lo', '##eb', 'is', 'the', 'author', 'of', 'nuclear', 'culture', '(', 'new', 'society', 'publishers', ')', 'and', 'hope', 'in', 'hard', 'times', '(', 'lexington', 'books', ')', '.', 'he', 'is', 'writing', 'a', 'book', 'on', 'the', 'political', 'sen', '##sibility', 'of', 'today', \"'\", 's', 'college', 'students', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wywdbBISguFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXhgejEgxRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSaU7DbkgzWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUBxUm8g0ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJEqe9QDQ2A6",
        "colab_type": "code",
        "outputId": "7a4de16b-c215-4ebb-ef3f-bdef4d6e4e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(df_train), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2485 2485 2485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QPyBRDiIx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0A0HbAiMpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POlslC2AiPAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "490c7ce3-e24e-475e-c571-0de70e99c9ca"
      },
      "source": [
        "# Using Softmax activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu2c6CWSiP3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5704447d-a42a-4ec3-d08b-1f7f681b0e14"
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2236 samples, validate on 249 samples\n",
            "Epoch 1/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 1.1809e-04 - acc: 1.0000 - val_loss: 0.7248 - val_acc: 0.9076\n",
            "Epoch 2/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 7.4625e-05 - acc: 1.0000 - val_loss: 0.7633 - val_acc: 0.9036\n",
            "Epoch 3/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 5.9153e-05 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.9036\n",
            "Epoch 4/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 4.9050e-05 - acc: 1.0000 - val_loss: 0.8266 - val_acc: 0.9036\n",
            "Epoch 5/10\n",
            "2236/2236 [==============================] - 15s 6ms/step - loss: 4.2048e-05 - acc: 1.0000 - val_loss: 0.8510 - val_acc: 0.9036\n",
            "Epoch 6/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 3.6717e-05 - acc: 1.0000 - val_loss: 0.8711 - val_acc: 0.9036\n",
            "Epoch 7/10\n",
            "2236/2236 [==============================] - 13s 6ms/step - loss: 3.2580e-05 - acc: 1.0000 - val_loss: 0.8895 - val_acc: 0.8996\n",
            "Epoch 8/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 2.9235e-05 - acc: 1.0000 - val_loss: 0.9050 - val_acc: 0.8996\n",
            "Epoch 9/10\n",
            "2236/2236 [==============================] - 13s 6ms/step - loss: 2.6451e-05 - acc: 1.0000 - val_loss: 0.9202 - val_acc: 0.8996\n",
            "Epoch 10/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 2.4133e-05 - acc: 1.0000 - val_loss: 0.9329 - val_acc: 0.8996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ2NT4S8FWzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "db1ae434-82b3-413b-996c-0abe6be7c3d2"
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df_test.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df_test.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'calls', 'for', 'the', 'creation', 'of', 'a', 'no', '-', 'fly', 'zone', 'over', 'libya', 'grew', 'louder', 'in', 'the', 'united', 'states', 'as', 'civilians', 'came', 'under', 'fire', 'from', 'forces', 'loyal', 'to', 'the', 'libyan', 'leader', '.', 'col', '.', 'mu', '##am', '##mar', 'e', '##i', '-', 'q', '##ad', '##da', '##fi', ',', 'and', 'opposition', 'forces', 'suffered', 'a', 'number', 'of', 'battlefield', 'set', '##backs', '.', 'over', 'the', 'course', 'of', 'a', 'handful', 'of', 'brutal', 'days', 'in', 'early', 'march', ',', 'pro', '-', 'q', '##ad', '##da', '##fi', 'forces', 'reclaimed', 'communities', 'and', 'much', 'of', 'an', 'oil', 'facility', 'that', 'had', 'been', 'under', 'control', 'of', 'q', '##ad', '##da', '##fi', \"'\", 's', 'lightly', 'armed', 'resistance', '.', 'the', 'poorly', 'organized', 'opposition', 'force', ',', 'which', 'first', 'emerged', 'as', 'a', 'peaceful', 'protest', 'against', 'q', '##ad', '##da', '##fi', \"'\", 's', 'strong', '-', 'arm', 'rule', 'in', 'mid', '-', 'february', ',', 'came', 'under', 'heavy', 'fire', 'from', 'tanks', ',', 'helicopters', 'and', 'jet', 'fighters', 'that', 'halted', 'a', 'somewhat', 'chaotic', 'advance', 'toward', 'the', 'capital', ',', 'tripoli', '.', 'unarmed', 'protesters', 'against', 'q', '##ad', '##da', '##fi', 'rule', 'in', 'tripoli', 'were', 'met', 'with', 'automatic', 'weapons', ',', 'and', 'non', '##comb', '##ata', '##nts', 'have', 'been', 'in', 'harm', \"'\", 's', 'way', 'at', 'other', 'sites', 'of', 'conflict', 'because', 'of', 'the', 'ind', '##is', '##cr', '##imi', '##nate', 'use', 'of', 'force', 'by', 'q', '##ad', '##da', '##fi', 'loyalists', '.', 'congressional', 'leaders', ',', 'including', 'senator', 'john', 'mccain', 'of', 'arizona', ',', 'a', 'republican', ',', 'and', 'senator', 'john', 'kerry', 'of', 'massachusetts', ',', 'the', 'democrat', 'who', 'is', 'chairman', 'of', 'the', 'senate', 'foreign', 'administration', '.', 'but', 'any', 'un', '##ila', '##tera', '##l', 'u', '.', 's', '.', 'action', 'would', 'be', 'a', 'mistake', ',', 'said', 'em', '##ad', 'shah', '##in', ',', 'the', 'henry', 'r', '.', 'luce', 'associate', 'professor', 'of', 'religion', ',', 'conflict', 'and', 'peace', '##building', 'at', 'the', 'k', '##ro', '##c', 'institute', 'for', 'international', 'peace', 'studies', 'at', 'the', 'university', 'of', 'notre', 'dame', '.', '\"', 'the', 'united', 'states', 'has', 'two', 'presence', '##s', 'in', 'the', 'region', 'that', 'are', 'already', 'viewed', 'by', 'many', 'as', 'a', 'kind', 'of', 'occupation', ',', '\"', 'shah', '##in', 'said', '.', '\"', 'any', 'kind', 'of', 'conspicuous', 'or', 'heavy', 'u', '.', 's', '.', 'presence', 'in', 'this', 'conflict', 'is', 'not', 'good', 'for', 'the', 'opposition', 'and', 'it', \"'\", 's', 'not', 'good', 'for', 'the', 'united', 'states', ',', '\"', 'which', 'should', 'first', 'seek', 'the', 'endorsement', 'of', 'the', 'african', 'union', 'or', 'the', 'arab', 'league', '.', '\"', 'i', 'think', 'that', 'the', 'libyan', '##s', 'need', 'protection', 'from', 'the', 'strikes', 'being', 'launched', 'against', 'them', '\"', 'shah', '##in', 'said', '.', 'but', 'without', 'regional', 'support', ',', 'a', 'u', '.', 's', '.', 'move', 'against', 'q', '##ad', '##da', '##fi', 'would', 'del', '##eg', '##iti', '##mi', '##ze', 'the', 'resistance', 'movement', ',', 'shah', '##in', 'said', ',', 'and', 'would', 'play', 'directly', 'into', 'q', '##ad', '##da', '##fi', \"'\", 's', 'narrative', 'of', 'external', 'forces', 'manipulating', 'the', 'opposition', '.', 'a', 'un', '##ila', '##tera', '##l', 'gesture', 'would', 'also', 'further', 'weaken', 'the', 'u', '.', 's', '.', 'position', 'in', 'the', 'middle', 'east', ',', 'he', 'said', ',', 'where', '\"', 'the', 'united', 'states', 'does', 'not', 'it', 'works', 'toward', 'a', 'potential', 'no', '-', 'fly', 'zone', '\"', 'in', 'cooperation', 'with', 'other', 'states', ',', '\"', 'shah', '##in', 'said', ',', 'the', 'united', 'states', 'should', 'aggressively', 'continue', 'diplomatic', 'efforts', 'to', 'press', 'the', 'q', '##ad', '##da', '##fi', 'regime', ',', 'including', 'possible', 'recognition', 'of', 'the', 'resistance', 'forces', 'as', 'the', 'legitimate', 'government', 'of', 'libya', '.', 'the', 'united', 'states', 'could', 'also', 'do', 'more', 'to', 'respond', 'to', 'the', 'refugee', 'crisis', 'emerging', 'from', 'the', 'conflict', '.', '\"', 'i', 'think', 'this', 'is', 'as', 'far', 'as', 'they', 'can', 'go', ',', '\"', 'shah', '##in', 'said', '.', 'in', 'tripoli', 'on', 'march', '4', 'a', 'degree', 'of', 'calm', 'appeared', 'to', 'have', 'been', 'restored', 'through', 'fear', ',', 'as', 'residents', 'ceased', 'conducting', 'protests', 'against', 'the', 'regime', 'after', 'they', 'had', 'been', 'repeatedly', 'met', 'by', 'lethal', 'force', '.', 'bishop', 'giovanni', 'inn', '##oc', '##en', '##zo', 'martin', '##elli', ',', 'the', 'apostolic', 'vicar', 'of', 'tripoli', ',', 'libya', ',', 'said', ':', '\"', 'the', 'situation', 'is', 'very', 'uncertain', ',', 'and', 'for', 'the', 'moment', 'anything', 'is', 'possible', '.', 'in', 'my', 'view', ',', 'the', 'international', 'em', '##bar', '##go', 'and', 'threats', 'will', 'be', 'unlikely', 'to', 'convince', 'the', 'libyan', 'authorities', 'to', 'surrender', '.', '\"', 'i', 'believe', 'that', 'we', 'can', 'find', 'another', 'way', 'out', 'of', 'this', 'situation', ',', '\"', 'bishop', 'martin', '##elli', 'said', '.', '\"', 'and', 'it', 'is', 'not', 'difficult', 'to', 'structures', 'that', 'assist', 'reconciliation', '.', 'there', 'may', 'be', 'people', 'that', 'would', 'favor', 'a', 'turn', 'in', 'negotiating', '.', 'in', 'my', 'humble', 'opinion', ',', 'it', 'is', 'the', 'only', 'way', 'to', 'sur', '##pass', 'the', 'crisis', 'and', 'prevent', 'more', 'blood', '##shed', '.', 'there', 'are', 'better', 'ways', 'than', 'with', 'violence', '.', '\"', 'meanwhile', ',', 'a', 'car', '##itas', 'international', '##is', 'team', 'was', 'at', 'work', 'on', 'the', 'egypt', '##lib', '##ya', 'border', 'in', 'sal', '##lou', '##m', ',', 'where', 'thousands', 'of', 'foreign', 'workers', 'fleeing', 'the', 'violence', 'had', 'been', 'stranded', '.', 'around', '5', ',', '000', 'more', 'were', 'arriving', 'daily', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVTDXXaMEb6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65ec9df3-d7d3-40eb-da9c-6a9b575bb90d"
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "514/514 [==============================] - 1s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfDBWCm9HH-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0bd011e-115d-455b-df94-b0037b7c0154"
      },
      "source": [
        "evaluate_lstm\n",
        "#returns loss value & metrics values"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.8203000056938976, 0.8035019455252919]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DjqUvYzWHu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "53326cff-eee0-4f72-92bd-8a31c132e6d8"
      },
      "source": [
        "# Using Sigmoid Activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='sigmoid'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 128, 128)          3906816   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               91600     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 3,998,618\n",
            "Trainable params: 3,998,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmUaXaQEWHvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d1f2dc22-cdae-433e-ff29-d4a5b7cc67a8"
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2236 samples, validate on 249 samples\n",
            "Epoch 1/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 0.3242 - acc: 0.9217 - val_loss: 0.2705 - val_acc: 0.9237\n",
            "Epoch 2/10\n",
            "2236/2236 [==============================] - 13s 6ms/step - loss: 0.2543 - acc: 0.9293 - val_loss: 0.2803 - val_acc: 0.9237\n",
            "Epoch 3/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 0.2354 - acc: 0.9293 - val_loss: 0.3083 - val_acc: 0.9237\n",
            "Epoch 4/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 0.1140 - acc: 0.9517 - val_loss: 0.4009 - val_acc: 0.9076\n",
            "Epoch 5/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 0.0201 - acc: 0.9928 - val_loss: 0.5101 - val_acc: 0.8996\n",
            "Epoch 6/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.5673 - val_acc: 0.8956\n",
            "Epoch 7/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 0.0040 - acc: 0.9982 - val_loss: 0.6022 - val_acc: 0.9157\n",
            "Epoch 8/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 6.8797e-04 - acc: 1.0000 - val_loss: 0.6239 - val_acc: 0.9116\n",
            "Epoch 9/10\n",
            "2236/2236 [==============================] - 14s 6ms/step - loss: 3.1914e-04 - acc: 1.0000 - val_loss: 0.6579 - val_acc: 0.9036\n",
            "Epoch 10/10\n",
            "2236/2236 [==============================] - 13s 6ms/step - loss: 2.2256e-04 - acc: 1.0000 - val_loss: 0.6848 - val_acc: 0.9036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2jvW92WFuA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afaf15a5-c999-4b76-8127-738898426c18"
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)\n",
        "evaluate_lstm"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "514/514 [==============================] - 1s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5126004551924856, 0.8015564202334631]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSixR6Ql_DqO",
        "colab_type": "text"
      },
      "source": [
        "## Train and test for each year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RwaJAd8L60q",
        "colab_type": "text"
      },
      "source": [
        "#### Period 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ayf4eZjLRLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "915f2442-959f-4f2a-842e-4887021a05f8"
      },
      "source": [
        "period0 = data[data.period==0]\n",
        "period0"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>section : movements from socialist to republi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>inside a dusty cement-block house with worn ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>even if saddam hussein fails to get his way ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>section : spectator 's journal  dateline : li...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>soviet jewry is giving israel new life -- bu...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2477</th>\n",
              "      <td>they signed up with human smugglers in their...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2478</th>\n",
              "      <td>moscow -- russia 's disastrous war in chechn...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2479</th>\n",
              "      <td>robert s. mcnamara 's recently expressed reg...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480</th>\n",
              "      <td>the stories that fill our lives rarely make ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2481</th>\n",
              "      <td>the mission was simple .  find out what denv...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>947 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "3      section : movements from socialist to republi...     0       0\n",
              "4       inside a dusty cement-block house with worn ...     0       0\n",
              "6       even if saddam hussein fails to get his way ...     1       0\n",
              "7      section : spectator 's journal  dateline : li...     1       0\n",
              "8       soviet jewry is giving israel new life -- bu...     1       0\n",
              "...                                                 ...   ...     ...\n",
              "2477    they signed up with human smugglers in their...     0       0\n",
              "2478    moscow -- russia 's disastrous war in chechn...     0       0\n",
              "2479    robert s. mcnamara 's recently expressed reg...     0       0\n",
              "2480    the stories that fill our lives rarely make ...     0       0\n",
              "2481    the mission was simple .  find out what denv...     0       0\n",
              "\n",
              "[947 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZGQ0msjSx75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb56ff8f-401c-460b-c7e7-87699c73dec1"
      },
      "source": [
        "period0.label.sum()/len(period0)\n",
        "# number of right wing media"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06019007391763464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLXGWSoENU9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "period0_train, period0_test = train_test_split(period0, random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CnCVzaLN8YP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b727164f-ddfb-4e4d-e111-1d6b7514a20f"
      },
      "source": [
        "print(len(period0_train), len(period0_test))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "899 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k4BntgwK_pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f423f53b-a4a9-42fd-de8e-1d925230b73a"
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period0_train.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period0_train.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'prime', 'minister', 'yi', '##tz', '##hak', 'sham', '##ir', 'stressed', 'today', 'that', 'israel', 'should', 'seize', 'the', 'opportunity', 'to', 'attend', 'the', 'american', '-', 'designed', 'peace', 'conference', ',', 'but', 'only', 'if', 'no', 'palestinians', 'from', 'east', 'jerusalem', 'or', 'with', 'any', 'connection', 'to', 'the', 'palestine', 'liberation', 'organization', 'will', 'be', 'there', '.', 'the', 'prime', 'minister', 'said', 'that', 'if', 'it', 'turns', 'out', 'that', 'no', 'palestinians', 'are', 'prepared', 'to', 'come', 'to', 'a', 'conference', 'on', 'such', 'terms', ',', 'israel', 'would', 'be', 'willing', 'to', 'negotiate', 'directly', 'with', 'any', 'arab', 'country', '.', 'but', 'he', 'caution', '##ed', 'both', 'syria', 'and', 'jordan', 'that', 'they', 'must', 'not', 'expect', 'to', 'get', 'any', 'land', 'for', 'peace', '-', '-', 'only', 'peace', 'for', 'peace', '.', 'the', 'thrust', 'of', 'mr', '.', 'sham', '##ir', \"'\", 's', 'remarks', ',', 'as', 'well', 'as', 'those', 'made', 'by', 'palestinians', 'today', ',', 'suggested', 'that', 'the', 'bush', 'administration', \"'\", 's', 'initial', 'optimism', 'that', 'a', 'peace', 'conference', 'could', 'be', 'announced', 'when', 'the', 'president', 'meets', 'with', 'president', 'mikhail', 's', '.', 'go', '##rba', '##chev', 'in', 'moscow', 'next', 'week', 'may', 'be', 'premature', '.', 'dispute', 'over', 'representation', 'it', 'appears', 'that', 'neither', 'israelis', 'nor', 'palestinians', 'are', 'prepared', 'to', 'compromise', 'on', 'the', 'long', '##standing', 'issue', 'of', 'who', 'will', 'represent', 'the', 'palestinians', '.', 'and', 'until', 'that', 'is', 'resolved', ',', 'it', 'is', 'difficult', 'to', 'imagine', ',', 'mr', '.', 'sham', '##ir', 'held', 'out', 'no', 'hope', 'to', 'the', 'syrian', '##s', 'and', 'jordanian', '##s', 'that', 'if', 'they', 'engaged', 'in', 'direct', 'talks', 'about', '\"', 'real', 'and', 'true', 'peace', ',', '\"', 'they', 'had', 'a', 'chance', 'to', 'regain', 'any', 'of', 'the', 'go', '##lan', 'heights', 'or', 'west', 'bank', 'occupied', 'by', 'israel', 'since', 'the', '1967', 'war', '.', 'the', 'prime', 'minister', ',', 'questioned', 'in', 'a', 'television', 'interview', ',', 'said', ':', '\"', 'i', 'do', 'not', 'believe', 'in', 'territorial', 'compromise', '.', 'our', 'country', 'is', 'very', 'small', '.', 'where', 'would', 'you', 'find', 'among', 'the', 'nations', 'of', 'the', 'world', 'a', 'people', 'who', 'would', 'be', 'ready', 'to', 'give', 'up', 'the', 'territory', 'of', 'their', 'homeland', '?', \"'\", 'it', 'all', 'goes', 'together', \"'\", '\"', 'i', 'believe', 'with', 'all', 'my', 'heart', 'and', 'soul', 'that', 'we', 'are', 'eternal', '##ly', 'tied', 'to', 'this', 'homeland', '.', 'peace', 'and', 'security', 'go', 'together', '.', 'security', ',', 'and', 'a', 'territory', ',', 'a', 'homeland', '-', '-', 'it', 'all', 'goes', 'together', '.', 'that', 'is', 'our', 'belief', ',', 'that', 'is', 'the', 'belief', 'of', 'the', 'party', 'i', 'belong', 'to', 'and', 'in', 'my', 'opinion', ',', 'that', 'is', 'the', 'feeling', 'of', 'a', 'large', 'majority', 'of', 'the', 'jewish', 'nation', '.', '\"', 'the', 'conditions', 'that', 'mr', '.', 'sham', '##ir', 'laid', 'down', 'were', 'several', 'reasons', '.', 'first', ',', 'they', 'marked', 'his', 'first', 'public', 'comments', 'in', 'hebrew', 'in', 'reaction', 'to', 'the', 'news', 'brought', 'sunday', 'by', 'secretary', 'of', 'state', 'james', 'a', '.', 'baker', '3d', 'that', 'syria', ',', 'jordan', 'and', 'lebanon', 'had', 'all', 'agreed', 'to', 'take', 'part', 'in', 'a', 'peace', 'conference', 'with', 'israel', '.', 'up', 'to', 'now', ',', 'mr', '.', 'sham', '##ir', 'has', 'been', 'quoted', 'by', 'aide', '##s', 'as', 'saying', 'optimistic', 'things', 'in', 'private', 'or', 'to', 'various', 'visiting', 'american', 'groups', '.', 'but', 'when', 'it', 'came', 'to', 'addressing', 'his', 'own', 'constituency', 'in', 'hebrew', ',', 'it', 'was', 'clear', 'that', 'the', 'news', 'that', 'syria', 'was', 'ready', 'to', 'attend', 'a', 'peace', 'conference', 'that', 'would', 'open', 'the', 'way', 'for', 'the', 'first', 'direct', 'talks', 'with', 'israel', 'was', 'not', 'something', 'so', 'en', '##tic', '##ing', 'to', 'mr', '.', 'sham', '##ir', 'that', 'he', 'is', 'prepared', 'to', 'drop', 'his', 'conditions', 'on', 'palestinian', 'representation', ',', 'or', 'to', 'suggest', 'even', 'the', 'remote', '##st', 'hope', 'of', 'a', 'territorial', 'compromise', '.', 'separate', 'deal', 'with', 'syria', '?', 'second', ',', 'his', 'comments', 'were', 'important', 'because', 'they', 'suggested', 'that', 'mr', '.', 'sham', '##ir', 'already', 'realizes', 'that', 'it', 'may', 'be', 'impossible', 'to', 'bridge', 'the', 'gap', 'between', 'israel', 'and', 'the', 'palestinians', 'over', 'how', 'the', 'palestinians', 'should', 'be', 'represented', ',', 'and', 'he', 'is', 'already', 'trying', 'struck', 'with', 'egypt', 'in', '1978', 'at', 'camp', 'david', ',', 'md', '.', '-', '-', 'but', 'without', 'offering', 'the', 'syrian', '##s', 'a', 'chance', 'to', 'recover', 'their', 'occupied', 'land', ',', 'as', 'the', 'egyptians', 'recovered', 'sinai', '.', 'there', 'is', 'no', 'indication', 'at', 'this', 'stage', 'that', 'the', 'syrian', '##s', 'would', 'accept', 'such', 'a', 'separate', 'negotiation', '.', 'they', 'have', 'said', 'the', 'opposite', 'in', 'public', '.', 'thus', ',', 'if', 'mr', '.', 'baker', 'does', 'not', 'find', 'a', 'way', 'to', 'resolve', 'the', 'question', 'of', 'palestinian', 'representation', ',', 'it', 'is', 'hard', 'to', 'see', 'how', 'a', 'conference', 'can', 'begin', '.', 'the', 'israelis', 'say', 'they', 'will', 'not', 'formally', 'agree', 'to', 'attend', 'until', 'they', 'get', 'a', 'list', 'of', 'palestinians', 'from', 'the', 'united', 'states', '.', 'american', 'officials', 'say', 'they', 'are', 'waiting', 'for', 'the', 'jordanian', '##s', 'and', 'palestinians', 'to', 'agree', 'on', 'a', 'joint', 'delegation', '.', 'mr', '.', 'baker', ',', 'when', 'he', 'spoke', 'with', 'three', 'palestinian', 'representatives', 'in', 'jerusalem', 'on', 'sunday', ',', 'told', 'them', 'that', 'while', 'he', 'sy', '##mp', '##athi', '##zed', 'with', 'their', 'plight', ',', 'the', 'realities', 'of', 'power', 'in', 'the', 'mid', '##ea', '##st', 'now', 'were', 'such', 'that', 'if', 'they', 'wanted', 'to', 'take', 'part', 'in', 'negotiations', ',', 'they', 'would', 'have', 'to', 'essentially', 'accept', 'the', 'israeli', 'conditions', '.', 'baker', 'proposal', 'rejected', 'he', 'tried', 'to', 'make', 'it', 'east', 'jerusalem', 'palestinians', 'would', 'be', 'in', 'the', 'opening', 'round', 'of', 'talks', ',', 'which', 'will', 'be', 'limited', 'to', 'the', 'question', 'of', 'autonomy', 'for', 'the', 'palestinians', 'in', 'the', 'west', 'bank', 'and', 'gaza', 'strip', '.', 'but', 'they', 'can', 'be', 'in', 'discussions', 'later', ',', 'when', 'the', 'final', 'status', 'of', 'the', 'territories', 'will', 'be', 'decided', '.', 'the', 'israelis', 'rejected', 'this', 'idea', ',', 'as', 'did', 'the', 'palestinians', '.', '\"', 'we', 'told', 'mr', '.', 'baker', 'that', 'the', 'united', 'states', 'does', 'not', 'recognize', 'the', 'israeli', 'annexation', 'of', 'east', 'jerusalem', ',', 'so', 'how', 'does', 'he', 'expect', 'us', 'to', '?', '\"', 'said', 'hana', '##n', 'ash', '##ra', '##wi', ',', 'one', 'of', 'the', 'west', 'bank', 'palestinians', 'who', 'has', 'been', 'meeting', 'with', 'mr', '.', 'baker', '.', '\"', 'there', 'is', 'no', 'palestinian', '-', '-', 'credible', 'or', 'incredible', '-', '-', 'who', 'will', 'be', 'prepared', 'to', 'take', 'part', 'in', 'a', 'peace', 'conference', 'in', 'which', 'no', 'palestinians', 'from', 'east', 'jerusalem', 'are', 'allowed', ',', '\"', 'he', 'said', '.', '\"', 'it', 'is', 'a', 'matter', 'of', 'substance', '.', 'it', 'means', 'accepting', 'the', 'israeli', 'annexation', '.', '\"', 'in', 'a', 'parallel', 'diplomatic', 'move', ',', 'the', 'french', 'foreign', 'ministry', 'announced', 'today', 'that', 'fai', '##sal', 'al', '-', 'hussein', '##i', ',', 'the', 'other', 'leading', 'west', 'bank', 'palestinian', 'involved', 'in', 'talks', 'with', 'mr', '.', 'baker', ',', 'foreign', 'minister', ',', 'roland', 'du', '##mas', ',', 'on', 'the', 'palestinian', 'representation', 'issue', '.', 'french', 'help', 'sought', 'palestinians', 'say', 'mr', '.', 'hussein', '##i', 'will', 'try', 'to', 'enlist', 'french', 'help', 'to', 'soft', '##en', 'the', 'american', 'and', 'israeli', 'positions', ',', 'something', 'that', 'will', 'not', 'be', 'greeted', 'warmly', 'by', 'mr', '.', 'baker', ',', 'who', 'has', 'had', 'a', 'monopoly', 'up', 'to', 'now', 'on', 'the', 'diplomacy', '.', 'mr', '.', 'sham', '##ir', 'made', 'it', 'quite', 'clear', 'that', 'for', 'him', 'this', 'issue', 'was', 'non', '-', 'ne', '##go', '##tia', '##ble', '.', 'he', 'will', 'only', 'talk', 'to', 'palestinians', 'from', 'the', 'west', 'bank', 'and', 'gaza', 'strip', ',', 'or', 'to', 'palestinian', 'refugees', 'living', 'in', 'jordan', 'and', 'carrying', 'jordanian', 'passports', '.', 'to', 'talk', 'to', 'palestinians', 'from', 'east', 'jerusalem', 'or', 'palestinian', 'refugees', 'from', 'abroad', ',', 'he', 'argues', ',', 'would', 'be', 'to', 'admit', 'that', 'east', 'jerusalem', 'is', 'ne', '##go', '##tia', '##ble', 'and', 'that', 'palestinian', 'refugees', 'have', 'a', 'right', 'to', 'return', 'to', 'their', 'homes', 'in', 'israel', 'as', 'the', 'country', 'existed', 'before', 'the', '1967', 'war', '.', 'in', 'mr', '.', 'sham', '##ir', \"'\", 's', 'view', 'there', 'are', 'only', 'two', 'solutions', 'for', 'the', 'palestinians', '.', 'one', 'is', 'for', 'those', 'living', 'in', 'the', 'west', 'bank', 'and', 'gaza', 'to', 'accept', 'an', 'autonomy', 'arrangement', 'that', 'recognizes', 'israeli', 'rule', 'and', 'the', 'other', 'is', 'for', 'those', 'living', 'in', 'jordan', 'jordan', 'to', 'reach', 'a', 'peace', 'agreement', 'between', 'them', '.', \"'\", 'this', 'is', 'our', 'right', \"'\", '\"', 'we', 'must', 'know', 'who', 'are', 'the', 'people', 'who', 'will', 'compose', 'the', 'palestinian', 'delegation', 'within', 'the', 'palestinian', '-', 'jordanian', 'delegation', ',', '\"', 'the', 'prime', 'minister', 'said', '.', '\"', 'this', 'is', 'our', 'right', '.', 'under', 'no', 'circumstances', 'can', 'they', 'be', 'from', 'east', 'jerusalem', '.', 'jerusalem', ',', 'united', 'in', 'its', 'complete', '##ness', ',', 'is', 'the', 'capital', 'of', 'israel', '.', '\"', 'the', 'palestinians', 'reject', 'such', 'conditions', ',', 'asking', ':', 'in', 'what', 'negotiation', 'in', 'history', 'can', 'one', 'side', 'demand', 'to', 'choose', 'the', 'other', \"'\", 's', 'delegation', '?', '\"', 'they', 'do', 'n', \"'\", 't', 'agree', 'with', 'us', 'and', 'for', 'now', ',', 'because', 'of', 'this', ',', 'there', 'is', 'no', 'palestinian', '-', 'jordanian', 'delegation', ',', '\"', 'mr', '.', 'sham', '##ir', 'said', '.', '\"', 'we', 'know', 'this', 'process', 'will', 'not', 'begin', 'until', 'all', 'the', 'parties', 'agree', 'to', 'take', 'part', 'in', 'it', '.', 'but', 'you', 'know', 'something', ':', 'i', 'am', 'prepared', 'to', 'negotiate', 'only', 'with', 'syria', ',', 'with', 'all', 'the', 'arab', 'states', '.', 'i', 'do', 'n', \"'\", 't', 'need', 'them', 'all', '.', '\"', 'asked', 'if', 'that', 'meant', 'he', 'was', 'was', 'ready', 'to', 'go', 'to', 'a', 'conference', 'that', 'did', 'not', 'have', 'palestinians', 'there', ',', 'concerned', ',', 'yes', '.', '\"', 'a', 'package', 'deal', 'the', 'problem', 'the', 'united', 'states', 'now', 'faces', ',', 'though', ',', 'is', 'that', 'it', 'has', 'presented', 'the', 'peace', 'conference', ',', 'and', 'sold', 'it', 'in', 'the', 'arab', 'world', 'and', 'israel', ',', 'as', 'a', 'package', '.', 'politically', ',', 'the', 'arab', 'countries', 'need', 'the', 'palestinians', 'there', 'to', 'cover', 'their', 'own', 'direct', 'talks', 'with', 'israel', '.', 'it', 'would', 'be', 'very', 'difficult', 'now', 'for', 'the', 'united', 'states', 'and', 'the', 'soviet', 'union', ',', 'the', 'co', '-', 'sponsors', 'of', 'the', 'conference', ',', 'to', 'simply', 'issue', 'invitations', ',', 'as', 'mr', '.', 'bush', 'and', 'mr', '.', 'go', '##rba', '##chev', 'were', 'hoping', 'to', 'do', 'at', 'their', 'summit', 'meeting', 'in', 'moscow', 'that', 'begins', 'tuesday', '.', 'first', ',', 'they', 'have', 'no', 'one', 'to', 'send', 'the', 'palestinian', 'invitation', 'to', 'at', 'this', 'point', ',', 'and', 'second', ',', 'mr', '.', 'sham', '##ir', 'warned', 'them', 'not', 'to', 'try', 'to', 'paint', 'him', 'into', 'a', 'corner', 'by', 'demanding', 'that', 'he', 'agree', 'to', 'the', 'conference', 'and', 'leave', 'the', 'detail', 'of', 'palestinian', 'representation', 'to', 'be', 'worked', 'out', 'before', 'the', 'talks', 'open', 'in', 'october', '.', 'mr', '.', 'sham', '##ir', 'said', 'mr', '.', 'baker', 'explained', 'to', 'him', 'that', 'the', 'syrian', '##s', 'had', 'decided', 'to', 'accept', 'the', 'conference', 'out', 'of', 'recognition', 'that', 'they', 'had', 'to', 'now', 'that', 'the', 'soviet', 'union', 'was', 'in', 'decline', 'and', 'america', 'was', 'the', 'world', \"'\", 's', 'only', 'real', 'super', '##power', '.', 'president', 'anwar', 'el', '-', 'sad', '##at', 'of', 'egypt', 'was', 'also', 'driven', 'to', 'break', 'from', 'the', 'arab', 'consensus', 'and', 'offer', 'peace', 'with', 'israel', 'in', 'the', 'late', '1970', \"'\", 's', 'out', 'of', 'a', 'desire', 'to', 'gain', 'american', 'favor', 'and', 'aid', ',', 'mr', '.', 'sham', '##ir', 'indicated', ',', 'but', 'he', 'said', 'there', 'was', 'one', 'important', 'difference', '.', '\"', 'sad', '##at', ',', '\"', 'the', 'prime', 'minister', 'said', ',', '\"', 'started', 'his', 'move', 'with', 'an', 'appeal', 'to', 'israel', ',', 'not', 'to', 'the', 'united', 'states', '.', '\"', '\"', 'he', 'came', 'to', 'jerusalem', ',', 'to', 'the', 'knesset', ',', 'and', 'said', 'to', 'the', 'jewish', 'nation', ':', \"'\", 'i', 'want', 'peace', 'with', 'you', ',', 'i', 'understand', 'your', 'security', 'problems', ',', \"'\", '\"', 'mr', '.', 'sham', '##ir', 'said', '.', '\"', 'ass', '##ad', 'has', 'not', 'said', 'that', 'up', 'to', 'today', '.', 'he', 'turned', 'first', 'of', 'all', 'to', 'the', 'americans', ',', 'and', 'only', 'the', 'americans', '.', 'up', 'to', 'now', 'he', 'has', 'not', 'spoken', 'at', 'all', 'about', 'peace', '.', 'he', 'talks', 'about', \"'\", 'resolving', 'the', 'dispute', '.', \"'\", 'it', 'is', 'not', 'the', 'same', 'thing', '.', '\"', 'correction', '-', 'date', ':', 'july', '26', ',', '1991', ',', 'an', 'article', 'yesterday', 'about', 'israeli', 'remarks', 'on', 'a', 'regional', 'peace', 'conference', 'referred', 'incorrectly', 'in', 'some', 'editions', 'to', 'hana', '##n', 'ash', '##ra', '##wi', ',', 'a', 'west', 'bank', 'palestinian', '.', 'she', 'is', 'a', 'woman', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s3kh-Z9_MXpV",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "946a0e61-b515-458c-e36b-0026729e95c6",
        "id": "TBvKvyBGMXpa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period0_train), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "899 899 899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d56lK4LLMXpe",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.05)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NhisdbyPYr6",
        "colab_type": "text"
      },
      "source": [
        "###### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8vumphXOggm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "926cce70-43d7-4778-8a73-6b1ade89daa4",
        "id": "QflUDVEKOrqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Using Sigmoid Activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='sigmoid'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d195bcd2-da4a-472c-8718-20b7c1e03e1d",
        "id": "LtjhoTROOrqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 854 samples, validate on 45 samples\n",
            "Epoch 1/10\n",
            "854/854 [==============================] - 6s 8ms/step - loss: 0.4396 - acc: 0.9157 - val_loss: 0.0342 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.2400 - acc: 0.9379 - val_loss: 0.0456 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.2338 - acc: 0.9379 - val_loss: 0.0564 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.2337 - acc: 0.9379 - val_loss: 0.0915 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.2308 - acc: 0.9379 - val_loss: 0.0591 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.2184 - acc: 0.9379 - val_loss: 0.0705 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "854/854 [==============================] - 6s 6ms/step - loss: 0.2067 - acc: 0.9379 - val_loss: 0.0781 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.1900 - acc: 0.9379 - val_loss: 0.0680 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.1646 - acc: 0.9379 - val_loss: 0.0569 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "854/854 [==============================] - 5s 6ms/step - loss: 0.1247 - acc: 0.9379 - val_loss: 0.0481 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjj2Qqs2PLhq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5f719b98-85c7-404e-d54d-05492670fdad"
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period0_test.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period0_test.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'the', 'first', 'universal', 'nation', ':', 'leading', 'indicators', 'and', 'ideas', 'about', 'the', 'surge', 'of', 'america', 'in', 'the', '1990s', 'ben', 'j', '.', 'watt', '##enberg', '/', 'the', 'free', 'press', '/', '41', '##8', 'pp', '.', '$', '22', '.', '95', 'there', 'are', 'only', 'two', 'nations', 'on', 'this', 'earth', 'that', 'are', 'simultaneously', 'fantasies', 'and', 'realities', ',', 'those', 'two', 'being', ',', 'of', 'course', ',', 'the', 'tremendous', 'united', 'states', 'of', 'america', 'and', 'tiny', 'israel', '.', 'maybe', 'that', 'is', 'what', 'makes', 'them', ',', 'even', 'when', 'they', \"'\", 're', 'wr', '##ang', '##ling', ',', 'such', 'a', 'nice', 'couple', '.', 'if', 'america', 'con', '##notes', 'new', 'beginnings', ',', 'fresh', 'starts', 'for', 'one', 'and', 'all', ',', 'the', 'meanings', 'of', 'zion', 'are', 'too', 'well', 'known', 'to', 'mention', '.', 'the', 'motto', 'of', 'both', 'of', 'these', 'very', 'peculiar', 'u', '.', 'n', '.', 'members', 'might', 'be', ',', '\"', 'look', 'up', ',', 'better', 'yet', 'come', 'to', 'me', '.', '\"', 'in', 'fact', ',', 'america', 'has', 'sometimes', 'been', 'pictured', 'as', 'a', 'bigger', 'if', 'not', 'better', 'version', 'of', 'zion', ',', 'for', 'example', 'by', 'john', 'winthrop', ',', 'who', 'is', 'quoted', 'in', 'ben', 'watt', '##enberg', \"'\", 's', 'happy', 'new', 'book', 'as', 'comparing', 'massachusetts', 'to', '\"', 'a', 'city', 'upon', 'a', 'hill', '.', '\"', 'put', 'another', 'way', ',', 'just', 'as', 'there', 'are', 'two', 'jerusalem', '##s', ',', 'the', 'heavenly', 'and', 'the', 'earthly', ',', 'not', 'invariably', 'falling', 'somewhat', 'short', 'of', 'the', 'ideal', '.', 'watt', '##enberg', 'is', 'an', 'american', 'who', 'came', 'this', 'close', 'to', 'being', 'an', 'israeli', '.', 'it', 'was', 'because', 'his', 'mother', 'and', 'father', 'left', 'what', 'was', 'then', 'palestine', 'in', 'the', '1920s', 'that', 'he', 'could', 'become', 'a', 'model', 'of', 'that', 'type', 'of', 'first', '-', 'generation', 'american', 'whose', 'pride', 'and', 'belief', 'in', 'his', 'native', 'country', 'is', 'permanent', 'and', 'never', ',', 'ever', 'a', 'secret', '.', 'for', 'years', 'now', ',', 'watt', '##enberg', 'has', 'been', 'airing', 'his', 'views', 'by', 'means', 'of', 'tv', 'appearances', ',', 'his', 'newspaper', 'column', ',', 'and', 'books', '.', 'he', 'also', 'has', 'an', 'appointment', 'at', 'the', 'american', 'enterprise', 'institute', ',', 'one', 'of', 'the', 'too', '-', 'many', 'think', '-', 'tanks', 'in', 'washington', 'that', 'chu', '##rn', 'out', 'position', 'papers', 'for', 'presidential', 'hopeful', '##s', 'to', 'brood', 'on', '.', 'ae', '##i', 'was', 'at', 'one', 'time', 'something', 'special', ',', 'helping', 'give', 'intellectual', 'cache', '##t', 'to', 'the', 'reagan', 'revolution', 'at', 'home', 'and', 'abroad', '.', 'not', 'that', 'watt', '##enberg', 'is', 'a', 'republican', '-', '-', 'like', 'many', 'of', 'the', 'patriots', 'who', 'backed', 'and', 'guided', 'reagan', ',', 'especially', 'the', 'jews', 'among', 'them', ',', 'he', \"'\", 's', 'a', 'refugee', 'from', 'the', 'democratic', 'party', 'who', 'still', 'pray', '##s', 'that', 'it', 'will', 'someday', 'come', 'to', 'its', 'senses', '.', 'in', 'the', 'meantime', ',', 'he', 'continues', 'justify', '##ing', 'his', 'feelings', 'with', 'statistics', '.', 'this', 'is', 'what', 'he', 'does', 'also', 'in', 'his', 'new', 'book', '.', 'it', \"'\", 's', 'a', 'collection', 'of', 'columns', 'and', 'speeches', 'from', 'the', 'day', 'reagan', 'took', 'office', 'through', 'the', 'day', 'saddam', 'took', 'kuwait', ',', 'always', 'chat', '##ty', ',', 'always', 'upbeat', ',', 'always', 'contrary', 'to', 'the', 'conventional', 'liberal', 'wisdom', ',', 'sp', '##lice', '##d', 'together', 'by', 'commentary', ',', 'autobiography', ',', 'and', 'lots', 'of', 'numbers', '.', '\"', 'some', '65', 'percent', 'of', 'americans', ',', '\"', 'we', 'learn', ',', '\"', 'believe', 'there', 'is', 'a', 'personal', 'god', ',', 'compared', 'with', '.', '.', '.', '19', 'percent', 'of', 'sw', '##edes', '.', '\"', 'watt', '##enberg', \"'\", 's', 'point', 'is', 'that', 'americans', 'are', 'different', ',', 'and', ',', 'yes', ',', 'better', ',', 'deserved', '##ly', 'still', 'the', 'envy', ',', 'ideal', ',', 'and', 'hope', 'of', 'the', 'world', '.', 'reprinted', 'here', 'are', 'prophet', '##ic', 'items', 'on', 'the', 'collapse', 'of', 'communism', 'and', 'how', 'america', 'can', 'haste', '##n', 'it', '.', 'there', \"'\", 's', 'something', 'of', 'a', 'defense', 'of', 'ollie', 'north', ',', 'and', 'an', 'accurate', 'prediction', 'of', 'the', 'fate', 'of', 'the', 'sand', '##ini', '##sta', '##s', 'at', 'the', 'ballot', 'box', '.', 'there', 'are', 'statistics', 'according', 'to', 'which', 'poverty', 'in', 'america', 'is', 'down', ',', 'income', 'up', ',', 'and', 'the', 'high', 'school', 'drop', '-', 'out', 'rate', 'at', 'an', 'all', '-', 'time', 'low', '.', 'confident', 'than', 'ever', '.', 'and', 'predictions', 'that', 'the', 'best', 'is', 'yet', 'to', 'come', 'for', 'the', '\"', 'first', 'universal', 'nation', '.', '\"', 'the', 'term', 'is', 'watt', '##enberg', \"'\", 's', 'alias', 'for', 'his', 'country', 'on', 'the', 'morrow', 'of', 'the', 'cold', 'war', '.', 'having', 'won', 'it', ',', 'the', 'u', '.', 's', '.', 'is', 'en', '##sco', '##nce', '##d', '-', '-', 'politically', ',', 'mil', '##ita', '##rily', ',', 'and', ',', 'most', 'important', ',', 'culturally', '-', '-', 'as', '\"', 'number', 'one', '\"', 'everywhere', ',', 'to', 'use', 'his', 'language', '.', 'even', 'when', 'it', 'comes', 'to', 'making', 'and', 'selling', ',', 'he', \"'\", 's', 'not', 'depressed', 'by', 'the', 'competition', '.', 'his', 'figures', 'say', 'that', 'the', 'american', 'birth', '##rate', 'is', 'rising', ',', 'but', 'that', 'western', 'europe', \"'\", 's', 'is', 'stuck', 'at', 'below', 'replacement', 'level', 'and', 'japan', \"'\", 's', 'is', 'below', 'r', '##l', 'and', 'tumbling', '.', 'a', 'high', 'birth', '##rate', 'and', 'large', '-', 'scale', 'immigration', 'ensure', 'american', 'economic', 'supremacy', ',', 'because', 'shrinking', ',', 'aging', 'populations', 'ca', 'n', \"'\", 't', 'compete', 'or', 'grow', '.', 'not', 'that', 'watt', '##enberg', 'wants', 'the', 'readers', 'of', 'his', 'book', 'to', 'relax', '-', '-', 'this', 'is', 'a', 'pep', 'talk', '.', 'far', 'from', 'pulling', 'in', 'its', 'horns', ',', 'as', 'pat', 'buchanan', 'would', 'have', 'it', 'do', ',', 'victorious', 'america', 'should', 'embrace', 'its', 'destiny', 'an', '##ew', '.', 'this', 'means', 'continuing', 'to', 'push', 'for', 'democracy', 'worldwide', ',', 'but', 'way', 'of', 'life', 'as', 'well', 'as', 'a', 'political', 'system', ',', 'is', 'not', 'the', 'same', 'as', ',', 'say', ',', 'the', 'pinched', 'stu', '##lt', '##ification', 'of', 'swedish', '##sty', '##le', 'democracy', '.', 'and', 'in', 'eastern', 'europe', 'today', ',', 'it', \"'\", 's', 'said', 'that', 'the', 'newly', 'free', 'countries', 'should', 'go', 'swedish', '.', 'american', 'taxpayers', 'did', 'n', \"'\", 't', 'spend', 'trillion', '##s', 'on', 'the', 'cold', 'war', 'to', 'create', 'more', 'sweden', '##s', '.', '.', '.', '.', 'as', 'the', 'last', 'super', '##power', 'we', 'ought', 'to', 'try', 'to', 'shape', 'evolution', '.', 'watt', '##enberg', 'is', 'sang', '##uin', '##e', 'that', 'america', 'can', 'do', 'this', 'if', 'it', 'even', 'half', '-', 'tries', ',', 'because', 'culturally', 'the', 'u', '.', 's', '.', 'is', 'un', '##ri', '##vale', '##d', ',', 'and', '\"', 'it', \"'\", 's', 'culture', 'that', 'counts', '.', '\"', 'while', 'the', 'japanese', 'may', 'make', 'and', 'sell', 'all', 'the', 'vc', '##rs', ',', 'almost', 'all', 'the', 'software', 'is', 'red', '##w', '##hit', '##e', '-', 'and', '-', 'blue', ',', 'and', 'it', \"'\", 's', 'ko', '##jak', 'and', 'madonna', 'who', 'are', 'members', 'of', 'the', 'family', 'on', 'every', 'continent', '.', 'the', 'fact', 'that', 'the', 'culture', 'exported', 'by', 'the', 'u', '.', 's', '.', 'to', 'a', 'fascinated', 'and', 'ins', '##ati', '##able', 'universe', 'is', 'mostly', 'garbage', 'does', 'n', \"'\", 't', 'matter', '.', 'what', \"'\", 's', 'important', 'is', 'that', 'the', 'sounds', 'and', 'images', 'convey', 'the', 'message', 'of', 'freedom', ',', 'of', 'infinite', 'tolerance', 'and', 'opportunity', ',', 'the', 'opposite', 'of', 'stu', '##lt', '##ification', '.', 'the', 'whole', 'world', 'seems', 'to', 'be', 'getting', 'the', 'message', 'the', 'cold', 'war', '.', 'the', 'ongoing', 'proof', 'is', ',', 'at', 'one', 'end', 'of', 'the', 'continuum', ',', 'all', 'the', 'foreign', 'imitation', '##s', 'of', 'american', 'sc', '##hl', '##ock', ',', 'and', 'at', 'the', 'other', ',', 'the', '\"', 'foreigners', 'flock', '##ing', 'to', 'american', 'universities', 'because', 'they', 'know', 'they', 'are', 'the', 'best', 'in', 'the', 'world', '.', '\"', 'so', 'far', ',', 'it', 'sounds', 'as', 'if', 'watt', '##enberg', 'were', 'a', 'patriotic', 'polly', '##anna', '.', 'has', 'the', 'us', '.', ',', 'functioning', 'as', 'the', 'benign', 'empire', ',', 'brought', 'the', 'heavenly', 'jerusalem', 'down', 'to', 'earth', '?', 'not', 'really', '.', '\"', 'america', 'has', 'some', 'severe', 'problems', ',', '\"', 'he', 'con', '##cede', '##s', '.', 'among', 'these', 'are', 'the', 'level', 'of', 'ordinary', 'schooling', ',', 'which', 'is', 'awful', ',', 'the', 'ago', '##nies', 'of', 'a', '\"', 'black', 'community', 'which', 'is', 'clearly', 'a', 'troubled', 'one', ',', '\"', 'the', 'deficit', ',', 'and', 'the', 'savings', 'and', 'loan', 'de', '##ba', '##cle', '.', 'but', 'for', 'watt', '##enberg', ',', 'none', 'of', 'these', '\"', 'severe', 'problems', '\"', 'arises', 'from', 'any', 'basic', 'failure', 'of', 'the', 'american', 'way', ',', 'nor', 'does', 'he', 'doubt', 'for', 'a', 'minute', 'that', 'america', 'can', 'get', 'over', 'them', '.', 'the', 'ill', '##iter', '##acy', 'produced', 'in', 'most', 'high', 'schools', 'is', 'the', 'direct', 'though', 'not', 'necessary', 'consequence', 'of', 'an', 'ad', '##mir', '##able', 'decision', 'to', 'try', 'to', 'educate', 'everyone', ',', '.', '\"', 'the', 'answer', 'is', 'to', 'introduce', 'higher', 'standards', 'and', 'enforce', 'discipline', 'for', 'everyone', '.', 'the', 'blacks', 'are', 'in', 'trouble', ',', 'but', 'there', 'are', 'more', 'and', 'more', 'black', 'university', 'graduates', 'and', 'a', 'growing', 'black', 'middle', 'class', ',', 'which', 'will', 'give', 'those', 'left', 'in', 'the', 'sl', '##ums', 'a', 'model', 'of', 'health', '##ier', 'behavior', '.', 'the', 'deficit', 'is', 'shrinking', '.', 'crime', '?', '\"', 'there', 'is', 'still', 'plenty', 'to', 'do', '.', '\"', 'the', 'people', 'who', 'looted', 'the', 's', '&', 'l', '##s', ';', '\"', 'ought', 'to', 'go', 'to', 'jail', '.', '\"', 'drugs', '?', 'do', 'n', \"'\", 't', 'legal', '##ize', 'them', ',', 'fight', 'them', '.', 'which', 'is', 'to', 'say', 'that', ',', 'although', 'watt', '##enberg', 'knows', 'that', 'things', 'are', 'n', \"'\", 't', 'perfect', 'at', 'home', ',', 'he', 'counts', 'on', 'america', 'to', 'pull', 'its', 'socks', 'up', 'as', 'it', 'has', 'done', 'before', ',', 'to', 'live', 'up', 'to', 'its', '\"', 'remarkable', 'record', 'of', 'dealing', 'with', 'our', 'own', 'problems', '.', '\"', 'above', 'all', ',', 'he', 'wants', 'to', 'di', '##sp', '##el', 'the', 'st', '##yl', '##ish', ',', 'dangerous', ',', 'persistent', 'cl', '##iche', 'that', 'there', \"'\", 's', 'something', 'basically', 'wrong', ',', 'that', 'america', 'has', 'lost', 'its', 'touch', '.', 'after', 'a', 'delay', 'of', 'only', 'a', 'few', 'months', ',', 'american', 'bestseller', '##s', 'reach', 'israel', ',', 'giving', 'reviewers', 'here', 'a', 'pretty', 'good', 'idea', 'contend', 'with', '.', '\"', 'decline', '\"', 'and', '\"', 'eclipse', '\"', 'seem', 'to', 'be', 'the', 'catch', '##words', '.', 'victory', 'in', 'the', 'cold', 'war', 'has', 'spawned', 'books', 'claiming', 'with', 'figures', 'and', 'rhetoric', 'that', 'america', \"'\", 's', 'time', 'has', 'come', 'and', 'gone', '-', '-', 'the', 'one', 'that', 'stayed', 'popular', 'longest', 'was', 'paul', 'kennedy', \"'\", 's', 'the', 'rise', 'and', 'fall', 'of', 'the', 'great', 'powers', '.', 'the', 'retirement', 'of', 'ronald', 'reagan', 'was', 'followed', 'by', 'books', 'de', '##pl', '##orin', '##g', 'the', 'effects', 'of', 'his', 'revolution', 'states', '##ide', '-', '-', 'the', 'best', 'known', 'is', 'the', 'politics', 'of', 'rich', 'and', 'poor', ',', 'by', 'kevin', 'phillips', '.', 'that', 'their', 'books', 'and', 'others', 'on', 'how', 'to', 'compete', 'with', 'the', 'japanese', 'should', 'be', 'bestseller', '##s', 'says', 'something', 'about', 'america', 'west', 'of', 'the', 'hudson', '.', 'indeed', ',', 'if', 'polls', 'are', 'to', 'be', 'trusted', 'any', 'more', 'than', 'statistics', ',', 'the', 'man', '-', 'in', '-', 'the', '-', 'street', 'is', 'feeling', 'lou', '##sy', 'about', 'his', 'country', '.', 'a', 'gall', '##up', 'poll', 'last', 'november', 'discovered', 'that', '64', 'percent', 'of', 'americans', 'were', '\"', 'unhappy', 'about', 'how', 'things', 'are', 'going', '.', '\"', 'likewise', ',', 'a', 'time', '/', 'cnn', 'survey', 'in', 'october', ',', 'asking', 'the', 'question', ',', '\"', 'how', 'well', 'do', 'you', 'think', 'things', 'are', 'going', 'in', 'the', 'u', '.', 's', '.', 'these', 'days', '?', '\"', 'got', 'only', '42', 'percent', 'of', 'a', 'representative', 'well', ',', '\"', 'compared', 'with', '67', 'percent', 'half', 'a', 'year', 'before', '.', 'possibly', ',', 'if', 'a', 'fresh', 'sounding', 'were', 'taken', ',', 'the', 'opt', '##imi', '##sts', 'would', 'pre', '##va', '##il', 'again', '.', 'even', 'if', 'they', 'did', ',', 'these', 'drastic', 'ups', 'and', 'downs', 'betray', 'something', 'fever', '##ish', 'in', 'the', 'american', 'psyche', '.', 'optimism', 'and', 'pe', '##ssi', '##mism', 'are', 'always', ',', 'of', 'course', ',', 'mostly', 'in', 'the', 'mind', '.', 'when', 'watt', '##enberg', 'and', 'others', 'who', 'feel', 'and', 'hope', 'as', 'he', 'does', 'about', 'america', '-', '-', 'one', 'thinks', 'of', 'george', 'will', 'in', 'his', 'most', 'recent', 'book', 'suddenly', ',', 'joseph', 'ny', '##e', 'in', 'bound', 'to', 'lead', ',', 'and', 'alfred', 'bal', '##k', 'in', 'the', 'myth', 'of', 'american', 'eclipse', '-', '-', 'marshal', 'their', 'figures', 'and', 'rhetoric', ',', 'they', \"'\", 're', 'trying', 'to', 'make', 'history', 'by', 'per', '##su', '##ading', 'their', 'fellow', 'citizens', 'to', 'think', ',', 'and', 'therefore', 'act', ',', 'like', 'winners', ',', 'to', 'explore', 'again', 'the', 'power', 'of', 'positive', 'thinking', '.', 'no', 'one', ',', 'least', 'of', 'all', 'an', 'israeli', ',', 'can', 'predict', 'whether', 'they', \"'\", 'll', 'be', 'able', 'to', 'pull', 'off', 'this', 'trick', '.', 'there', 'are', 'two', 'possibilities', ',', 'equally', 'plausible', 'as', 'this', 'review', 'is', 'being', 'written', ',', 'days', 'before', 'the', 'u', '.', 'n', '.', 'ultimatum', 'to', 'saddam', 'comes', 'due', '.', 'historians', 'who', 'are', 'now', 'will', 'either', 'agree', 'that', 'the', 'american', 'century', ',', 'an', 'abbreviated', 'one', ',', 'started', 'with', 'hiroshima', 'and', 'ended', 'with', 'the', 'evacuation', 'of', 'the', 'american', 'embassy', 'in', 'saigon', ',', 'soon', 'after', 'which', 'germany', ',', 'japan', ',', 'and', 'iraq', 'took', 'their', 'places', 'in', 'the', 'sun', ',', 'or', 'that', 'the', 'empire', ',', 'rallied', 'by', 'reagan', 'and', 'mobilized', 'by', 'his', 'successor', ',', 'struck', 'back', 'master', '##fully', '.', 'watt', '##enberg', 'saw', 'this', 'back', 'in', 'august', ',', 'and', 'wrote', ',', '\"', 'if', 'america', 'acts', 'firmly', 'on', 'the', 'iraqi', 'invasion', 'of', 'kuwait', 'then', 'the', 'u', '.', 's', '.', 'will', 'be', 'able', 'to', 'tell', 'him', ',', \"'\", 'thanks', 'saddam', ',', 'we', 'needed', 'that', \"'\", 'and', 'can', 'go', 'ahead', 'fashion', '##ing', 'a', 'new', 'world', 'order', '.', '\"', 'victory', 'over', 'saddam', 'would', 'send', 'america', \"'\", 's', 'stock', 'soaring', 'worldwide', '.', 'it', 'might', 'also', 'brace', 'the', 'american', 'psyche', ',', 'which', ',', 'god', 'knows', ',', 'needs', 'a', 'tonic', '.', 'as', 'an', 'ax', '-', 'american', 'who', 'moved', 'to', 'israel', 'years', 'ago', ',', 'who', 'still', 'roots', 'for', 'the', 'pittsburgh', 'pirates', 'and', 'pays', 'occasional', 'visits', 'to', 'the', 'old', 'country', ',', 'i', 'find', 'the', 'streets', 'of', 'new', 'york', 'and', 'washington', 'mean', '##er', 'with', 'each', 'visit', '.', 'worse', 'than', 'that', ',', 'my', 'old', 'friends', 'are', 'ever', 'bitter', '##er', 'and', 'the', 'homeless', ',', 'the', 'strung', '-', 'out', ',', 'the', 'menacing', 'leap', 'out', 'at', 'you', '.', 'the', 'american', 'middle', 'class', ',', 'ever', 'prey', 'to', 'the', 'twin', 'fears', 'of', 'getting', 'mug', '##ged', 'and', 'getting', 'fired', ',', 'has', 'long', 'since', 'adapted', 'to', 'them', '.', 'the', 'visitor', 'finds', 'america', 'enormous', ',', 'abundant', 'as', 'always', ',', 'and', 'wa', '##cky', '.', 'commercial', 'tv', 'is', 'idiot', '##ic', ',', 'non', '##com', '##mer', '##cial', 'creepy', ',', 'and', 'no', 'one', 'under', 'thirty', '-', 'five', 'can', 'form', 'a', 'complete', 'sentence', '.', 'there', 'does', 'n', \"'\", 't', 'seem', 'to', 'be', 'any', 'sort', 'of', 'ruling', 'class', 'anymore', '.', 'genuine', 'decade', '##nce', 'is', 'gaining', ',', 'innocence', 'retreating', '.', 'the', 'coastal', 'downtown', '##s', 'and', 'detroit', 'have', 'become', 'black', 'homeland', '##s', ',', 'no', '-', 'go', 'zones', 'reminiscent', 'of', 'certain', 'parts', 'of', 'certain', 'middle', 'eastern', 'cities', '-', '-', 'it', 'would', 'be', 'interesting', 'to', 'know', 'when', 'the', 'last', 'time', 'was', 'that', 'watt', '##enberg', 'walked', 'home', 'from', 'the', 'ae', '##i', '.', 'a', 'visitor', 'who', 'wishes', 'america', 'well', 'and', 'always', 'despised', 'henry', 'adams', 'has', 'to', 'wonder', 'at', 'watt', '##enberg', \"'\", 's', 'confidence', '.', 'it', 'takes', 'some', 'reading', 'between', 'the', 'lines', 'in', 'the', 'first', 'universal', 'nation', ',', 'and', 'a', 'look', 'at', 'an', 'earlier', ',', 'less', 'happy', 'book', 'of', 'his', ',', 'to', 'realize', 'that', 'watt', '##enberg', 'has', 'secret', 'doubts', 'himself', '.', 'that', 'book', 'was', 'the', 'americans', 'of', 'european', 'ancestry', 'were', 'having', 'fewer', 'than', 'the', '2', '.', '1', 'children', 'per', 'woman', 'needed', 'to', 'replace', 'themselves', ',', 'while', 'those', 'of', 'non', '-', 'european', 'ancestry', ',', 'including', 'blacks', 'and', 'immigrants', 'from', 'the', 'third', 'world', ',', 'were', 'having', 'more', '.', 'this', 'trend', 'worried', 'watt', '##enberg', 'sufficiently', 'to', 'inspire', 'an', 'entire', 'book', '.', 'he', 'feared', 'that', ',', 'while', 'the', 'melting', 'pot', 'could', 'cope', 'brilliant', '##ly', 'with', 'people', 'from', 'the', 'area', 'del', '##imi', '##ted', 'by', 'ireland', ',', 'scandinavia', ',', 'russia', ',', 'and', 'sicily', ',', 'it', 'was', 'liable', 'to', 'crack', 'if', 'it', 'had', 'to', 'ass', '##imi', '##late', 'the', 'whole', 'world', 'in', 'proportions', 'such', 'as', 'would', 'alter', 'america', \"'\", 's', 'complexion', 'from', 'white', 'to', 'colored', '.', 'e', 'pl', '##uri', '##bus', 'un', '##um', ',', 'yes', ',', 'but', 'only', 'to', 'a', 'point', '.', 'his', 'solution', '?', 'prop', '##aga', '##ndi', '##ze', 'white', 'women', 'to', 'have', 'more', 'babies', ';', 'make', 'it', 'a', 'paying', 'proposition', ',', 'as', 'in', 'sweden', '.', 'that', 'was', 'honest', 'of', 'him', ',', 'indeed', 'brave', '-', '-', 'he', 'exposed', 'himself', 'to', 'charges', 'of', 'racism', '.', 'watt', '##enberg', 'is', 'less', 'ad', '##mir', '##able', ',', 'because', 'less', 'forth', '##right', ',', 'in', 'his', 'new', 'book', ',', 'when', 'confronting', 'what', 'is', 'basically', 'the', 'same', 'subject', '.', 'first', ',', 'he', 'reports', 'that', 'the', 'children', '-', 'per', '-', 'woman', 'figure', 'in', 'the', 'u', '.', 's', '.', 'is', 'climbing', ',', 'is', 'much', 'better', 'than', 'japan', \"'\", 's', '.', 'he', 'forget', '##s', ',', 'however', ',', 'to', 'break', 'the', 'american', 'figure', 'down', 'into', 'its', 'white', 'and', 'non', '-', 'white', 'components', '.', 'instead', ',', 'he', 'plump', '##s', 'for', 'more', 'immigration', ',', 'along', 'lines', 'a', 'reader', 'has', 'to', 'think', 'about', 'before', 'understanding', 'the', 'likely', 'demographic', 'ups', '##hot', '.', 'far', 'from', 'being', 'reassured', 'by', 'the', 'rev', '##iving', 'birth', 'rate', ',', 'watt', '##enberg', 'is', 'ready', 'to', 'try', 'to', 'use', 'the', 'laws', ',', 'those', 'on', 'the', 'books', 'and', 'those', 'to', 'be', 'added', ',', 'to', 'keep', 'america', 'basically', 'white', '.', 'illegal', 'immigration', ',', 'running', 'into', 'the', 'millions', 'and', 'stemming', 'mainly', 'from', 'the', 'third', 'world', ',', 'would', 'be', 'suppressed', 'through', 'enforcement', 'of', 'existing', 'laws', '.', 'meanwhile', ',', 'legal', 'quota', '##s', 'would', 'be', 'increased', 'and', 'criteria', 'for', 'admission', 'reformed', 'so', 'as', 'to', 'give', 'preference', 'to', 'people', 'with', 'education', ',', 'skills', ',', 'and', 'capital', '.', 'the', 'net', 'effect', '?', 'more', 'new', 'americans', 'from', 'europe', ',', 'fewer', 'from', 'elsewhere', '.', 'about', 'the', 'first', 'outcome', ',', 'watt', '##enberg', 'is', 'explicit', '.', 'the', 'second', 'he', 'leaves', 'to', 'be', 'in', '##fer', '##red', '.', 'in', 'other', 'words', ',', 'he', \"'\", 's', 'pe', '##ssi', '##mist', '##ic', 'or', 'realistic', 'enough', 'to', 'believe', 'that', ',', 'while', 'the', 'first', 'universal', 'nation', 'should', 'go', 'on', 'export', '##ing', 'its', 'ideas', 'ca', 'n', \"'\", 't', '-', '-', 'nor', 'should', 'it', '-', '-', 'try', 'to', 'take', 'in', 'the', 'huddled', ',', 'mobile', 'masses', 'from', 'mexico', 'to', 'the', 'philippines', 'to', 'bangladesh', 'to', 'egypt', ',', 'not', 'if', 'it', 'wants', 'to', 'preserve', 'a', 'se', '##mb', '##lance', 'of', 'peace', 'in', 'its', 'streets', 'and', 'remain', 'number', 'one', '.', 'interesting', '##ly', 'enough', ',', 'congress', 'has', 'now', 're', '##written', 'the', 'immigration', 'laws', 'roughly', 'as', 'watt', '##enberg', 'suggested', '.', 'the', 'number', 'of', 'visas', 'granted', 'yearly', 'has', 'been', 'raised', 'from', '500', ',', '000', 'to', '700', ',', '000', ',', 'with', 'the', 'st', '##ip', '##ulation', 'that', 'most', 'of', 'the', 'added', 'number', 'be', 'not', 'for', 'relatives', 'of', 'earlier', 'immigrants', ',', 'but', 'for', 'skilled', 'or', 'rich', 'people', '.', 'not', 'for', 'the', 'first', 'time', 'in', 'its', 'short', 'history', ',', 'america', 'is', 'trying', 'to', 'reconcile', 'its', 'pie', '##ties', 'with', 'its', 'interests', ',', 'its', 'past', 'with', 'its', 'future', ',', 'its', 'dreams', 'with', 'its', 'nightmares', '.', 'meanwhile', ',', 'laws', 'or', 'no', 'laws', ',', 'the', 'mexican', '##s', ',', 'filipino', '##s', ',', 'and', ',', 'yes', ',', 'the', 'israelis', 'keep', 'see', '##ping', 'and', 'sneaking', 'in', '.', 'israel', 'is', 'the', 'other', 'fantastic', 'country', 'in', 'formation', '.', 'if', 'america', \"'\", 's', 'symbol', 'is', 'the', 'golden', 'door', ',', 'the', 'jewish', 'state', \"'\", 's', 'is', 'the', 'ing', '##ath', '##ering', 'of', 'the', 'exiles', ',', 'an', 'ideal', 'somewhat', 'similar', 'and', 'altogether', 'different', 'the', 'law', 'of', 'return', ',', 'giving', 'all', 'jews', 'the', 'right', 'to', 'enter', 'israel', 'and', 'get', 'citizenship', 'at', 'any', 'time', '.', 'you', 'have', 'to', 'be', 'a', 'very', 'important', 'parole', '-', 'breaker', ',', 'a', 'morton', 'sob', '##ell', 'or', 'meyer', 'lan', '##sky', ',', 'to', 'be', 'turned', 'back', '.', 'otherwise', ',', 'no', 'matter', 'what', 'the', 'burden', ',', 'no', 'matter', 'what', 'the', 'pain', ',', 'cost', ',', 'or', 'confusion', ',', 'no', 'matter', 'what', 'the', 'economists', 'or', 's', '##nob', '##s', 'say', ',', 'all', 'the', 'jews', 'who', 'knock', 'on', 'israel', \"'\", 's', 'doors', 'are', 'welcomed', '.', 'today', ',', 'this', 'means', 'a', 'million', 'or', 'more', 'refugee', '-', 'immigrants', 'from', 'the', 'di', '##sin', '##te', '##grating', 'soviet', 'union', '.', 'the', 'young', 'and', 'the', 'old', ',', 'the', 'wicked', 'and', 'the', 'good', ',', 'the', 'capitalist', '##s', 'and', 'the', 'stalin', '##ists', ',', 'the', 'healthy', 'and', 'the', 'hiv', '-', 'positive', ',', 'are', 'all', 'being', 'allowed', 'to', 'stream', 'in', 'on', 'principle', ',', 'and', 'ben', 'gu', '##rion', 'airport', 'is', 'a', 'throw', '##back', 'to', 'ellis', 'island', 'in', 'the', 'old', 'days', '.', 'whether', 'israel', 'will', 'rise', 'on', 'this', 'tide', 'or', 'drown', 'in', 'it', 'is', 'a', 'hard', 'question', '.', 'what', \"'\", 's', 'clear', 'is', 'that', 'most', 'of', 'these', 'jews', 'are', 'coming', 'here', 'only', 'because', 'the', 'u', '.', 's', '.', 'quota', 'is', 'filled', '.', 'they', \"'\", 're', 'moving', 'to', 'zion', 'not', 'because', 'they', \"'\", 're', 'will', 'take', 'them', 'in', '.', 'similarly', ',', 'the', 'israelis', 'and', 'others', 'who', 'continue', 'to', 'sneak', 'into', 'america', 'do', 'so', 'not', 'because', 'it', \"'\", 's', 'better', 'than', 'ever', ',', 'as', 'native', 'son', 'ben', 'watt', '##enberg', 'would', 'have', 'it', ',', 'or', 'because', 'its', 'prospects', 'are', 'great', ',', 'but', 'because', ',', 'saddam', 'dead', 'or', 'alive', ',', 'it', \"'\", 's', 'the', 'only', 'america', 'around', '.', 'illustration', 'the', 'colonel', ':', 'the', 'life', 'and', 'wars', 'of', 'henry', 'st', '##im', '##son', ',', '1867', '-', '1950', 'philip', 'ter', '##zia', '##n', 'godfrey', 'hodgson', '/', 'alfred', 'a', '.', 'kn', '##op', '##f', '/', '402', '##pp', '.', '$', '24', '.', '95', 'the', 'next', 'time', 'david', 's', '.', 'bro', '##der', 'or', 'r', '.', 'w', '.', 'apple', ',', 'jr', '.', ',', 'or', 'george', 'f', '.', 'will', ',', 'or', 'any', 'other', 'of', 'our', 'pol', '##itic', '##o', '-', 'journalist', '##ic', 'lu', '##mina', '##ries', 'spec', '##ulates', 'in', 'print', 'about', 'george', 'bush', \"'\", 's', '\"', 'vision', 'thing', ',', '\"', 'it', 'would', 'be', 'worth', '##while', 'to', 'refer', 'him', 'to', 'this', 'volume', '.', 'although', 'the', 'british', 'journalist', 'godfrey', 'hodgson', \"'\", 's', 'long', '##awa', '##ited', 'life', 'of', 'henry', 'st', '##im', '##son', 'is', 'infinitely', 'less', 'than', 'any', 'hopeful', 'peru', '##ser', 'has', 'a', 'right', 'to', 'expect', ',', 'it', 'does', 'answer', 'a', 'question', 'that', 'has', 'plagued', 'the', 'city', 'of', 'washington', 'for', 'the', 'past', 'few', 'years', '.', 'for', 'in', 'colonel', 'st', '##im', '##son', 'we', 'find', 'the', 'american', 'equivalent', 'of', 'the', 'aristocratic', 'principle', ':', 'of', 'those', 'to', 'and', 'so', 'on', '.', 'our', 'pop', '##uli', '##st', ',', 'democratic', 'ideals', 'prevent', 'us', 'from', 'thinking', 'comfortably', 'in', 'these', 'terms', ',', 'but', 'there', 'it', 'is', '.', 'no', 'doubt', 'the', 'colonel', \"'\", 's', 'famous', 'speech', 'to', 'george', 'bush', \"'\", 's', 'graduating', 'class', 'at', 'andover', '-', '-', 'the', 'one', 'in', 'which', 'he', 'said', 'that', 'he', 'did', 'n', \"'\", 't', 'pity', 'the', 'boys', 'but', 'en', '##vie', '##d', 'them', ',', 'since', 'providence', 'had', 'deposited', 'them', 'at', 'a', 'moment', 'in', 'time', '(', '1940', ')', 'that', 'gave', 'them', 'the', 'responsibility', 'of', 'choosing', 'between', 'good', 'and', 'evil', 'for', 'the', 'world', '-', '-', 'had', 'the', 'intended', 'effect', '.', 'those', 'who', 'expect', 'our', 'president', \"'\", 's', 'ideology', 'to', 'be', 'revealed', 'in', 'cu', '##omo', '##es', '##que', 'or', '##ations', 'or', 'keynote', 'addresses', 'or', 'soaring', 'declaration', '##s', 'need', 'look', 'no', 'further', 'than', 'the', 'scouting', 'manual', 'of', 'the', 'gentleman', 'squire', ':', 'duty', ',', 'fidelity', ',', 'nobles', '##se', 'ob', '##li', '##ge', ',', 'imp', '##ec', '##cable', 'manners', ',', 'sword', 'at', 'the', 'ready', '.', 'indeed', ',', 'colonel', 'st', '##im', '##son', 'seldom', 'en', '##un', '##cia', '##ted', 'what', 'might', 'pass', 'today', 'for', 'a', 'constitutional', 'creed', 'or', 'a', 'declaration', 'of', 'political', 'principle', '.', 'neither', 'the', 'great', 'society', 'nor', 'the', 'century', 'of', 'the', 'common', 'man', 'would', 'have', 'meant', 'much', 'to', 'this', 'skull', 'and', 'bones', '##ter', '.', 'his', 'was', 'a', 'sharp', ',', 'con', '##st', '##ricted', 'vision', 'of', 'an', 'imperfect', 'world', 'in', 'guidance', 'and', 'some', 'fundamental', 'pre', '##ce', '##pts', '.', 'just', 'as', 'the', 'great', 'mercantile', 'enterprises', 'of', 'the', 'gilded', 'age', 'had', '##lo', '##oked', 'to', 'young', 'st', '##im', '##son', 'for', 'protection', 'and', 'advice', ',', 'so', 'his', 'country', '##men', 'sought', 'old', 'st', '##im', '##son', \"'\", 's', 'guidance', 'in', 'their', 'violent', 'transition', 'to', 'super', '##power', 'status', '.', 'the', 'colonel', ',', 'who', 'ran', 'only', 'once', 'for', 'elect', '##ive', 'office', ',', 'did', 'not', 'necessarily', 'pursue', 'such', 'prestige', ';', 'it', 'was', 'his', 'obligation', 'to', 'assume', 'responsibility', '.', 'this', 'particular', 'tradition', 'in', 'our', 'national', 'political', 'life', 'did', 'not', 'begin', 'with', 'henry', 'st', '##im', '##son', ',', 'but', 'for', 'two', 'generations', 'of', 'american', 'foreign', 'policy', 'he', 'person', '##ified', 'it', '.', 'born', 'in', 'new', 'york', 'city', 'two', 'years', 'after', 'the', 'end', 'of', 'the', 'civil', 'war', ',', 'st', '##im', '##son', 'died', 'three', 'months', 'after', 'the', 'outbreak', 'of', 'fighting', 'in', 'korea', ':', 'from', 'general', 'sherman', 'to', 'dr', '.', 'op', '##pen', '##heimer', 'in', 'one', 'lifetime', '.', 'st', '##im', '##son', \"'\", 's', 'mother', 'died', 'when', 'he', 'was', 'eight', 'years', 'old', ',', 'and', 'his', 'father', ',', 'a', 'banker', 'who', 'switched', 'careers', 'to', 'medicine', 'to', 'dia', '##gno', '##se', 'and', 'treat', 'his', 'ai', '##ling', 'spouse', ',', 'left', 'his', 'orphaned', 'children', 'in', 'the', 'care', 'of', 'their', 'grandparents', 'while', 'he', 'threw', 'himself', 'ob', '##ses', '##sive', '##ly', 'into', 'his', 'medical', 'work', '.', 'henry', 'st', '##im', '##son', 'was', 'a', 'sensitive', ',', 'aus', '##ter', '##e', 'boy', ',', 'the', 'phillips', 'academy', '(', 'andover', ')', ',', 'yale', 'college', ',', 'and', 'harvard', 'law', 'school', '.', 'his', 'childless', 'marriage', 'to', 'a', 'woman', 'of', 'slightly', 'less', 'ex', '##al', '##ted', 'social', 'rank', 'distressed', 'his', 'father', ',', 'but', 'it', 'was', 'a', 'successful', ',', 'possibly', 'even', 'bliss', '##ful', ',', 'match', 'that', 'lasted', 'half', 'a', 'century', '.', 'he', 'settled', 'into', 'the', 'practice', 'of', 'corporate', 'law', 'in', '1890s', 'manhattan', ',', 'rode', 'to', 'hounds', 'in', 'the', 'country', ',', 'and', ',', 'as', 'he', 'slowly', 'descended', 'into', 'early', 'middle', 'age', ',', 'came', 'under', 'the', 'inevitable', 'influence', 'of', 'eli', '##hu', 'root', ',', 'senior', 'partner', 'in', 'his', 'firm', ',', 'and', 'theodore', 'roosevelt', ',', 'the', 'great', 'white', 'chief', 'of', 'his', 'governing', 'tribe', '.', 'to', 'the', 'career', '-', 'minded', 'among', 'us', ',', 'st', '##im', '##son', \"'\", 's', 'life', 'has', 'a', 'rare', 'narrative', 'grande', '##ur', '.', 'once', 'eli', '##hu', 'root', 'was', 'brought', 'into', 'the', 'mckinley', 'administration', 'in', '1899', 'to', 'organize', 'the', 'bureau', '##cratic', 'chaos', 'in', 'the', 'war', 'department', 'after', 'the', 'victory', 'over', 'spain', ',', 'st', '##im', '##son', \"'\", 's', 'star', 'was', 'destined', 'to', 'rise', '.', 'as', 'secretary', 'of', 'state', ',', 'root', 'commended', 'his', 'younger', 'partner', 'to', 'theodore', 'roosevelt', ',', 'who', 'appointed', 'st', '##im', '##son', 'u', '.', 's', '.', 'attorney', 'for', 'the', 'southern', 'district', 'of', 'new', 'york', '.', 'there', 'he', 'assembled', 'an', 'elite', 'brigade', 'of', 'younger', 'often', 'duplicate', '##d', 'since', '.', 'next', ',', 'he', 'presided', 'for', 'two', 'years', 'over', 'william', 'howard', 'taft', \"'\", 's', 'war', 'department', ',', 'and', ',', 'when', 'conflict', 'finally', 'came', 'for', 'the', 'united', 'states', 'in', '1917', ',', 'he', 'spent', 'several', 'happy', 'months', 'as', 'an', 'aging', 'artillery', '##man', 'in', 'france', '.', 'the', 'harding', 'years', 'found', 'him', 'practicing', 'law', ',', 'but', 'in', '1927', ',', 'when', 'calvin', 'cool', '##idge', 'sought', 'a', 'media', '##tor', 'for', 'nicaragua', \"'\", 's', 'insurgency', ',', 'he', 'recruited', 'st', '##im', '##son', '.', 'the', 'arrangements', 'negotiated', 'under', 'the', 'thorn', 'tree', 'at', 'tip', '##ita', '##pa', '-', '-', 'boycott', '##ed', ',', 'of', 'course', ',', 'by', 'augusto', 'sand', '##ino', '-', '-', 'were', 'characteristic', 'of', 'st', '##im', '##son', '.', 'establishing', 'his', 'authority', 'by', 'plain', 'strength', 'of', 'will', ',', 'he', 'was', 'both', 'cunning', 'and', 'wise', ',', 'rigorous', 'and', 'def', '##ere', '##ntial', ',', 'sc', '##rup', '##ulous', 'and', 'candi', '##d', ';', 'and', 'he', 'expected', 'his', 'ga', '##udy', 'players', 'to', 'observe', 'those', 'same', 'standards', '.', 'presumably', ',', 'his', 'actions', 'offended', 'latin', 'temper', '##s', ',', 'but', 'they', 'did', 'appeal', 'to', 'yankee', 'sentiment', ',', 'and', 'cool', '##idge', 'of', 'vermont', 'rewarded', 'st', '##im', '##son', 'of', 'new', 'york', 'with', 'a', 'pro', '##con', '##sul', '##ar', 'plum', ':', 'governor', '-', 'general', 'of', 'the', 'philippines', '.', 'thus', ',', 'on', 'the', 'basis', 'of', 'status', 'and', 'merit', '-', '-', 'and', 'on', 'that', 'basis', 'alone', '-', '-', 'he', 'was', 'herbert', 'hoover', \"'\", 's', 'choice', 'as', 'secretary', ':', 'hoover', 'port', '##ly', ',', 'ph', '##leg', '##matic', ',', 'cynical', ',', 'dei', '##lib', '##erate', ',', 'slow', 'to', 'anger', ',', 'with', 'a', 'memory', 'for', 'slight', '##s', ';', 'st', '##im', '##son', 'gaunt', ',', 'imp', '##et', '##uous', ',', 'instinct', '##ive', ',', 'ideal', '##istic', ',', 'quick', 'to', 'react', 'but', 'swift', 'to', 'forgive', '.', 'out', 'of', 'the', 'japanese', 'invasion', 'of', 'manchu', '##ria', '(', '1931', ')', 'came', 'the', 'st', '##im', '##son', 'doctrine', ',', 'the', 'notion', 'of', 'with', '##holding', 'diplomatic', 'recognition', 'on', 'grounds', 'of', 'principle', '.', 'st', '##im', '##son', '##ism', 'in', 'practice', 'combined', 'wilson', '##ian', 'ideal', '##ism', 'with', 'roosevelt', '##ian', 'action', ',', 'and', 'his', 'ind', '##ign', '##ation', 'over', 'japanese', 'per', '##fi', '##dy', 'was', 'characteristic', '##ally', 'deep', '.', 'the', 'united', 'states', 'should', 'not', 'merely', 'record', 'its', 'disapproval', ',', 'he', 'believed', ',', 'but', 'punish', 'the', 'trans', '##gre', '##sso', '##r', '.', 'but', 'hoover', ',', 'deep', 'in', 'the', 'depression', 'and', 'per', '##ce', '##iving', 'that', 'his', 'influence', 'in', 'japanese', '-', 'occupied', 'north', 'china', 'was', 'not', 'likely', 'to', 'be', 'great', ',', 'saw', 'little', 'point', 'in', 'bela', '##bor', '##ing', 'the', 'issue', '.', 'modern', 'readers', 'will', 'appreciate', 'the', 'sixty', '-', 'year', '-', 'old', 'debates', 'on', 'the', 'efficacy', 'of', 'em', '##bar', '##gos', 'and', 'the', 'varied', 'definitions', 'of', 'american', 'interest', '.', 'but', 'in', 'their', 'differences', 'over', 'manchu', '##ria', ',', 'st', '##im', '##son', 'and', 'hoover', 'were', 'a', 'vision', 'of', 'the', 'future', ':', 'intervention', '##ism', ',', 'isolation', '##ism', ';', 'bundles', 'for', 'britain', ',', 'america', 'first', '.', 'towards', 'the', 'end', 'of', 'his', 'life', ',', 'st', '##im', '##son', 'wrote', 'war', ')', 'with', 'the', 'help', 'of', 'young', 'mc', '##ge', '##org', '##e', 'bun', '##dy', ',', 'the', 'son', 'of', 'a', 'longtime', 'aide', '.', 'four', 'years', 'after', 'st', '##im', '##son', \"'\", 's', 'death', ',', 'richard', 'n', '.', 'current', 'issued', 'a', 'hostile', 'study', '(', 'secretary', 'st', '##im', '##son', ')', ',', 'and', 'in', '1960', ',', 'el', '##ting', 'e', '.', 'mori', '##son', ',', 'editor', 'of', 'theodore', 'roosevelt', \"'\", 's', 'letters', ',', 'published', 'an', 'authorized', 'life', '(', 'turmoil', 'and', 'tradition', ')', '.', 'godfrey', 'hodgson', 'is', 'a', 'journalist', ',', 'not', 'a', 'historian', ',', 'and', 'as', 'far', 'as', 'i', 'can', 'tell', ',', 'he', 'has', 'added', 'nearly', 'nothing', 'to', 'the', 'substance', 'of', 'these', 'earlier', 'volumes', '.', 'moreover', ',', 'he', 'is', 'a', 'brit', '##on', ',', 'and', 'what', 'he', 'might', 'regard', 'as', 'an', 'outsider', \"'\", 's', 'insights', 'are', 'often', 'a', 'stranger', \"'\", 's', 'mis', '##per', '##ception', '##s', '.', 'his', 'judgment', 'of', 'st', '##im', '##son', \"'\", 's', 'character', 'is', 'slightly', 'less', 'def', '##ere', '##ntial', 'than', 'mori', '##son', \"'\", 's', ';', 'meanwhile', ',', 'his', 'view', 'of', 'st', '##im', '##son', \"'\", 's', 'policies', 'and', 'opinions', 'is', 'very', 'nearly', 'identical', 'to', 'current', \"'\", 's', '.', 'he', 'breaks', 'new', 'ground', 'only', 'in', 'reminding', 'us', 'constantly', 'that', 'st', '##im', '##son', 'and', 'his', 'mentor', '##s', '(', 'root', ',', 'taft', ',', 'leonard', 'wood', ')', 'had', 'politically', 'incorrect', 'ideas', 'on', 'race', '.', 'no', 'one', 'can', 'dispute', 'that', 'eli', '##hu', 'root', 'held', 'views', 'about', 'asian', '##s', 'that', 'no', 'st', '##im', '##son', \"'\", 's', 'rational', '##e', 'for', 'the', 'internment', 'of', 'japanese', '-', 'americans', 'in', '1942', 'seems', 'largely', 'un', '##con', '##vin', '##cing', 'fifty', 'years', 'later', '.', 'but', 'so', 'what', '?', 'i', 'should', 'imagine', 'that', 'even', 'godfrey', 'hodgson', \"'\", 's', 'sen', '##si', '##bilities', 'have', 'evolved', 'in', 'the', 'course', 'of', 'decades', ',', 'and', 'most', 'of', 'the', 'offenders', 'in', 'this', 'volume', 'were', 'born', 'when', 'slavery', 'was', 'still', 'extant', '.', 'it', 'is', 'perhaps', 'not', 'coincide', '##ntal', 'that', 'hodgson', 'has', 'formed', 'many', 'of', 'his', 'conclusions', 'on', 'the', 'basis', 'of', 'conversations', 'with', 'such', 'contemporary', 'na', '##bo', '##bs', 'as', 'the', 'bun', '##dy', 'brothers', '(', 'mc', '##ge', '##org', '##e', 'and', 'william', ')', 'and', 'the', 'late', 'king', '##man', 'brewster', '.', 'the', 'echoes', 'of', 'those', 'comfortable', 'anglo', '-', 'american', 'chat', '##s', 'sound', 'heavily', 'in', 'these', 'pages', '.', 'the', 'assertion', 'that', 'st', '##im', '##son', 'was', 'some', 'kind', 'of', 'pioneer', 'new', 'frontiers', '##man', '-', 'a', 'herald', 'tribune', 'republican', 'whose', 'faith', 'has', 'been', 'betrayed', 'in', 'our', 'bush', '/', 'reagan', '##ite', 'times', '-', '-', 'is', 'wholly', 'un', '##pers', '##ua', '##sive', ',', 'thoroughly', 'predictable', ',', 'and', 'patent', '##ly', 'wrong', '.', 'the', 'distance', 'of', 'decades', 'gives', 'hodgson', 'the', 'freedom', 'to', 'psycho', '##anal', '##y', '##ze', 'his', 'dead', 'subject', '.', 'st', '##im', '##son', ',', 'to', 'be', 'sure', ',', 'is', 'eligible', 'for', 'some', 'sessions', 'on', 'the', 'couch', ':', 'the', 'mother', '##less', 'boy', 'with', 'the', 'cold', ',', 'distant', 'father', ';', 'the', 'apparent', 'ste', '##ril', '##ity', ',', 'possibly', 'caused', 'by', 'mum', '##ps', ';', 'the', 'temper', ';', 'st', '##ren', '##uous', 'physical', 'exercise', ';', 'the', 'rigid', 'adherence', 'to', 'his', 'gentleman', \"'\", 's', 'code', ';', 'the', 'imp', '##erson', '##al', 'diaries', ',', 'devoid', 'of', 'reflections', 'on', 'the', 'arts', 'or', 'the', 'flesh', '.', 'but', 're', '##tic', '##ence', 'is', 'not', 'always', 'a', 'means', 'of', 'conceal', '##ing', 'things', ',', 'and', 'orphans', 'sometimes', 'avoid', 'psychic', 'wounds', '.', 'it', 'is', 'entirely', 'possible', 'that', 'st', '##im', '##son', \"'\", 's', 'marriage', 'influenced', 'his', 'public', 'behavior', ',', 'or', 'vice', 'versa', ',', 'but', 'hodgson', \"'\", 's', 'speculation', '##s', 'are', 'obvious', 'and', 'sim', '##pl', '##istic', ',', 'and', 'his', 'only', 'living', 'witnesses', 'knew', 'st', '##im', '##son', 'in', 'old', 'age', ',', 'when', 'history', 'had', 'already', 'wrapped', 'him', 'like', 'a', 'sh', '##roud', '.', 'in', 'the', 'summer', 'of', '1940', ',', 'when', 'france', 'cap', '##it', '##ulated', 'and', 'dunkirk', 'fell', ',', 'justice', 'felix', 'frankfurt', '##er', 'swung', 'into', 'action', '.', 'franklin', 'roosevelt', 'had', 'been', 'anxious', 'to', 'convey', 'some', 'message', 'to', 'britain', 'and', 'hitler', 'with', 'symbolic', 'appointments', 'in', 'his', 'peace', '##time', 'cabinet', ';', 'he', 'was', 'also', 'concerned', 'about', 'the', 'republican', 'national', 'convention', 'opening', 'in', 'philadelphia', '.', 'st', '##im', '##son', ',', 'who', 'had', 'known', 'f', '##dr', 'socially', 'and', 'kept', 'in', 'friendly', 'contact', 'throughout', 'the', 'new', 'deal', ',', 'was', 'the', 'best', '-', 'known', 'republican', 'intervention', '##ist', 'in', 'america', '.', 'the', 'el', '##ope', '##ment', 'of', 'st', '##im', '##son', 'and', 'roosevelt', ',', 'engineered', 'by', 'frankfurt', '##er', ',', 'outraged', 'the', 'republicans', 'but', 'impressed', 'the', 'europeans', ':', 'best', 'means', 'roosevelt', 'had', 'to', 'take', 'sides', 'and', 'prepare', 'america', 'to', 'fight', '.', 'st', '##im', '##son', 'in', 'the', 'war', 'department', ',', 'and', 'frank', 'knox', 'in', 'the', 'navy', 'department', ',', 'were', 'more', 'than', 'mere', 'figure', '##heads', ';', 'but', ',', 'by', 'virtue', 'of', 'their', 'age', 'and', 'the', 'demands', 'of', 'global', 'warfare', ',', 'they', 'were', 'destined', 'to', 'rat', '##ify', ',', 'not', 'initiate', ',', 'u', '.', 's', '.', 'policies', '.', 'by', 'the', 'time', 'st', '##im', '##son', 'presided', 'over', 'the', 'decision', 'to', 'use', 'the', 'atomic', 'bomb', 'against', 'the', 'japanese', ',', 'he', 'was', '78', 'years', 'old', ',', 'weary', ',', 'distracted', ',', 'and', 'very', 'nearly', 'worn', 'out', '.', 'yet', 'it', 'must', 'be', 'said', 'that', 'the', 'process', 'by', 'which', 'he', 'de', '##hm', '##ed', 'the', 'nuclear', 'problem', '-', '-', 'the', 'disco', '##nce', '##rting', 'meeting', 'of', 'human', 'morality', 'and', 'national', 'obligation', '-', '-', 'was', 'exactly', 'the', 'kind', 'of', 'service', 'that', 'st', '##im', '##son', 'rendered', 'best', ',', 'and', 'holds', 'up', 'rather', 'well', '.', 'no', 'american', 'statesman', 'better', 'understood', 'military', 'necessity', ',', 'principle', '##d', 'action', ',', 'the', 'need', 'to', 'arrive', 'at', 'a', 'timely', 'consensus', '.', 'this', 'was', ',', 'as', 'he', 'saw', 'it', ',', 'the', 'dread', 'responsibility', 'for', 'which', 'he', 'had', 'been', 'trained', '.', 'such', 'burden', '##s', 'were', 'un', '##we', '##lco', '##me', ',', 'but', 'always', 'assumed', '-', 'and', 'the', 'model', 'of', 'service', 'that', 'was', 'henry', 'st', '##im', '##son', \"'\", 's', 'death', '.', 'philip', 'ter', '##zia', '##n', 'is', 'editor', 'of', 'the', 'editorial', 'pages', 'at', 'the', 'providence', 'journal', '.', 'by', 'edward', 'nord', '##en', 'edward', 'nord', '##en', 'is', 'a', 'writer', 'living', 'in', 'jerusalem', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "17eab1e5-0792-4642-ed66-55176874dc06",
        "id": "t2x6_FhxOrqn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)\n",
        "evaluate_lstm"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48/48 [==============================] - 0s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3465331370631854, 0.9166666666666666]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6muAlDeiac7",
        "colab_type": "text"
      },
      "source": [
        "###### BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln3UsOQpkv0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_labels.astype(np.int64)\n",
        "validation_labels = validation_labels.astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDljt39iddQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgmJ-E_VigTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYkR2tJxiiJk",
        "colab_type": "code",
        "outputId": "21bf4610-2da7-4cc4-8586-fe20f0b30188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ee16350569a47b08cf81b384dd88279",
            "fb883f3d0280437d9ae480ef22144207",
            "d345ea71540946368901fb423beb95c7",
            "11989d1e4cab49148e901a67a1da4f5f",
            "8fa39f52445447eb80ae07aeb7ac0495",
            "61f7acdc1cbf4840b1f45da3bc7c3b82",
            "7aa290f3ba254038950e44209799884f",
            "a17ed805a3b74ff5a13ad928678d76f6",
            "375c2bfd19c648ceb2ab69f75a7e72bb",
            "1b5b7c13d05844959aec52fba51b8586",
            "477c12db3d2a44528e0401ee12266a69",
            "f6bacae83315436c9ad1879b389b5d51",
            "db6b1e77ac5a4d21a7787d347dac3892",
            "70d04d81526647959009c7ad9df5da8a",
            "9782b23db0974ced8a7c57b480d0b843",
            "9e41afc3802b45d08be98f06c7c607d6"
          ]
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ee16350569a47b08cf81b384dd88279",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "375c2bfd19c648ceb2ab69f75a7e72bb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfwPyoh9ii0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likqi1-FimRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJcXKErgioEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-fvXJW_ip1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtCJt0A0irEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwi-bj1itbL",
        "colab_type": "code",
        "outputId": "32f5a3ef-16a0-437c-9930-25871eed8516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:00:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uXYaxgmBL5l",
        "colab_type": "code",
        "outputId": "c509bb62-222e-4681-be34-97b9fd43caf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31005677773996637,\n",
              " 0.19703539626465905,\n",
              " 0.15298841303835312,\n",
              " 0.11099545119537248]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOFcoj54FuL_",
        "colab_type": "code",
        "outputId": "05bf0b91-4fa9-47bf-8f74-a0a10a084d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(period0_test)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFogz7jeiycc",
        "colab_type": "code",
        "outputId": "41da0541-0d15-4ac6-9397-a243d22a4229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(period0_test.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = period0_test.sentence.values\n",
        "labels = period0_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length=512\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels.astype(np.int64))\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 48\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3S6GaadjK48",
        "colab_type": "code",
        "outputId": "0f8bdc8e-1cb1-40cf-fe35-5444433a208a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 48 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fqVB6CtzTo",
        "colab_type": "code",
        "outputId": "544b6897-4ca3-4cea-a10f-91b7b0de314c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (period0_test.label.sum(), len(period0_test.label), (period0_test.label.sum() / len(period0_test.label) * 100.0)))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 4 of 48 (8.33%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoseqqzt1pc",
        "colab_type": "code",
        "outputId": "a9f70321-2dc7-43ac-c887-209761e38d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BreKUJ9t25l",
        "colab_type": "code",
        "outputId": "8e697969-6173-4bc0-85eb-496f8ec5d5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20s0Poipt6XT",
        "colab_type": "code",
        "outputId": "f012b025-315f-4919-baeb-ba4b772214b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1Y-b7-90S9",
        "colab_type": "code",
        "outputId": "67e19361-482e-4724-ac52-698a0e2effb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './period0/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./period0/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./period0/vocab.txt',\n",
              " './period0/special_tokens_map.json',\n",
              " './period0/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okt1rI2lShvL"
      },
      "source": [
        "#### Period 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a2456a0e-5827-47a5-bdc4-846fee9a0b54",
        "id": "i_WIWP2uShvM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "period1 = data[data.period==1]\n",
        "period1"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>twenty-five years ago phyllis chesler 's book...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>it 's tough suddenly changing from one alphab...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>ii.s. military personnel of all ranks are fee...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>on the redemptive pain of loving the natural ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>\" . . . the last few decades have brought gre...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2804</th>\n",
              "      <td>a new round of the violence that has beset i...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2805</th>\n",
              "      <td>miami - when maria cueto sees little elian g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2806</th>\n",
              "      <td>while snow pelted denver , aspiring russian ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2807</th>\n",
              "      <td>back in the last century , when he worked fo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2808</th>\n",
              "      <td>' indecision 2000 '  ' win at all costs '  i...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>563 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "280    twenty-five years ago phyllis chesler 's book...     0       1\n",
              "284    it 's tough suddenly changing from one alphab...     0       1\n",
              "285    ii.s. military personnel of all ranks are fee...     0       1\n",
              "287    on the redemptive pain of loving the natural ...     0       1\n",
              "289    \" . . . the last few decades have brought gre...     0       1\n",
              "...                                                 ...   ...     ...\n",
              "2804    a new round of the violence that has beset i...     0       1\n",
              "2805    miami - when maria cueto sees little elian g...     0       1\n",
              "2806    while snow pelted denver , aspiring russian ...     0       1\n",
              "2807    back in the last century , when he worked fo...     0       1\n",
              "2808    ' indecision 2000 '  ' win at all costs '  i...     0       1\n",
              "\n",
              "[563 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAB9agReTpDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1eeb84f-5653-4be3-d359-401448802587"
      },
      "source": [
        "period1.label.sum()/len(period1)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06571936056838366"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Mch-qjkShvO",
        "colab": {}
      },
      "source": [
        "period1_train, period1_test = train_test_split(period1, random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2aebe236-defd-4d0b-c3fa-697e2bb12447",
        "id": "lBNKZXoFShvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period1_train), len(period1_test))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "534 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4ff612e-6e33-4375-e000-508588c5798e",
        "id": "daSyVkLFShvQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period1_train.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period1_train.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', '!', 'mr', '-', 'russ', '##ert', ':', 'but', 'first', ':', 'former', 'pow', ',', 'now', 'presidential', 'candidate', ',', 'senator', 'john', 'mccain', 'of', 'arizona', ',', 'welcome', '.', '!', 'mr', '-', 'mccain', ':', 'thank', 'you', ',', 'tim', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'your', 'campaign', 'is', 'hot', ',', 'dead', 'even', 'with', 'george', 'bush', 'in', 'new', 'hampshire', '.', 'the', 'cover', 'of', 'time', 'magazine', ',', 'the', 'real', 'mccain', '.', 'but', 'still', 'continuing', 'focus', 'on', 'the', 'hot', 'temper', 'issue', '.', '!', 'mr', '-', 'mccain', ':', 'yes', ',', 'indeed', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'here', \"'\", 's', 'the', 'new', 'york', 'times', ':', 'mccain', 'releases', 'medical', 'files', 'to', 'counter', 'whisper', 'campaign', '.', 'what', \"'\", 's', 'the', 'whisper', 'campaign', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'do', 'n', \"'\", 't', 'know', ',', 'and', 'i', 'hope', 'it', 'goes', 'away', '.', 'but', ',', 'by', 'the', 'way', ',', 'it', 'may', 'have', 'the', 'effect', 'of', 'alla', '##ying', 'any', 'concerns', 'that', 'people', 'might', 'have', '.', 'but', 'i', 'had', 'planned', 'on', 'releasing', 'my', 'medical', 'records', 'from', 'the', 'beginning', '.', 'i', 'thought', 'it', 'was', 'just', 'something', 'that', 'presidential', 'candidates', 'do', '.', 'and', ',', 'by', 'the', 'way', ',', 'it', \"'\", 's', 'about', '1', ',', '500', 'pages', 'of', 'an', 'or', '##th', '##oped', '##ic', 'surgeon', \"'\", 's', 'nightmare', 'or', 'dream', ',', 'depending', 'on', 'how', 'you', 'view', 'it', '.', 'but', 'i', 'do', 'n', \"'\", 't', 'know', 'if', 'there', \"'\", 's', 'any', 'whispering', 'think', 'the', 'thing', 'is', 'that', 'we', 'need', 'to', 'move', 'forward', 'and', 'i', 'hope', 'that', 'this', 'will', 'be', '-', '-', 'at', 'least', 'have', 'that', 'beneficial', 'effect', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'do', 'you', 'think', 'other', 'candidates', 'should', 'release', 'their', 'medical', 'records', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'think', 'that', \"'\", 's', 'up', 'to', 'them', '.', 'i', 'would', 'not', '.', '.', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'in', 'your', 'records', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', '.', '.', '.', 'push', 'them', 'to', 'do', 'that', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'in', 'your', 'records', ',', 'it', 'said', 'that', 'five', ',', 'six', 'years', 'ago', ',', 'you', 'had', 'mel', '##ano', '##ma', ',', 'cancer', '.', 'are', 'you', 'now', 'cancer', '-', 'free', '?', '!', 'mr', '-', 'mccain', ':', 'yes', '.', 'i', 'had', 'a', 'mali', '##gnant', 'mel', '##ano', '##ma', 'in', 'my', 'shoulder', ',', 'which', 'was', 'removed', '.', 'i', 'think', 'it', \"'\", 's', 'well', 'to', 'point', 'out', 'that', 'they', 'say', 'once', 'you', \"'\", 've', 'had', 'one', ',', 'there', \"'\", 's', 'a', 'further', '-', '-', 'you', 'know', ',', 'greater', 'likelihood', 'of', 'having', 'another', 'one', '.', 'but', 'i', 'do', 'n', \"'\", 't', 'see', 'it', 'as', 'any', 'great', 'danger', '.', 'and', 'i', \"'\", 've', 'had', 'a', 'lot', 'of', 'little', 'things', 'cut', 'off', 'my', 'face', '.', 'i', 'think', 'it', \"'\", 's', 'an', 'irish', 'or', 'scotch', 'curse', 'that', 'when', 'you', 'have', 'fair', 'skin', 'and', 'spend', 'a', 'lot', 'of', 'time', 'in', 'the', 'sun', 'i', 'see', 'a', 'doctor', 'regularly', 'and', 'get', 'various', 'cutting', 'and', 'slicing', '.', '.', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'but', 'no', 're', '##oc', '##cu', '##rre', '##nce', '?', '!', 'mr', '-', 'mccain', ':', 'no', ',', 'no', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'in', 'your', 'book', ',', 'you', 'mentioned', 'in', 'the', 'dark', 'days', 'of', 'a', 'cell', ',', 'of', 'pow', ',', 'you', 'contemplated', 'suicide', '.', '!', 'mr', '-', 'mccain', ':', 'at', 'one', 'point', ',', 'yes', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'have', 'there', 'been', 'any', 'other', 'depression', '##s', 'or', 'bouts', 'with', 'depression', 'since', 'then', '?', '!', 'mr', '-', 'mccain', ':', 'no', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'none', 'whatsoever', '?', '!', 'mr', '-', 'mccain', ':', 'no', '.', 'and', 'that', 'was', 'a', 'very', '-', '-', 'i', 'contemplated', 'an', 'action', 'which', 'i', 'do', 'n', \"'\", 't', 'know', 'i', 'would', 'have', 'carried', 'out', 'because', 'i', 'thought', 'i', '-', '-', 'and', 'did', 'indeed', 'fail', '.', 'and', 'i', 'felt', 'that', 'i', 'had', 'not', 'only', 'failed', 'myself', 'but', 'my', 'family', 'and', 'my', 'fellow', 'prisoners', '.', 'and', 'it', \"'\", 's', 'something', 'that', 'i', 'have', 'to', 'live', 'with', 'all', 'my', 'life', '.', 'but', 'it', 'does', 'n', \"'\", 't', 'particularly', 'de', '##press', 'me', 'anymore', '.', 'as', 'i', 'say', 'in', 'my', 'book', ',', 'you', 'have', 'to', 'learn', 'from', 'lessons', 'in', 'life', 'and', 'move', 'on', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'there', 'was', 'article', 'in', 'the', 'new', 'york', 'times', 'some', 'weeks', 'ago', 'which', 'talked', 'about', 'the', 'current', 'governor', 'of', '!', 'mr', '-', 'russ', '##ert', ':', 'of', 'arizona', '.', '!', 'mr', '-', 'mccain', ':', 'yeah', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'opposing', 'your', 'candidacy', ',', 'talking', 'about', 'your', 'temper', '.', 'the', 'former', 'mayor', 'of', 'phoenix', 'said', 'that', 'he', 'worried', 'about', 'your', 'stability', 'and', 'finger', 'on', 'the', 'button', '.', 'a', 'former', 'attorney', 'general', 'friend', 'of', 'yours', 'said', 'he', 'has', 'n', \"'\", 't', 'spoken', 'to', 'you', 'in', 'a', 'long', 'time', '.', 'your', 'hometown', 'paper', 'had', 'this', 'to', 'say', '.', 'and', 'let', 'me', 'give', 'you', 'a', 'chance', 'to', 'respond', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'sure', '.', '!', 'mr', '-', 'russ', '##ert', ':', '.', '.', '.', 'and', 'try', 'to', 'put', 'this', 'issue', 'to', 'rest', ':', '\"', 'if', 'mccain', 'is', 'truly', 'a', 'serious', 'contender', 'of', 'the', 'presidency', ',', 'it', \"'\", 's', 'time', 'the', 'rest', 'of', 'the', 'nation', 'learned', 'about', 'the', 'john', 'mccain', 'we', 'know', 'in', 'arizona', '.', 'there', 'is', 'reason', 'to', 'seriously', 'question', 'whether', 'mccain', 'has', 'the', 'temperament', 'and', 'political', 'approach', 'and', 'skills', 'we', 'want', 'in', 'the', 'next', 'president', 'of', 'the', 'united', 'states', '.', '\"', 'why', 'are', 'people', 'who', 'know', 'you', 'best', 'suggesting', 'that', 'there', 'may', 'a', 'problem', 'with', 'your', 'temperament', 'to', 'be', 'president', '?', '!', 'mr', '-', 'mccain', ':', 'well', ',', 'actually', ',', 'the', 'people', 'that', 'know', 'me', 'best', ',', 'which', 'is', 'the', 'mayor', 'of', 'phoenix', 'and', 'the', 'mayor', 'republican', 'sheriff', '##s', ',', 'all', 'of', 'the', 'republican', 'mayors', ',', '98', 'percent', 'of', 'those', 'people', 'who', 'i', 'know', 'well', '-', '-', 'in', 'fact', ',', 'perhaps', 'the', 'most', 'important', 'sign', 'is', 'that', '70', 'percent', 'of', 'the', 'people', 'of', 'arizona', 'decided', 'to', 're', '-', 'elect', 'me', 'again', 'to', 'the', 'united', 'states', 'senate', ',', 'including', '55', 'percent', 'of', 'our', 'hispanic', 'population', ',', 'including', 'all', 'of', 'our', 'indian', 'tribes', ',', 'which', 'honors', 'me', 'enormous', '##ly', '.', 'look', ',', 'i', 'hate', 'to', 'get', 'into', 'this', 'with', 'the', 'arizona', 'republic', 'again', ',', 'but', 'the', 'fact', 'is', 'they', 'published', 'a', 'very', 'cruel', 'cartoon', 'about', 'my', 'wife', ';', 'not', 'about', 'me', ',', 'but', 'about', 'my', 'wife', '.', 'i', 'took', 'strong', 'exception', 'to', 'it', 'as', 'a', 'husband', '.', 'i', 'still', 'take', 'strong', 'exception', '.', 'i', 'continue', 'to', 'take', 'strong', 'exception', 'when', 'they', 'ran', 'it', 'at', 'the', 'end', 'the', 'year', 'as', 'one', 'of', 'their', 'best', '-', '-', '\"', 'best', 'cartoons', '.', '\"', 'and', 'my', 'relationship', 'with', 'all', 'of', 'the', 'other', 'newspapers', ',', 'television', 'stations', 'and', 'radio', 'stations', 'in', 'arizona', 'is', 'excellent', '.', 'and', 'i', 'appreciate', 'that', '.', 'unfortunately', ',', 'it', \"'\", 's', 'not', 'the', 'case', 'for', 'that', 'newspaper', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'washington', 'journalist', 'elizabeth', 'drew', 'said', 'that', 'trent', 'republican', 'senators', ',', 'were', 'part', 'of', 'a', 'whispering', 'campaign', 'to', 'suggest', 'that', 'you', 'were', 'not', 'fit', 'to', 'be', 'president', '.', '!', 'mr', '-', 'mccain', ':', 'well', ',', 'i', 'have', '.', '.', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'trent', 'lot', '##t', 'has', 'denied', 'it', '.', 'who', 'do', 'you', 'believe', ',', 'elizabeth', 'drew', 'or', 'trent', 'lot', '##t', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'take', 'trent', 'lot', '##t', 'at', 'his', 'word', '.', 'i', 'ca', 'n', \"'\", 't', '.', '.', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'so', 'elizabeth', 'drew', 'got', 'it', 'wrong', '?', '!', 'mr', '-', 'mccain', ':', 'no', ',', 'i', 'ca', 'n', \"'\", 't', 'know', 'these', 'things', 'and', 'i', 'ca', 'n', \"'\", 't', 'worry', 'about', 'them', '.', 'i', 'admire', 'and', 'respect', 'elizabeth', 'drew', '.', 'she', ',', 'i', 'believe', ',', 'has', 'a', 'well', '-', 'deserved', 'reputation', 'of', 'being', 'an', 'outstanding', 'journalist', '.', 'trent', 'lot', '##t', 'has', 'been', 'a', 'personal', 'friend', 'of', 'mine', 'for', 'many', ',', 'many', 'years', '.', 'i', 'ca', 'n', \"'\", 't', 'get', 'into', 'that', 'kind', 'of', 'thing', '.', 'i', \"'\", 've', 'got', 'to', 'move', 'on', 'with', 'my', 'campaign', ',', 'my', 'agenda', 'for', 'the', 'future', 'of', 'the', 'country', ',', 'tim', '.', 'that', \"'\", 's', 'the', 'important', 'thing', 'here', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'we', \"'\", 're', 'going', 'to', 'get', 'to', 'that', ',', 'but', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'yeah', ',', 'sure', '.', '!', 'mr', '-', 'russ', '##ert', ':', '.', '.', '.', 'when', 'all', 'this', 'came', 'out', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'at', 'the', 'san', 'francisco', 'airport', '.', 'you', 'said', ',', '\"', 'i', 'guess', 'the', 'memo', 'from', 'the', 'bush', 'campaign', 'has', 'come', 'out', 'to', 'attack', 'john', 'mccain', '.', '\"', '!', 'mr', '-', 'mccain', ':', 'i', '.', '.', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'was', 'there', 'any', 'memo', 'or', 'do', 'you', 'regret', 'saying', 'that', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'regret', 'saying', 'it', 'and', 'that', 'i', 'was', 'speaking', 'metaphor', '##ically', '.', 'and', 'ever', 'since', 'then', ',', 'i', \"'\", 've', 'said', ',', '\"', 'look', ',', 'i', 'have', 'no', 'evidence', ',', '\"', 'and', 'the', 'fact', 'is', ',', 'it', \"'\", 's', 'not', 'something', 'that', 'i', 'can', 'worry', 'about', '.', 'i', 'think', 'the', 'people', 'of', 'this', 'country', 'will', 'have', 'ample', 'opportunity', 'to', 'view', 'me', 'as', 'they', 'did', 'the', 'other', 'night', 'and', 'they', 'will', 'again', '.', 'and', 'they', \"'\", 'll', 'be', 'able', 'to', 'make', 'their', 'judgments', 'about', 'me', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'let', 'me', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'i', 'have', 'no', 'doubt', 'about', 'that', ',', 'tim', '.', '!', 'mr', '-', 'russ', '##ert', ':', '.', '.', '.', 'go', 'back', 'to', 'one', 'incident', 'and', 'give', 'you', 'a', 'chance', 'to', 'talk', 'about', 'that', '.', '1992', ',', 'a', 'meeting', 'of', 'the', 'pow', '-', 'mia', 'select', 'committee', '.', 'you', 'and', 'senator', 'chuck', 'grass', '##ley', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'sure', '.', '!', 'mr', '-', 'russ', '##ert', ':', '.', '.', '.', 'face', 'to', 'face', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'sure', '.', '!', 'mr', '-', 'russ', '##ert', 'one', 'senator', 'said', 'he', 'was', 'so', 'concerned', 'that', 'you', 'were', 'going', 'to', 'head', '##bu', '##tt', 'him', 'and', 'seriously', 'in', '##jure', 'him', 'until', 'you', 'stormed', 'out', 'of', 'the', 'room', '.', '!', 'mr', '-', 'mccain', ':', 'oh', ',', 'that', '-', '-', 'first', 'of', 'all', ',', 'i', 'did', 'n', \"'\", 't', 'storm', 'out', 'of', 'the', 'room', '.', 'second', 'of', 'all', ',', 'i', 'took', 'strong', 'exception', 'to', 'his', 'allegations', 'that', 'somehow', 'i', 'was', 'ignoring', 'important', 'evidence', 'concerning', 'the', 'issue', '.', 'listen', ',', 'we', \"'\", 're', 'talking', 'about', 'an', 'issue', 'of', 'literally', 'life', 'and', 'death', 'here', '.', 'chuck', 'grass', '##ley', 'and', 'i', 'are', 'good', 'friends', '.', 'we', 'get', 'along', 'extremely', 'well', '.', 'have', 'him', 'on', 'the', 'program', '.', 'he', \"'\", 'll', 'tell', 'you', 'he', 'and', 'i', 'had', 'a', 'very', 'strong', 'difference', 'on', 'that', 'issue', '.', 'but', 'look', ',', 'when', 'we', \"'\", 're', 'talking', 'about', 'live', 'or', 'dead', 'americans', ',', 'this', 'is', 'serious', 'stuff', '.', 'and', 'i', \"'\", 'll', 'be', 'serious', 'about', 'it', 'and', 'do', 'n', \"'\", 't', 'think', 'i', 'wo', 'n', \"'\", 't', 'be', 'in', 'the', 'future', 'because', 'if', 'someone', 'has', 'information', 'or', 'allegations', 'that', 'there', \"'\", 's', 'an', 'american', 'alive', ',', 'a', 'guy', 'who', 'served', 'with', 'me', 'in', 'the', 'military', ',', 'i', 'will', 'either', 'do', 'everything', 'in', 'my', 'power', 'to', 'get', 'it', 'resolved', ',', 'all', '##ega', '##tion', ',', 'i', 'will', 'react', 'very', 'strongly', 'to', 'it', '.', 'and', 'by', 'the', 'way', ',', 'the', 'families', 'and', 'others', 'expect', 'me', 'to', 'be', 'very', 'strong', 'on', 'that', 'issue', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'how', 'many', 'senators', 'have', 'you', 'apologized', 'to', 'for', 'blowing', 'your', 'stack', '?', '!', 'mr', '-', 'mccain', ':', 'oh', ',', 'maybe', 'three', 'or', 'four', 'from', 'time', 'to', 'time', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'your', 'temper', \"'\", 's', 'under', 'control', '.', '!', 'mr', '-', 'mccain', ':', 'absolutely', '.', 'well', ',', 'again', ',', 'it', \"'\", 's', 'not', 'only', 'under', 'control', ',', 'but', 'i', 'have', 'been', 'through', 'now', '11', ',', '12', 'months', 'of', 'this', 'campaign', '.', 'i', 'have', ',', 'i', 'think', ',', 'conducted', 'this', 'campaign', 'with', 'the', 'dignity', 'that', 'people', 'expect', '.', 'and', 'i', \"'\", 'm', 'very', 'proud', 'of', 'the', 'way', 'that', 'i', 'have', 'conducted', 'this', 'campaign', 'and', 'myself', 'in', 'serving', 'the', 'people', 'of', 'the', 'state', 'of', 'arizona', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'let', 'me', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'look', ',', 'anybody', 'who', 'does', 'n', \"'\", 't', 'want', 'the', 'status', 'quo', 'in', 'washington', ',', 'does', 'n', \"'\", 't', 'want', '-', '-', 'who', 'wants', 'the', 'status', 'quo', 'to', 'remain', 'in', 'washington', 'does', 'not', 'want', 'john', 'mccain', '.', 'let', \"'\", 's', 'make', 'that', 'very', 'clear', ',', 'ok', '?', '!', 'mr', '-', 'russ', '##ert', ':', 'let', \"'\", 's', 'turn', 'to', 'an', 'issue', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'eli', '##an', 'gonzalez', ',', 'young', 'six', '-', 'year', '-', 'old', 'cuban', 'boy', 'on', 'a', 'raft', '.', 'his', 'mother', 'died', '.', 'he', 'was', 'taken', 'into', 'custody', '.', 'he', \"'\", 's', 'in', 'florida', '.', 'fide', '##l', 'castro', 'is', 'demanding', 'his', 'return', '.', 'he', 'said', 'there', \"'\", 'll', 'be', 'huge', 'demonstrations', 'in', 'the', 'streets', 'of', 'havana', 'unless', 'the', 'boy', 'is', 'returned', '.', 'would', 'president', 'mccain', 'author', '##ize', 'the', 'return', 'of', 'that', 'young', 'cuban', 'boy', '?', '!', 'mr', '-', 'mccain', ':', 'of', 'course', 'not', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'you', \"'\", 'd', 'keep', 'him', 'here', '?', '!', 'mr', '-', 'mccain', ':', 'sure', '.', 'freedom', 'is', 'something', 'that', 'i', 'want', 'for', 'all', 'people', ',', 'in', 'cuba', 'and', 'every', 'op', '##pressed', 'country', 'in', 'the', 'world', '.', 'and', 'i', 'certainly', 'would', 'not', 'want', 'to', 'put', 'anybody', 'in', 'the', 'hands', 'of', 'mr', '.', 'castro', 'unless', 'it', \"'\", 's', 'absolutely', 'necessary', 'to', 'do', 'so', '.', 'and', 'certainly', 'not', 'necessary', 'in', 'this', 'case', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'russia', 'continues', 'to', 'bomb', '##ard', 'che', '##ch', '##nya', ',', 'the', 'runaway', 'republic', '.', 'the', 'united', 'states', 'is', 'considering', 'several', 'options', ':', 'cutting', 'off', 'international', 'aid', ',', 'monetary', 'funds', ',', 'loans', '.', 'should', 'that', 'be', 'done', 'now', ',', 'immediately', '?', '!', 'mr', '-', 'mccain', ':', 'it', 'should', 'have', 'been', 'done', 'the', 'day', 'that', 'the', 'first', 'artillery', 'barrage', ',', 'or', 'bombing', 'russians', '.', 'this', 'has', 'serious', 'implications', '.', 'and', ',', 'look', ',', 'i', 'know', 'how', 'you', 'like', 'to', 'rush', 'through', 'these', 'questions', ',', 'but', 'let', 'me', 'say', 'this', 'has', 'significant', 'implications', '.', 'there', \"'\", 's', 'implication', 'of', 'the', 'role', 'of', 'the', 'russian', 'military', 'in', 'russia', ',', 'because', 'they', \"'\", 're', 'clearly', 'asserting', 'themselves', '.', 'it', 'has', 'to', 'do', 'with', 'stability', 'in', 'the', 'region', ',', 'including', 'the', 'bu', '##rgeon', '##ing', 'little', 'democracy', 'called', 'georgia', ',', 'where', 'one', 'of', 'the', 'great', 'heroes', 'of', 'this', 'world', ',', 'mr', '.', 'she', '##vard', '##nad', '##ze', ',', 'has', 'been', 'trying', 'to', 'bring', 'about', 'freedom', 'and', 'democracy', '.', 'it', 'has', 'implications', 'as', 'far', 'as', 'the', 'oil', 'and', 'gas', 'reserves', 'that', 'are', 'in', 'central', 'asia', ',', 'which', 'would', 'play', 'a', 'major', 'role', 'in', 'the', 'future', 'and', 'how', 'that', 'oil', 'and', 'gas', 'gets', 'out', 'of', 'there', ';', 'hopefully', ',', 'through', 'georgia', 'if', 'it', 'was', 'a', 'free', 'and', 'independent', 'nation', '.', 'and', ',', 'finally', ',', 'it', 'has', 'a', 'lot', 'to', 'do', 'with', 'the', 'future', 'of', 'russia', 'itself', ',', 'because', 'the', 'prime', 'minister', \"'\", 's', 'popularity', 'ratings', 'have', 'gone', 'up', 'while', 'the', 'slaughter', 'has', 'begun', '.', 'and', 'one', 'of', 'the', 'things', 'that', 'has', 'continued', '-', '-', 'one', 'of', 'the', 'things', 'that', 'i', 'regret', 'commentary', ',', 'tim', '-', '-', 'because', 'we', 'do', 'n', \"'\", 't', 'see', 'it', 'on', 'television', 'screens', ',', 'it', 'does', 'n', \"'\", 't', 'seem', 'to', 'move', 'us', '.', 'that', \"'\", 's', 'a', 'terrible', 'thing', '.', 'and', 'it', \"'\", 's', 'up', 'to', 'the', 'president', 'of', 'the', 'united', 'states', 'to', 'tell', 'the', 'american', 'people', 'what', \"'\", 's', 'going', 'on', 'here', 'and', 'the', 'implications', 'that', 'it', 'has', 'for', 'the', 'future', '.', 'i', \"'\", 'm', 'sorry', 'for', 'the', 'long', 'answer', ',', 'but', 'it', \"'\", 's', 'a', 'very', 'important', 'question', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'that', \"'\", 's', 'why', 'you', \"'\", 're', 'here', '.', '!', 'mr', '-', 'mccain', ':', 'yes', ',', 'sir', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'using', 'that', 'same', 'standard', ',', 'should', 'we', 'also', 'cut', 'off', 'international', 'lending', ',', 'funding', ',', 'to', 'china', 'as', 'long', 'as', 'they', 'have', 'forced', 'abortion', '##s', '?', 'as', 'long', 'as', 'they', 'deny', 'political', 'freedom', '?', 'as', 'long', 'as', 'they', 'arrest', 'people', 'of', 'religious', 'sect', '##s', '?', '!', 'mr', '-', 'mccain', ':', 'no', ',', 'i', 'do', 'not', 'believe', 'that', '.', 'but', 'i', 'do', 'not', 'believe', 'that', 'they', \"'\", 're', 'our', 'strategic', 'partner', 'either', 'as', 'the', 'president', 'of', 'the', 'united', 'states', 'stated', 'in', '1998', 'when', 'he', 'went', 'to', 'china', '.', 'our', 'strategic', 'partner', 'is', 'japan', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'why', 'the', 'different', 'standard', 'for', 'china', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'bombings', 'and', 'ethnic', 'cleansing', 'and', 'creating', 'hundreds', 'of', 'thousands', 'of', 'refugees', '.', 'and', 'i', \"'\", 'm', 'not', 'ap', '##olo', '##gizing', 'for', 'what', 'they', \"'\", 're', 'doing', '.', 'but', 'there', 'is', 'also', 'the', 'question', 'of', 'how', 'can', 'we', 'have', 'a', 'beneficial', 'effect', 'on', 'china', '?', 'there', \"'\", 's', 'only', 'one', 'time', 'that', 'economic', 'sanctions', 'have', 'worked', 'since', 'the', 'end', 'of', 'world', 'war', 'ii', ',', 'and', 'that', 'was', 'in', 'the', 'case', 'of', 'south', 'africa', '.', 'and', 'that', \"'\", 's', 'where', 'every', 'nation', 'imposed', 'those', 'economic', 'sanctions', '.', 'i', 'believe', '-', '-', 'and', 'i', 'am', 'an', 'eternal', 'opt', '##imi', '##st', ',', 'and', 'i', \"'\", 'm', 'very', 'optimistic', 'about', 'the', 'future', 'of', 'this', 'country', 'and', 'its', 'leadership', 'of', 'the', 'world', '.', 'i', \"'\", 'm', 'very', 'optimistic', 'that', 'with', 'the', 'internet', ',', 'information', 'is', 'knowledge', 'and', 'knowledge', 'is', 'freedom', '.', 'and', 'if', 'we', 'can', 'keep', 'the', 'pressure', 'on', 'the', 'chinese', 'to', 'improve', 'their', 'human', 'rights', ',', 'to', 'stop', 'this', '-', '-', 'terrible', 'things', 'that', 'they', \"'\", 're', 'doing', 'in', 'tibet', ',', 'that', 'we', 'will', 'see', 'democracy', 'and', 'freedom', 'eventually', 'come', 'to', 'china', '.', 'but', ',', 'finally', ',', 'we', 'have', 'to', 'make', 'it', 'very', 'clear', 'to', 'china', 'that', 'any', 'aggression', 'committed', 'against', 'taiwan', 'is', 'a', 'violation', 'of', 'the', 'one', 'china', 'will', 'be', 'peaceful', 'reunification', '.', 'and', 'the', 'consequences', 'of', 'aggression', 'against', 'taiwan', 'would', 'be', 'very', 'significant', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'if', 'you', 'had', 'evidence', 'that', 'saddam', 'hussein', 'had', 'developed', 'weapons', 'of', 'mass', 'destruction', ',', 'would', 'you', 'take', 'those', 'weapons', 'out', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'think', 'that', 'one', 'of', 'the', 'things', 'we', 'can', 'not', 'do', 'as', 'president', 'of', 'the', 'united', 'states', 'is', 'define', 'what', 'we', 'will', 'do', 'specifically', '.', 'i', 'think', 'saddam', 'hussein', 'should', 'know', 'that', 'if', 'he', 'develops', 'those', 'weapons', 'of', 'mass', 'destruction', 'that', 'we', 'will', 'exercise', 'our', 'options', ',', 'but', 'i', 'wo', 'n', \"'\", 't', 'say', 'i', \"'\", 'll', 'take', 'him', 'out', '.', 'i', 'will', 'explore', 'every', 'option', 'that', 'there', 'is', '.', 'but', 'the', 'message', 'to', 'saddam', 'hussein', 'is', 'it', \"'\", 's', 'not', 'acceptable', '.', 'it', \"'\", 's', 'not', 'acceptable', '.', 'and', 'the', 'problem', 'is', 'that', 'too', 'often', ',', 'particularly', 'this', 'president', ',', 'has', 'threatened', 'action', 'and', 'not', 'carried', 'them', 'out', 'wasting', 'the', 'world', \"'\", 's', 'only', 'super', '##power', \"'\", 's', 'most', 'precious', 'asset', ',', 'and', 'that', \"'\", 's', 'our', 'credibility', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'hussein', 'himself', 'saying', 'there', 'will', 'never', 'be', 'inspections', 'again', 'in', 'his', 'country', 'by', 'the', 'united', 'nations', 'unless', 'the', 'economic', 'sanctions', 'are', 'lifted', '.', 'do', 'we', 'lift', ',', 'not', '.', 'and', ',', 'see', ',', 'this', 'is', 'the', 'reason', '-', '-', 'the', 'reason', 'for', 'this', 'is', 'because', 'the', 'leadership', 'this', 'country', 'has', 'exercised', '.', 'we', 'told', 'saddam', 'hussein', 'that', 'unless', 'they', 'allowed', 'the', 'inspectors', 'in', 'we', 'would', 'take', 'military', 'action', 'to', 'force', 'that', 'to', 'happen', '.', 'i', 'mean', ',', 'that', 'was', 'what', 'the', 'president', 'of', 'the', 'united', 'states', 'said', '.', 'we', 'took', 'military', 'action', ',', 'which', 'was', '-', '-', 'well', ',', 'half', '-', 'hearted', 'at', 'best', 'and', 'half', '-', 'measures', 'at', 'best', '.', 'and', 'what', 'was', 'the', 'result', '?', 'have', 'you', 'seen', 'any', 'inspectors', 'anywhere', 'near', 'baghdad', 'lately', '?', 'so', 'we', 'lose', 'our', 'credibility', 'here', '.', 'we', 'lose', 'our', 'credibility', 'when', 'we', 'say', 'that', 'we', 'will', 'not', 'allow', 'the', 'president', 'of', 'taiwan', 'to', 'come', 'to', 'the', 'united', 'states', ',', 'and', 'then', 'have', 'him', 'here', '.', 'that', 'was', 'fine', 'for', 'me', '-', '-', 'come', '.', 'that', 'we', 'lose', 'credibility', 'when', 'we', 'make', 'these', 'idle', 'threats', 'throughout', 'the', 'world', 'and', 'do', 'n', \"'\", 't', 'carry', 'them', 'out', '.', 'that', \"'\", 's', 'why', 'i', \"'\", 'm', 'reluctant', 'to', 'be', 'very', 'specific', '.', 'but', 'at', 'the', 'same', 'time', ',', 'i', 'would', 'our', 'ad', '##vers', '##aries', 'very', 'well', 'aware', 'that', 'the', 'united', 'states', 'treats', 'our', 'role', 'as', 'responsibilities', 'to', 'keep', 'it', 'safe', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'you', 'have', 'said', 'regularly', 'that', 'your', 'top', 'priority', 'as', 'president', 'would', 'be', 'campaign', 'finance', 'reform', ';', 'clean', 'up', 'the', 'system', '.', 'as', 'you', 'know', ',', 'you', 'were', 'opposed', 'to', 'campaign', 'finance', 'reform', 'in', '1987', ',', \"'\", '88', '.', 'you', 'had', 'a', 'come', '-', 'to', '-', 'jesus', 'meeting', 'after', 'charles', 'keating', ',', 'the', 'keating', 'five', ',', 'someone', 'who', 'had', 'bank', '##roll', '##ed', 'your', 'campaigns', '.', 'you', 'were', 'brought', 'before', 'a', 'set', 'of', 'committees', ',', 'found', 'that', 'you', 'had', 'used', 'bad', 'judgment', 'and', 'became', 'an', 'advocate', 'for', 'campaign', 'finance', 'reform', '.', 'senator', 'trent', 'lot', '##t', 'is', 'now', 'saying', 'that', 'when', 'you', \"'\", 're', 'out', 'there', 'raising', 'money', 'right', 'and', 'left', 'and', 'then', 'talking', 'about', 'how', 'you', \"'\", 'd', 'reform', 'the', 'system', ',', 'it', 'rings', 'a', 'little', 'hollow', '.', 'and', 'what', 'he', 'points', 'out', ',', 'and', 'others', ',', 'are', 'that', ',', 'in', 'fact', ',', 'people', 'who', 'go', 'before', 'your', 'committee', ',', 'as', 'chairman', ',', 'have', 'given', 'you', 'lots', 'of', 'money', '-', '-', 'telecommunications', 'industry', ',', '$', '1', 'million', ';', 'aviation', ',', '$', '200', ',', '000', ';', 'gaming', '$', '100', ',', '000', ';', 'railroads', ',', '$', '100', ',', '000', ';', 'liquor', ',', '$', '100', ',', '000', '.', 'and', 'that', 'people', 'who', 'do', 'business', 'with', 'you', 'that', 'corporations', 'that', '-', '-', 'who', 'testify', 'before', 'your', 'committee', '-', '-', 'union', 'pacific', ',', 'news', 'america', ',', 'bells', '##outh', '-', '-', 'provide', 'corporate', 'aircraft', '.', 'are', 'you', 'h', '##yp', '##oc', '##rit', '##ical', 'on', 'this', 'issue', ',', 'senator', '?', '!', 'mr', '-', 'mccain', ':', 'well', ',', 'i', 'do', 'n', \"'\", 't', 'think', 'so', '.', 'you', 'covered', 'a', 'lot', 'of', 'ground', ',', 'and', 'i', \"'\", 'll', 'try', 'and', 'respond', 'in', 'order', '.', 'one', 'is', 'that', 'i', 'was', 'in', 'favor', 'of', 'campaign', 'finance', 'reform', 'as', 'soon', 'as', 'i', 'came', 'to', 'the', 'senate', '.', 'i', 'worked', 'with', 'senator', 'david', 'bore', '##n', 'very', 'closely', 'on', 'the', 'issue', 'of', 'campaign', 'finance', 'reform', '.', 'so', 'i', 'not', 'only', 'did', 'n', \"'\", 't', 'oppose', 'it', ',', 'i', 'was', 'strongly', 'supportive', 'of', 'it', ',', 'as', 'well', 'as', 'other', 'reform', 'issues', ',', 'such', 'as', 'gift', 'ban', ',', 'lobbying', 'ban', ',', 'etc', '.', 'second', 'point', 'is', 'that', 'i', \"'\", 'm', 'very', 'pleased', 'to', 'have', 'received', 'over', '100', ',', '000', 'contributions', 'to', 'my', 'campaign', '.', 'literally', 'every', 'business', 'in', 'america', 'falls', 'under', 'the', 'commerce', 'committee', '.', 'and', 'that', \"'\", 's', 'why', 'the', 'name', 'of', 'it', 'is', 'commerce', '.', 'and', 'i', \"'\", 'm', 'very', 'pleased', 'that', 'i', 'get', 'support', 'from', 'many', 'corporations', 'and', 'companies', 'around', 'america', '.', 'and', 'i', 'restrict', 'those', 'contributions', 'to', '$', '1', ',', '000', ',', 'and', 'i', 'commit', 'and', 'have', 'committed', ',', 'that', 'as', 'a', 'nominee', 'of', 'my', 'party', ',', 'i', 'and', 'my', 'party', 'will', 'have', 'nothing', 'to', 'do', 'with', 'the', 'soft', 'money', ',', 'the', 'huge', 'amounts', 'of', 'un', '##con', '##tro', '##lled', 'contributions', 'that', 'came', 'in', 'in', 'the', '1996', 'campaign', 'to', 'the', 'clinton', '/', 'gore', 'campaign', 'and', 'de', '##base', '##d', 'the', 'institutions', 'of', 'government', '.', 'it', \"'\", 's', 'not', ',', 'tim', ',', 'the', '$', '1', ',', '000', 'contribution', 'that', 'has', 'corrupted', 'our', 'work', 'here', 'in', 'washington', '.', 'it', \"'\", 's', 'the', 'huge', 'un', '##con', '##tro', '##lled', 'now', 'multi', '##bill', '##ion', 'dollars', 'in', 'campaign', 'contributions', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'but', 'the', 'perception', 'that', 'companies', 'that', 'you', 'oversee', 'contribute', 'to', 'your', 'campaign', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'i', 'oversee', 'literally', 'every', 'company', 'in', 'america', ',', 'whether', 'it', 'be', 'trains', ',', 'planes', ',', 'buses', ',', 'telecommunications', ',', 'whatever', 'it', 'is', '.', 'those', 'are', 'my', 'responsibility', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'and', 'you', \"'\", 're', 'not', 'influenced', 'by', 'their', 'money', '?', '!', 'mr', '-', 'mccain', ':', 'of', 'course', 'not', '.', 'of', 'course', 'not', '.', 'in', 'fact', ',', 'if', 'you', 'ask', 'the', 'consumers', 'union', 'and', 'other', 'public', '-', 'advocate', 'organizations', ',', 'they', 'will', 'tell', 'you', 'the', 'vast', 'majority', 'of', 'the', 'time', 'i', 'am', 'on', 'their', 'side', ',', 'rather', 'than', 'the', 'other', 'side', ',', 'particularly', 'where', '-', '-', 'some', 'as', 'we', 'see', 'it', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'but', 'you', 'have', 'said', 'that', 'all', 'politicians', ',', 'including', 'yourself', ',', 'are', 'influenced', 'by', 'big', 'donors', '.', '!', 'mr', '-', 'mccain', ':', 'oh', ',', 'absolutely', '.', 'absolutely', '.', 'absolutely', '.', 'all', 'of', 'us', 'are', 'tainted', 'by', 'this', 'problem', ',', 'and', 'that', \"'\", 's', 'what', 'shame', '##s', 'me', 'so', 'much', ',', 'because', 'i', 'believe', 'that', 'public', 'service', 'is', 'honorable', '.', 'and', ',', 'unfortunately', ',', 'young', 'americans', 'today', 'are', 'beginning', 'to', 'believe', 'that', 'we', 'are', 'not', 'honorable', 'people', '.', 'and', 'there', \"'\", 's', 'poll', 'after', 'poll', 'that', 'show', 'that', '.', 'and', 'they', 'are', 'reflecting', 'that', 'by', 'voting', 'with', 'their', 'feet', 'away', 'from', 'the', 'ballot', 'box', 'and', 'away', 'from', 'public', 'service', '.', 'that', \"'\", 's', 'very', 'disturbing', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'there', \"'\", 's', 'a', 'lot', 'of', 'discussion', 'about', 'john', 'mccain', ',', 'just', 'who', 'he', 'is', '.', 'and', 'people', 'in', 'arizona', ',', 'and', 'others', ',', 'will', 'say', ',', '\"', 'he', \"'\", 's', 'an', 'extremely', 'conservative', 'man', '.', '\"', 'let', 'me', 'go', 'to', 'the', 'new', 'republic', 'and', 'quote', 'and', 'give', 'you', 'a', 'chance', 'to', 'talk', 'about', 'this', 'issue', 'a', 'little', 'bit', 'and', 'put', 'it', 'on', 'the', 'screen', '.', '\"', 'though', 'hailed', 'in', 'the', 'press', 'invariably', 'as', 'a', 'maverick', 'finance', 'reform', 'and', 'tobacco', ',', 'mccain', 'is', ',', 'in', 'fact', ',', 'more', 'conservative', 'than', 'the', 'man', 'whom', 'he', 'succeeded', 'in', 'the', 'senate', ',', 'barry', 'gold', '##water', '.', '\"', 'they', 'go', 'on', '.', '\"', 'he', 'endorsed', 'every', 'item', 'in', 'the', 'contract', 'with', 'america', '.', 'he', 'has', 'opposed', 'federal', 'funding', 'for', 'abortion', '##s', 'and', 'supported', 'a', 'constitutional', 'amendment', 'to', 'ban', 'them', '.', 'mccain', 'voted', 'in', 'the', 'house', 'against', 'making', 'martin', 'luther', 'king', 'day', 'a', 'federal', 'holiday', 'and', 'has', 'recently', 'opposed', 'raising', 'the', 'minimum', 'wage', '.', 'he', 'voted', 'against', 'modest', 'gun', 'controls', ',', 'including', 'the', \"'\", '94', 'assault', 'ban', '-', '-', 'weapons', 'ban', 'and', 'the', \"'\", '93', 'brady', 'bill', '.', '\"', 'john', 'mccain', \"'\", 's', 'very', 'conservative', '.', '!', 'mr', '-', 'mccain', ':', 'john', 'mccain', \"'\", 's', 'a', 'proud', 'conservative', '.', 'john', 'mccain', '-', '-', 'by', 'the', 'way', ',', 'on', 'the', 'martin', 'luther', 'king', 'issue', ',', 'we', 'all', 'learn', ',', 'ok', '?', 'we', 'all', 'learn', '.', 'i', 'will', 'admit', 'to', 'learning', ',', 'and', 'i', 'hope', 'that', 'the', 'people', 'that', 'i', 'represent', 'appreciate', 'that', ',', 'too', '.', 'i', 'voted', 'in', '1983', 'against', 'the', 'recognition', 'of', 'martin', 'luther', 'king', '.', 'it', 'became', 'a', 'huge', 'issue', 'in', 'my', 'state', '.', 'i', \"'\", 'm', 'proud', 'to', 'have', 'been', \"'\", 'm', 'proud', 'to', 'have', 'played', 'one', 'of', 'the', 'leadership', 'roles', 'in', 'seeking', 'and', 'obtaining', 'the', 'membership', '.', '.', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'so', 'you', 'regret', 'that', 'vote', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'regret', 'that', 'vote', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'any', 'other', 'vote', 'there', 'on', 'abortion', 'or', 'gun', 'control', 'you', 'regret', '?', '!', 'mr', '-', 'mccain', ':', 'no', ',', 'but', 'it', \"'\", 's', 'very', 'clear', 'that', 'i', ',', 'and', 'several', 'other', 'senators', ',', 'made', 'sure', 'that', 'the', 'bill', 'that', 'went', 'through', 'the', 'senate', 'had', 'some', 'provisions', 'in', 'it', '.', 'the', 'latest', 'bill', 'concerning', 'guns', 'that', 'went', 'through', 'the', 'senate', 'had', 'some', 'provisions', 'in', 'it', 'which', 'were', 'important', ',', 'such', 'as', 'safety', 'locks', ',', 'such', 'as', 'instant', 'background', 'checks', ',', 'such', 'as', 'some', 'other', 'issues', '.', 'and', 'i', 'fully', 'recognize', 'that', 'we', 'have', 'to', 'exist', 'and', 'enforce', 'existing', 'laws', 'and', 'also', 'take', 'measures', 'such', 'as', 'development', 'of', 'technology', 'that', 'allows', 'only', 'the', 'owner', 'of', 'a', 'gun', 'to', 'fire', 'a', 'gun', '.', 'so', 'i', 'do', 'not', 'deny', 'that', 'there', \"'\", 's', 'a', 'serious', 'problem', 'out', 'there', ',', 'availability', 'of', 'guns', 'to', 'young', 'people', ',', 'which', 'has', 'been', 'against', 'the', 'law', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'would', 'you', 'support', 'the', 'brady', 'bill', 'today', '?', '!', 'mr', '-', 'mccain', ':', 'no', 'need', ',', 'but', 'i', 'certainly', 'support', 'instant', 'background', 'checks', 'and', 'i', 'think', 'that', 'they', 'have', 'been', 'working', 'and', 'will', 'work', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'let', 'me', 'show', 'you', 'one', 'last', 'bite', 'from', 'this', 'article', 'and', 'put', 'it', 'on', 'the', 'screen', 'here', '.', 'and', 'this', 'is', ',', '\"', 'he', 'fought', 'against', 'legislation', 'barr', '##ing', 'job', 'discrimination', 'against', 'homosexual', '##s', '.', 'a', 'keynote', 'speaker', 'at', 'a', 'fund', '-', 'raise', '##r', 'for', 'the', 'oregon', 'citizens', 'alliance', ',', 'an', 'anti', '-', 'homosexual', 'lobby', '.', 'he', \"'\", 's', 'a', 'thousand', 'percent', 'anti', '-', 'gay', ',', 'says', 'barney', 'frank', ',', 'the', 'democratic', 'congressman', 'from', 'massachusetts', ',', 'whom', 'himself', 'is', 'gay', '.', 'he', \"'\", 's', 'not', 'even', 'on', 'the', 'moderate', 'side', 'on', 'abortion', '.', 'the', 'only', 'difference', 'between', 'him', 'and', 'other', 'conservatives', 'is', 'that', 'he', 'bash', '##es', 'with', 'his', 'votes', 'rather', 'than', 'his', 'rhetoric', '.', '\"', 'one', 'simple', 'question', ':', 'would', 'president', 'mccain', 'appoint', 'an', 'openly', 'gay', 'person', 'to', 'a', 'cabinet', 'position', '?', '!', 'mr', '-', 'mccain', ':', 'president', 'mccain', 'would', 'use', 'no', 'lit', '##mus', 'test', 'of', 'any', 'kind', 'in', 'an', 'appointment', 'to', 'a', 'cabinet', 'position', '.', 'but', 'let', 'me', ',', 'if', 'i', 'could', ',', 'respond', 'just', 'a', 'second', '.', 'i', 'was', 'certainly', 'disappointed', 'in', 'congressman', 'frank', \"'\", 's', 'comments', '.', 'yes', ',', 'i', 'support', 'the', 'in', 'the', 'military', '.', 'i', 'do', 'n', \"'\", 't', 'believe', 'that', 'gay', 'marriages', 'should', 'be', 'legal', '##ized', ',', 'the', 'same', 'status', 'as', 'heterosexual', 'marriages', '.', 'but', 'i', 'have', 'supported', 'every', 'effort', 'to', 'end', 'discrimination', '.', 'i', 'have', 'stood', 'up', 'for', 'and', 'defended', 'friends', 'of', 'mine', 'who', 'have', 'been', 'involved', 'in', 'this', 'issue', '.', 'those', 'people', ',', 'in', 'my', 'state', 'and', 'around', 'america', ',', 'know', 'that', '-', '-', 'look', ',', 'i', 'know', 'what', 'discrimination', 'is', 'like', '.', 'and', 'that', 'accusation', ',', 'unfortunately', ',', 'is', 'just', 'patent', '##ly', 'false', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'would', 'you', 'accept', 'someone', 'who', 'is', 'pro', '-', 'choice', 'as', 'your', 'vice', 'president', '?', '!', 'mr', '-', 'mccain', ':', 'again', ',', 'i', 'would', 'use', 'no', 'lit', '##mus', 'test', 'for', 'appointment', 'of', 'a', 'supreme', 'court', 'judge', ',', 'a', 'vice', 'president', 'of', 'the', 'united', 'states', 'or', 'any', 'other', 'position', '.', 'i', 'think', 'people', 'should', 'be', 'judged', 'on', 'their', 'entire', 'record', 'of', 'service', 'to', 'the', 'country', 'and', 'their', 'ability', 'to', 'serve', '.', 'and', 'that', ',', 'i', 'think', ',', 'is', 'the', 'fair', 'and', 'appropriate', 'way', 'of', 'making', 'these', 'judgments', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'governor', 'bush', 'told', 'me', 'that', 'he', 'respected', 'anton', '##in', 'sc', '##alia', 'and', 'clarence', 'thomas', '.', 'and', 'it', \"'\", 's', 'men', 'like', 'those', 'he', 'would', 'appoint', 'to', 'the', '.', 'i', 'would', 'also', 'throw', 'in', 'two', 'good', 'arizona', '##ns', ',', 'sandra', 'day', 'o', \"'\", 'connor', 'and', 'judge', 'william', 're', '##hn', '##quist', ',', 'we', 'were', 'proud', ',', 'as', 'arizona', '##ns', ',', 'to', 'have', '.', 'but', ',', 'yes', ',', 'of', 'course', '.', 'the', 'key', 'is', 'to', ',', 'in', 'my', 'criteria', ',', 'is', 'strict', 'construction', '##ists', '.', 'the', 'more', 'they', 'have', 'adhere', '##d', 'to', 'the', 'constitution', 'of', 'the', 'united', 'states', ',', 'in', 'my', 'view', ',', 'the', 'safer', 'grounds', 'we', 'are', 'on', 'because', 'we', 'can', 'not', 'predict', ',', 'obviously', ',', 'how', 'a', 'person', 'is', 'going', 'to', 'behave', 'as', 'a', 'member', 'of', 'the', 'supreme', 'court', 'except', 'by', 'the', 'basis', 'of', 'their', 'past', 'record', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'would', 'you', 'accept', 'the', 'vice', 'presidency', 'from', 'george', 'bush', '?', '!', 'mr', '-', 'mccain', ':', 'under', 'no', 'circumstances', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'you', 'would', 'not', 'serve', 'as', 'vice', 'president', '?', '!', 'mr', '-', 'mccain', ':', 'oh', ',', 'dear', '.', 'no', ',', 'sir', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'you', 'would', 'not', 'accept', 'the', 'nomination', '?', '!', 'mr', '-', 'mccain', ':', 'under', 'no', 'circumstances', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'none', 'whatsoever', '?', '!', 'mr', '-', 'mccain', ':', 'none', 'whatsoever', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'you', \"'\", 're', 'closing', 'it', 'down', 'completely', '?', '!', 'mr', '-', 'mccain', ':', 'i', \"'\", 'm', 'closing', 'it', 'down', 'completely', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'he', 'comes', 'to', 'you', 'and', 'says', 'i', 'need', 'you', 'in', 'order', 'to', 'win', ',', '\"', 'and', 'you', 'say', '.', '.', '.', '!', 'mr', '-', 'mccain', ':', 'my', 'answer', 'is', ':', '\"', 'mr', '.', 'vice', 'president', ',', 'there', 'are', 'a', 'lot', 'of', '.', '.', '.', '\"', '!', 'mr', '-', 'russ', '##ert', ':', 'mr', '.', 'president', '.', '!', 'mr', '-', 'mccain', ':', 'excuse', 'me', '.', '\"', 'mr', '.', 'president', '\"', '-', '-', 'a', 'freud', '##ian', 'slip', 'there', '.', '\"', 'mr', '.', 'president', ',', 'we', 'have', 'a', 'lot', 'of', 'great', 'patriots', 'in', 'our', 'party', 'and', 'i', 'think', 'they', 'can', 'serve', 'you', 'far', 'more', 'effectively', 'than', 'i', 'can', 'because', 'i', 'think', 'i', 'can', 'serve', 'you', 'more', 'effectively', 'in', 'the', 'united', 'states', 'senate', 'and', 'as', 'chairman', 'of', 'the', 'commerce', 'committee', 'and', 'the', 'influence', 'that', 'i', 'wi', '##eld', 'there', '.', '\"', '!', 'mr', '-', 'russ', '##ert', ':', 'how', 'about', 'secretary', 'of', 'defense', 'or', 'secretary', 'of', 'state', '?', '!', 'mr', '-', 'mccain', ':', 'i', 'think', ',', 'obviously', ',', 'those', 'would', 'be', '-', '-', 'have', 'some', 'attraction', 'associated', 'with', 'them', ',', 'but', 'my', 'first', 'preference', ',', 'obviously', ',', 'would', 'be', 'to', 'remain', 'in', 'the', 'united', 'states', 'senate', '.', 'and', 'if', 'i', 'could', 'just', 'say', ',', 'again', ',', 'i', \"'\", 'm', 'proud', 'of', 'my', 'list', 'of', 'legislative', 'accomplishments', 'in', 'the', 'united', 'states', 'senate', ',', 'including', 'y', '##2', '##k', 'liability', ',', 'including', 'the', 'internet', 'tax', 'prohibition', ',', 'plus', 'mora', '##torium', 'i', \"'\", 'm', 'strongly', 'in', 'favor', 'of', 'a', 'permanent', 'internet', 'tax', 'ban', '.', 'and', 'i', 'think', 'that', 'that', 'can', 'be', 'initiated', 'in', 'this', 'upcoming', 'campaign', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'let', 'me', 'pose', 'another', 'alternative', '.', '!', 'mr', '-', 'mccain', ':', 'sure', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'governor', 'jesse', 'ventura', ',', 'who', 'will', 'be', 'on', 'next', '.', 'i', 'talked', 'to', 'him', 'friday', 'and', 'this', 'is', 'what', 'he', 'had', 'to', 'say', 'about', 'john', 'mccain', ':', '(', 'video', '##ta', '##pe', '-', 'dec', '##em', '##be', ')', '!', 'gov', '-', 'ventura', ':', 'if', 'john', 'mccain', 'works', 'and', 'tries', 'very', 'hard', 'for', 'the', 'republican', 'nomination', 'but', 'by', 'the', 'end', 'of', 'february', ',', 'he', 'realizes', 'that', 'he', \"'\", 's', 'not', 'going', 'to', 'defeat', 'george', 'w', '.', 'bush', 'and', 'be', 'the', 'republican', 'nominee', ',', 'would', 'you', 'make', 'an', 'effort', 'to', 'try', 'to', 'recruit', 'john', 'mccain', 'to', 'be', 'the', 'reform', 'party', 'nominee', '?', 'i', 'certainly', 'would', '-', '-', 'i', 'would', 'enjoy', 'seeing', 'that', 'happen', '.', 'in', 'fact', ',', 'i', 'had', 'spoken', 'to', 'the', 'senator', 'a', 'few', 'months', 'ago', 'about', 'doing', 'just', 'that', '.', 'and', 'he', 'indicated', 'to', 'me', 'that', 'he', 'still', 'wanted', 'to', 'go', 'after', 'the', 'republican', 'nomination', ',', 'but', 'i', 'think', ',', 'in', 'many', 'ways', ',', 'senator', 'mccain', 'is', 'much', 'more', 'a', 'reform', 'party', 'member', 'than', 'he', 'is', 'a', 'republican', 'let', 'me', 'point', 'out', 'governor', 'ventura', 'and', 'i', 'have', 'a', 'lot', 'in', 'common', '.', 'we', 'were', 'both', 'in', 'the', 'united', 'states', 'navy', '.', 'i', 'was', 'a', 'med', '##io', '##cre', 'high', 'school', 'and', 'college', 'wrestler', '.', 'and', 'i', 'wear', 'a', 'feather', 'bo', '##a', 'around', 'the', 'senate', 'on', 'occasion', '.', 'i', 'respect', 'governor', 'ventura', '.', 'i', 'respect', 'what', 'he', 'has', 'done', 'in', 'minnesota', '.', 'i', 'respect', 'his', 'plain', 'talk', 'and', 'i', 'think', 'he', \"'\", 's', 'very', 'good', 'for', 'the', 'political', 'process', '.', 'i', 'love', 'the', 'party', 'of', 'abraham', 'lincoln', '.', 'i', 'cher', '##ish', 'our', 'principles', 'and', 'ideals', '.', 'i', 'believe', 'i', 'can', 'lead', 'our', 'party', 'back', 'to', 'the', 'respect', 'and', 'admiration', 'which', 'it', 'received', 'in', 'previous', 'years', ',', 'particularly', 'during', 'the', '1994', 'election', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'you', 'would', 'never', 'seek', 'the', 'reform', 'party', 'nomination', 'for', 'president', '?', '!', 'mr', '-', 'mccain', ':', 'no', ',', 'no', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'john', 'mccain', ',', 'we', 'have', 'to', 'leave', 'it', 'there', '.', 'we', 'hope', 'you', \"'\", 'll', 'come', 'back', '.', '!', 'mr', '-', 'mccain', ':', 'thank', 'you', ',', 'tim', '.', 'thanks', 'for', 'having', 'me', 'on', '.', '!', 'mr', '-', 'russ', '##ert', ':', 'coming', 'next', ':', 'the', 'reform', 'party', \"'\", 's', 'ross', 'per', '##ot', 'and', 'pat', 'buchanan', 'say', 'free', 'trade', 'hurts', 'the', 'environment', 'and', 'american', 'workers', ',', '-', '-', 'jesse', 'ventura', 'says', 'open', 'trade', 'benefits', 'the', 'economy', '.', 'late', 'friday', ',', 'he', 'told', 'us', 'why', '.', 'then', ':', '\"', 'hillary', \"'\", 's', 'choice', ',', '\"', 'author', 'gail', 'she', '##eh', '##y', 'on', 'why', 'the', 'first', 'lady', 'is', 'running', 'and', 'what', 'she', 'stands', 'for', '.', 'jesse', 'ventura', ',', 'gail', 'she', '##eh', '##y', ',', 'david', 'bro', '##der', ',', 'robert', 'novak', ',', 'andrea', 'mitchell', ',', 'all', 'coming', 'up', 'on', 'meet', 'the', 'press', '.', '(', 'announcements', ')', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oajd5cRhShvS",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1dc279da-9922-4f12-a05e-47d432f0ab9c",
        "id": "2rG9CJ79ShvU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period1_train), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "534 534 534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c_kg-U0UShvW",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.05)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZhOc3RfShvX"
      },
      "source": [
        "###### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aFXmfwP_ShvX",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ca00a416-6464-41c3-e3a7-9dd1f83d8d02",
        "id": "O91nMA3RShvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Using Sigmoid Activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='sigmoid'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "51194ff6-9d44-4423-cc8f-5f935745615f",
        "id": "MRrbeOsoShvb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 507 samples, validate on 27 samples\n",
            "Epoch 1/10\n",
            "507/507 [==============================] - 5s 9ms/step - loss: 0.5705 - acc: 0.9152 - val_loss: 0.2681 - val_acc: 0.9259\n",
            "Epoch 2/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.2434 - acc: 0.9369 - val_loss: 0.2641 - val_acc: 0.9259\n",
            "Epoch 3/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.2383 - acc: 0.9369 - val_loss: 0.2649 - val_acc: 0.9259\n",
            "Epoch 4/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.2353 - acc: 0.9369 - val_loss: 0.2648 - val_acc: 0.9259\n",
            "Epoch 5/10\n",
            "507/507 [==============================] - 3s 7ms/step - loss: 0.2373 - acc: 0.9369 - val_loss: 0.2658 - val_acc: 0.9259\n",
            "Epoch 6/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.2301 - acc: 0.9369 - val_loss: 0.2660 - val_acc: 0.9259\n",
            "Epoch 7/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.2261 - acc: 0.9369 - val_loss: 0.2692 - val_acc: 0.9259\n",
            "Epoch 8/10\n",
            "507/507 [==============================] - 3s 7ms/step - loss: 0.2168 - acc: 0.9369 - val_loss: 0.2672 - val_acc: 0.9259\n",
            "Epoch 9/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.1981 - acc: 0.9369 - val_loss: 0.2589 - val_acc: 0.9259\n",
            "Epoch 10/10\n",
            "507/507 [==============================] - 3s 6ms/step - loss: 0.1469 - acc: 0.9369 - val_loss: 0.2369 - val_acc: 0.9259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0c78119f-5e2b-45ae-a888-9ecaa1e908c8",
        "id": "0BwNwI9BShvd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period0_test.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period0_test.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'the', 'first', 'universal', 'nation', ':', 'leading', 'indicators', 'and', 'ideas', 'about', 'the', 'surge', 'of', 'america', 'in', 'the', '1990s', 'ben', 'j', '.', 'watt', '##enberg', '/', 'the', 'free', 'press', '/', '41', '##8', 'pp', '.', '$', '22', '.', '95', 'there', 'are', 'only', 'two', 'nations', 'on', 'this', 'earth', 'that', 'are', 'simultaneously', 'fantasies', 'and', 'realities', ',', 'those', 'two', 'being', ',', 'of', 'course', ',', 'the', 'tremendous', 'united', 'states', 'of', 'america', 'and', 'tiny', 'israel', '.', 'maybe', 'that', 'is', 'what', 'makes', 'them', ',', 'even', 'when', 'they', \"'\", 're', 'wr', '##ang', '##ling', ',', 'such', 'a', 'nice', 'couple', '.', 'if', 'america', 'con', '##notes', 'new', 'beginnings', ',', 'fresh', 'starts', 'for', 'one', 'and', 'all', ',', 'the', 'meanings', 'of', 'zion', 'are', 'too', 'well', 'known', 'to', 'mention', '.', 'the', 'motto', 'of', 'both', 'of', 'these', 'very', 'peculiar', 'u', '.', 'n', '.', 'members', 'might', 'be', ',', '\"', 'look', 'up', ',', 'better', 'yet', 'come', 'to', 'me', '.', '\"', 'in', 'fact', ',', 'america', 'has', 'sometimes', 'been', 'pictured', 'as', 'a', 'bigger', 'if', 'not', 'better', 'version', 'of', 'zion', ',', 'for', 'example', 'by', 'john', 'winthrop', ',', 'who', 'is', 'quoted', 'in', 'ben', 'watt', '##enberg', \"'\", 's', 'happy', 'new', 'book', 'as', 'comparing', 'massachusetts', 'to', '\"', 'a', 'city', 'upon', 'a', 'hill', '.', '\"', 'put', 'another', 'way', ',', 'just', 'as', 'there', 'are', 'two', 'jerusalem', '##s', ',', 'the', 'heavenly', 'and', 'the', 'earthly', ',', 'not', 'invariably', 'falling', 'somewhat', 'short', 'of', 'the', 'ideal', '.', 'watt', '##enberg', 'is', 'an', 'american', 'who', 'came', 'this', 'close', 'to', 'being', 'an', 'israeli', '.', 'it', 'was', 'because', 'his', 'mother', 'and', 'father', 'left', 'what', 'was', 'then', 'palestine', 'in', 'the', '1920s', 'that', 'he', 'could', 'become', 'a', 'model', 'of', 'that', 'type', 'of', 'first', '-', 'generation', 'american', 'whose', 'pride', 'and', 'belief', 'in', 'his', 'native', 'country', 'is', 'permanent', 'and', 'never', ',', 'ever', 'a', 'secret', '.', 'for', 'years', 'now', ',', 'watt', '##enberg', 'has', 'been', 'airing', 'his', 'views', 'by', 'means', 'of', 'tv', 'appearances', ',', 'his', 'newspaper', 'column', ',', 'and', 'books', '.', 'he', 'also', 'has', 'an', 'appointment', 'at', 'the', 'american', 'enterprise', 'institute', ',', 'one', 'of', 'the', 'too', '-', 'many', 'think', '-', 'tanks', 'in', 'washington', 'that', 'chu', '##rn', 'out', 'position', 'papers', 'for', 'presidential', 'hopeful', '##s', 'to', 'brood', 'on', '.', 'ae', '##i', 'was', 'at', 'one', 'time', 'something', 'special', ',', 'helping', 'give', 'intellectual', 'cache', '##t', 'to', 'the', 'reagan', 'revolution', 'at', 'home', 'and', 'abroad', '.', 'not', 'that', 'watt', '##enberg', 'is', 'a', 'republican', '-', '-', 'like', 'many', 'of', 'the', 'patriots', 'who', 'backed', 'and', 'guided', 'reagan', ',', 'especially', 'the', 'jews', 'among', 'them', ',', 'he', \"'\", 's', 'a', 'refugee', 'from', 'the', 'democratic', 'party', 'who', 'still', 'pray', '##s', 'that', 'it', 'will', 'someday', 'come', 'to', 'its', 'senses', '.', 'in', 'the', 'meantime', ',', 'he', 'continues', 'justify', '##ing', 'his', 'feelings', 'with', 'statistics', '.', 'this', 'is', 'what', 'he', 'does', 'also', 'in', 'his', 'new', 'book', '.', 'it', \"'\", 's', 'a', 'collection', 'of', 'columns', 'and', 'speeches', 'from', 'the', 'day', 'reagan', 'took', 'office', 'through', 'the', 'day', 'saddam', 'took', 'kuwait', ',', 'always', 'chat', '##ty', ',', 'always', 'upbeat', ',', 'always', 'contrary', 'to', 'the', 'conventional', 'liberal', 'wisdom', ',', 'sp', '##lice', '##d', 'together', 'by', 'commentary', ',', 'autobiography', ',', 'and', 'lots', 'of', 'numbers', '.', '\"', 'some', '65', 'percent', 'of', 'americans', ',', '\"', 'we', 'learn', ',', '\"', 'believe', 'there', 'is', 'a', 'personal', 'god', ',', 'compared', 'with', '.', '.', '.', '19', 'percent', 'of', 'sw', '##edes', '.', '\"', 'watt', '##enberg', \"'\", 's', 'point', 'is', 'that', 'americans', 'are', 'different', ',', 'and', ',', 'yes', ',', 'better', ',', 'deserved', '##ly', 'still', 'the', 'envy', ',', 'ideal', ',', 'and', 'hope', 'of', 'the', 'world', '.', 'reprinted', 'here', 'are', 'prophet', '##ic', 'items', 'on', 'the', 'collapse', 'of', 'communism', 'and', 'how', 'america', 'can', 'haste', '##n', 'it', '.', 'there', \"'\", 's', 'something', 'of', 'a', 'defense', 'of', 'ollie', 'north', ',', 'and', 'an', 'accurate', 'prediction', 'of', 'the', 'fate', 'of', 'the', 'sand', '##ini', '##sta', '##s', 'at', 'the', 'ballot', 'box', '.', 'there', 'are', 'statistics', 'according', 'to', 'which', 'poverty', 'in', 'america', 'is', 'down', ',', 'income', 'up', ',', 'and', 'the', 'high', 'school', 'drop', '-', 'out', 'rate', 'at', 'an', 'all', '-', 'time', 'low', '.', 'confident', 'than', 'ever', '.', 'and', 'predictions', 'that', 'the', 'best', 'is', 'yet', 'to', 'come', 'for', 'the', '\"', 'first', 'universal', 'nation', '.', '\"', 'the', 'term', 'is', 'watt', '##enberg', \"'\", 's', 'alias', 'for', 'his', 'country', 'on', 'the', 'morrow', 'of', 'the', 'cold', 'war', '.', 'having', 'won', 'it', ',', 'the', 'u', '.', 's', '.', 'is', 'en', '##sco', '##nce', '##d', '-', '-', 'politically', ',', 'mil', '##ita', '##rily', ',', 'and', ',', 'most', 'important', ',', 'culturally', '-', '-', 'as', '\"', 'number', 'one', '\"', 'everywhere', ',', 'to', 'use', 'his', 'language', '.', 'even', 'when', 'it', 'comes', 'to', 'making', 'and', 'selling', ',', 'he', \"'\", 's', 'not', 'depressed', 'by', 'the', 'competition', '.', 'his', 'figures', 'say', 'that', 'the', 'american', 'birth', '##rate', 'is', 'rising', ',', 'but', 'that', 'western', 'europe', \"'\", 's', 'is', 'stuck', 'at', 'below', 'replacement', 'level', 'and', 'japan', \"'\", 's', 'is', 'below', 'r', '##l', 'and', 'tumbling', '.', 'a', 'high', 'birth', '##rate', 'and', 'large', '-', 'scale', 'immigration', 'ensure', 'american', 'economic', 'supremacy', ',', 'because', 'shrinking', ',', 'aging', 'populations', 'ca', 'n', \"'\", 't', 'compete', 'or', 'grow', '.', 'not', 'that', 'watt', '##enberg', 'wants', 'the', 'readers', 'of', 'his', 'book', 'to', 'relax', '-', '-', 'this', 'is', 'a', 'pep', 'talk', '.', 'far', 'from', 'pulling', 'in', 'its', 'horns', ',', 'as', 'pat', 'buchanan', 'would', 'have', 'it', 'do', ',', 'victorious', 'america', 'should', 'embrace', 'its', 'destiny', 'an', '##ew', '.', 'this', 'means', 'continuing', 'to', 'push', 'for', 'democracy', 'worldwide', ',', 'but', 'way', 'of', 'life', 'as', 'well', 'as', 'a', 'political', 'system', ',', 'is', 'not', 'the', 'same', 'as', ',', 'say', ',', 'the', 'pinched', 'stu', '##lt', '##ification', 'of', 'swedish', '##sty', '##le', 'democracy', '.', 'and', 'in', 'eastern', 'europe', 'today', ',', 'it', \"'\", 's', 'said', 'that', 'the', 'newly', 'free', 'countries', 'should', 'go', 'swedish', '.', 'american', 'taxpayers', 'did', 'n', \"'\", 't', 'spend', 'trillion', '##s', 'on', 'the', 'cold', 'war', 'to', 'create', 'more', 'sweden', '##s', '.', '.', '.', '.', 'as', 'the', 'last', 'super', '##power', 'we', 'ought', 'to', 'try', 'to', 'shape', 'evolution', '.', 'watt', '##enberg', 'is', 'sang', '##uin', '##e', 'that', 'america', 'can', 'do', 'this', 'if', 'it', 'even', 'half', '-', 'tries', ',', 'because', 'culturally', 'the', 'u', '.', 's', '.', 'is', 'un', '##ri', '##vale', '##d', ',', 'and', '\"', 'it', \"'\", 's', 'culture', 'that', 'counts', '.', '\"', 'while', 'the', 'japanese', 'may', 'make', 'and', 'sell', 'all', 'the', 'vc', '##rs', ',', 'almost', 'all', 'the', 'software', 'is', 'red', '##w', '##hit', '##e', '-', 'and', '-', 'blue', ',', 'and', 'it', \"'\", 's', 'ko', '##jak', 'and', 'madonna', 'who', 'are', 'members', 'of', 'the', 'family', 'on', 'every', 'continent', '.', 'the', 'fact', 'that', 'the', 'culture', 'exported', 'by', 'the', 'u', '.', 's', '.', 'to', 'a', 'fascinated', 'and', 'ins', '##ati', '##able', 'universe', 'is', 'mostly', 'garbage', 'does', 'n', \"'\", 't', 'matter', '.', 'what', \"'\", 's', 'important', 'is', 'that', 'the', 'sounds', 'and', 'images', 'convey', 'the', 'message', 'of', 'freedom', ',', 'of', 'infinite', 'tolerance', 'and', 'opportunity', ',', 'the', 'opposite', 'of', 'stu', '##lt', '##ification', '.', 'the', 'whole', 'world', 'seems', 'to', 'be', 'getting', 'the', 'message', 'the', 'cold', 'war', '.', 'the', 'ongoing', 'proof', 'is', ',', 'at', 'one', 'end', 'of', 'the', 'continuum', ',', 'all', 'the', 'foreign', 'imitation', '##s', 'of', 'american', 'sc', '##hl', '##ock', ',', 'and', 'at', 'the', 'other', ',', 'the', '\"', 'foreigners', 'flock', '##ing', 'to', 'american', 'universities', 'because', 'they', 'know', 'they', 'are', 'the', 'best', 'in', 'the', 'world', '.', '\"', 'so', 'far', ',', 'it', 'sounds', 'as', 'if', 'watt', '##enberg', 'were', 'a', 'patriotic', 'polly', '##anna', '.', 'has', 'the', 'us', '.', ',', 'functioning', 'as', 'the', 'benign', 'empire', ',', 'brought', 'the', 'heavenly', 'jerusalem', 'down', 'to', 'earth', '?', 'not', 'really', '.', '\"', 'america', 'has', 'some', 'severe', 'problems', ',', '\"', 'he', 'con', '##cede', '##s', '.', 'among', 'these', 'are', 'the', 'level', 'of', 'ordinary', 'schooling', ',', 'which', 'is', 'awful', ',', 'the', 'ago', '##nies', 'of', 'a', '\"', 'black', 'community', 'which', 'is', 'clearly', 'a', 'troubled', 'one', ',', '\"', 'the', 'deficit', ',', 'and', 'the', 'savings', 'and', 'loan', 'de', '##ba', '##cle', '.', 'but', 'for', 'watt', '##enberg', ',', 'none', 'of', 'these', '\"', 'severe', 'problems', '\"', 'arises', 'from', 'any', 'basic', 'failure', 'of', 'the', 'american', 'way', ',', 'nor', 'does', 'he', 'doubt', 'for', 'a', 'minute', 'that', 'america', 'can', 'get', 'over', 'them', '.', 'the', 'ill', '##iter', '##acy', 'produced', 'in', 'most', 'high', 'schools', 'is', 'the', 'direct', 'though', 'not', 'necessary', 'consequence', 'of', 'an', 'ad', '##mir', '##able', 'decision', 'to', 'try', 'to', 'educate', 'everyone', ',', '.', '\"', 'the', 'answer', 'is', 'to', 'introduce', 'higher', 'standards', 'and', 'enforce', 'discipline', 'for', 'everyone', '.', 'the', 'blacks', 'are', 'in', 'trouble', ',', 'but', 'there', 'are', 'more', 'and', 'more', 'black', 'university', 'graduates', 'and', 'a', 'growing', 'black', 'middle', 'class', ',', 'which', 'will', 'give', 'those', 'left', 'in', 'the', 'sl', '##ums', 'a', 'model', 'of', 'health', '##ier', 'behavior', '.', 'the', 'deficit', 'is', 'shrinking', '.', 'crime', '?', '\"', 'there', 'is', 'still', 'plenty', 'to', 'do', '.', '\"', 'the', 'people', 'who', 'looted', 'the', 's', '&', 'l', '##s', ';', '\"', 'ought', 'to', 'go', 'to', 'jail', '.', '\"', 'drugs', '?', 'do', 'n', \"'\", 't', 'legal', '##ize', 'them', ',', 'fight', 'them', '.', 'which', 'is', 'to', 'say', 'that', ',', 'although', 'watt', '##enberg', 'knows', 'that', 'things', 'are', 'n', \"'\", 't', 'perfect', 'at', 'home', ',', 'he', 'counts', 'on', 'america', 'to', 'pull', 'its', 'socks', 'up', 'as', 'it', 'has', 'done', 'before', ',', 'to', 'live', 'up', 'to', 'its', '\"', 'remarkable', 'record', 'of', 'dealing', 'with', 'our', 'own', 'problems', '.', '\"', 'above', 'all', ',', 'he', 'wants', 'to', 'di', '##sp', '##el', 'the', 'st', '##yl', '##ish', ',', 'dangerous', ',', 'persistent', 'cl', '##iche', 'that', 'there', \"'\", 's', 'something', 'basically', 'wrong', ',', 'that', 'america', 'has', 'lost', 'its', 'touch', '.', 'after', 'a', 'delay', 'of', 'only', 'a', 'few', 'months', ',', 'american', 'bestseller', '##s', 'reach', 'israel', ',', 'giving', 'reviewers', 'here', 'a', 'pretty', 'good', 'idea', 'contend', 'with', '.', '\"', 'decline', '\"', 'and', '\"', 'eclipse', '\"', 'seem', 'to', 'be', 'the', 'catch', '##words', '.', 'victory', 'in', 'the', 'cold', 'war', 'has', 'spawned', 'books', 'claiming', 'with', 'figures', 'and', 'rhetoric', 'that', 'america', \"'\", 's', 'time', 'has', 'come', 'and', 'gone', '-', '-', 'the', 'one', 'that', 'stayed', 'popular', 'longest', 'was', 'paul', 'kennedy', \"'\", 's', 'the', 'rise', 'and', 'fall', 'of', 'the', 'great', 'powers', '.', 'the', 'retirement', 'of', 'ronald', 'reagan', 'was', 'followed', 'by', 'books', 'de', '##pl', '##orin', '##g', 'the', 'effects', 'of', 'his', 'revolution', 'states', '##ide', '-', '-', 'the', 'best', 'known', 'is', 'the', 'politics', 'of', 'rich', 'and', 'poor', ',', 'by', 'kevin', 'phillips', '.', 'that', 'their', 'books', 'and', 'others', 'on', 'how', 'to', 'compete', 'with', 'the', 'japanese', 'should', 'be', 'bestseller', '##s', 'says', 'something', 'about', 'america', 'west', 'of', 'the', 'hudson', '.', 'indeed', ',', 'if', 'polls', 'are', 'to', 'be', 'trusted', 'any', 'more', 'than', 'statistics', ',', 'the', 'man', '-', 'in', '-', 'the', '-', 'street', 'is', 'feeling', 'lou', '##sy', 'about', 'his', 'country', '.', 'a', 'gall', '##up', 'poll', 'last', 'november', 'discovered', 'that', '64', 'percent', 'of', 'americans', 'were', '\"', 'unhappy', 'about', 'how', 'things', 'are', 'going', '.', '\"', 'likewise', ',', 'a', 'time', '/', 'cnn', 'survey', 'in', 'october', ',', 'asking', 'the', 'question', ',', '\"', 'how', 'well', 'do', 'you', 'think', 'things', 'are', 'going', 'in', 'the', 'u', '.', 's', '.', 'these', 'days', '?', '\"', 'got', 'only', '42', 'percent', 'of', 'a', 'representative', 'well', ',', '\"', 'compared', 'with', '67', 'percent', 'half', 'a', 'year', 'before', '.', 'possibly', ',', 'if', 'a', 'fresh', 'sounding', 'were', 'taken', ',', 'the', 'opt', '##imi', '##sts', 'would', 'pre', '##va', '##il', 'again', '.', 'even', 'if', 'they', 'did', ',', 'these', 'drastic', 'ups', 'and', 'downs', 'betray', 'something', 'fever', '##ish', 'in', 'the', 'american', 'psyche', '.', 'optimism', 'and', 'pe', '##ssi', '##mism', 'are', 'always', ',', 'of', 'course', ',', 'mostly', 'in', 'the', 'mind', '.', 'when', 'watt', '##enberg', 'and', 'others', 'who', 'feel', 'and', 'hope', 'as', 'he', 'does', 'about', 'america', '-', '-', 'one', 'thinks', 'of', 'george', 'will', 'in', 'his', 'most', 'recent', 'book', 'suddenly', ',', 'joseph', 'ny', '##e', 'in', 'bound', 'to', 'lead', ',', 'and', 'alfred', 'bal', '##k', 'in', 'the', 'myth', 'of', 'american', 'eclipse', '-', '-', 'marshal', 'their', 'figures', 'and', 'rhetoric', ',', 'they', \"'\", 're', 'trying', 'to', 'make', 'history', 'by', 'per', '##su', '##ading', 'their', 'fellow', 'citizens', 'to', 'think', ',', 'and', 'therefore', 'act', ',', 'like', 'winners', ',', 'to', 'explore', 'again', 'the', 'power', 'of', 'positive', 'thinking', '.', 'no', 'one', ',', 'least', 'of', 'all', 'an', 'israeli', ',', 'can', 'predict', 'whether', 'they', \"'\", 'll', 'be', 'able', 'to', 'pull', 'off', 'this', 'trick', '.', 'there', 'are', 'two', 'possibilities', ',', 'equally', 'plausible', 'as', 'this', 'review', 'is', 'being', 'written', ',', 'days', 'before', 'the', 'u', '.', 'n', '.', 'ultimatum', 'to', 'saddam', 'comes', 'due', '.', 'historians', 'who', 'are', 'now', 'will', 'either', 'agree', 'that', 'the', 'american', 'century', ',', 'an', 'abbreviated', 'one', ',', 'started', 'with', 'hiroshima', 'and', 'ended', 'with', 'the', 'evacuation', 'of', 'the', 'american', 'embassy', 'in', 'saigon', ',', 'soon', 'after', 'which', 'germany', ',', 'japan', ',', 'and', 'iraq', 'took', 'their', 'places', 'in', 'the', 'sun', ',', 'or', 'that', 'the', 'empire', ',', 'rallied', 'by', 'reagan', 'and', 'mobilized', 'by', 'his', 'successor', ',', 'struck', 'back', 'master', '##fully', '.', 'watt', '##enberg', 'saw', 'this', 'back', 'in', 'august', ',', 'and', 'wrote', ',', '\"', 'if', 'america', 'acts', 'firmly', 'on', 'the', 'iraqi', 'invasion', 'of', 'kuwait', 'then', 'the', 'u', '.', 's', '.', 'will', 'be', 'able', 'to', 'tell', 'him', ',', \"'\", 'thanks', 'saddam', ',', 'we', 'needed', 'that', \"'\", 'and', 'can', 'go', 'ahead', 'fashion', '##ing', 'a', 'new', 'world', 'order', '.', '\"', 'victory', 'over', 'saddam', 'would', 'send', 'america', \"'\", 's', 'stock', 'soaring', 'worldwide', '.', 'it', 'might', 'also', 'brace', 'the', 'american', 'psyche', ',', 'which', ',', 'god', 'knows', ',', 'needs', 'a', 'tonic', '.', 'as', 'an', 'ax', '-', 'american', 'who', 'moved', 'to', 'israel', 'years', 'ago', ',', 'who', 'still', 'roots', 'for', 'the', 'pittsburgh', 'pirates', 'and', 'pays', 'occasional', 'visits', 'to', 'the', 'old', 'country', ',', 'i', 'find', 'the', 'streets', 'of', 'new', 'york', 'and', 'washington', 'mean', '##er', 'with', 'each', 'visit', '.', 'worse', 'than', 'that', ',', 'my', 'old', 'friends', 'are', 'ever', 'bitter', '##er', 'and', 'the', 'homeless', ',', 'the', 'strung', '-', 'out', ',', 'the', 'menacing', 'leap', 'out', 'at', 'you', '.', 'the', 'american', 'middle', 'class', ',', 'ever', 'prey', 'to', 'the', 'twin', 'fears', 'of', 'getting', 'mug', '##ged', 'and', 'getting', 'fired', ',', 'has', 'long', 'since', 'adapted', 'to', 'them', '.', 'the', 'visitor', 'finds', 'america', 'enormous', ',', 'abundant', 'as', 'always', ',', 'and', 'wa', '##cky', '.', 'commercial', 'tv', 'is', 'idiot', '##ic', ',', 'non', '##com', '##mer', '##cial', 'creepy', ',', 'and', 'no', 'one', 'under', 'thirty', '-', 'five', 'can', 'form', 'a', 'complete', 'sentence', '.', 'there', 'does', 'n', \"'\", 't', 'seem', 'to', 'be', 'any', 'sort', 'of', 'ruling', 'class', 'anymore', '.', 'genuine', 'decade', '##nce', 'is', 'gaining', ',', 'innocence', 'retreating', '.', 'the', 'coastal', 'downtown', '##s', 'and', 'detroit', 'have', 'become', 'black', 'homeland', '##s', ',', 'no', '-', 'go', 'zones', 'reminiscent', 'of', 'certain', 'parts', 'of', 'certain', 'middle', 'eastern', 'cities', '-', '-', 'it', 'would', 'be', 'interesting', 'to', 'know', 'when', 'the', 'last', 'time', 'was', 'that', 'watt', '##enberg', 'walked', 'home', 'from', 'the', 'ae', '##i', '.', 'a', 'visitor', 'who', 'wishes', 'america', 'well', 'and', 'always', 'despised', 'henry', 'adams', 'has', 'to', 'wonder', 'at', 'watt', '##enberg', \"'\", 's', 'confidence', '.', 'it', 'takes', 'some', 'reading', 'between', 'the', 'lines', 'in', 'the', 'first', 'universal', 'nation', ',', 'and', 'a', 'look', 'at', 'an', 'earlier', ',', 'less', 'happy', 'book', 'of', 'his', ',', 'to', 'realize', 'that', 'watt', '##enberg', 'has', 'secret', 'doubts', 'himself', '.', 'that', 'book', 'was', 'the', 'americans', 'of', 'european', 'ancestry', 'were', 'having', 'fewer', 'than', 'the', '2', '.', '1', 'children', 'per', 'woman', 'needed', 'to', 'replace', 'themselves', ',', 'while', 'those', 'of', 'non', '-', 'european', 'ancestry', ',', 'including', 'blacks', 'and', 'immigrants', 'from', 'the', 'third', 'world', ',', 'were', 'having', 'more', '.', 'this', 'trend', 'worried', 'watt', '##enberg', 'sufficiently', 'to', 'inspire', 'an', 'entire', 'book', '.', 'he', 'feared', 'that', ',', 'while', 'the', 'melting', 'pot', 'could', 'cope', 'brilliant', '##ly', 'with', 'people', 'from', 'the', 'area', 'del', '##imi', '##ted', 'by', 'ireland', ',', 'scandinavia', ',', 'russia', ',', 'and', 'sicily', ',', 'it', 'was', 'liable', 'to', 'crack', 'if', 'it', 'had', 'to', 'ass', '##imi', '##late', 'the', 'whole', 'world', 'in', 'proportions', 'such', 'as', 'would', 'alter', 'america', \"'\", 's', 'complexion', 'from', 'white', 'to', 'colored', '.', 'e', 'pl', '##uri', '##bus', 'un', '##um', ',', 'yes', ',', 'but', 'only', 'to', 'a', 'point', '.', 'his', 'solution', '?', 'prop', '##aga', '##ndi', '##ze', 'white', 'women', 'to', 'have', 'more', 'babies', ';', 'make', 'it', 'a', 'paying', 'proposition', ',', 'as', 'in', 'sweden', '.', 'that', 'was', 'honest', 'of', 'him', ',', 'indeed', 'brave', '-', '-', 'he', 'exposed', 'himself', 'to', 'charges', 'of', 'racism', '.', 'watt', '##enberg', 'is', 'less', 'ad', '##mir', '##able', ',', 'because', 'less', 'forth', '##right', ',', 'in', 'his', 'new', 'book', ',', 'when', 'confronting', 'what', 'is', 'basically', 'the', 'same', 'subject', '.', 'first', ',', 'he', 'reports', 'that', 'the', 'children', '-', 'per', '-', 'woman', 'figure', 'in', 'the', 'u', '.', 's', '.', 'is', 'climbing', ',', 'is', 'much', 'better', 'than', 'japan', \"'\", 's', '.', 'he', 'forget', '##s', ',', 'however', ',', 'to', 'break', 'the', 'american', 'figure', 'down', 'into', 'its', 'white', 'and', 'non', '-', 'white', 'components', '.', 'instead', ',', 'he', 'plump', '##s', 'for', 'more', 'immigration', ',', 'along', 'lines', 'a', 'reader', 'has', 'to', 'think', 'about', 'before', 'understanding', 'the', 'likely', 'demographic', 'ups', '##hot', '.', 'far', 'from', 'being', 'reassured', 'by', 'the', 'rev', '##iving', 'birth', 'rate', ',', 'watt', '##enberg', 'is', 'ready', 'to', 'try', 'to', 'use', 'the', 'laws', ',', 'those', 'on', 'the', 'books', 'and', 'those', 'to', 'be', 'added', ',', 'to', 'keep', 'america', 'basically', 'white', '.', 'illegal', 'immigration', ',', 'running', 'into', 'the', 'millions', 'and', 'stemming', 'mainly', 'from', 'the', 'third', 'world', ',', 'would', 'be', 'suppressed', 'through', 'enforcement', 'of', 'existing', 'laws', '.', 'meanwhile', ',', 'legal', 'quota', '##s', 'would', 'be', 'increased', 'and', 'criteria', 'for', 'admission', 'reformed', 'so', 'as', 'to', 'give', 'preference', 'to', 'people', 'with', 'education', ',', 'skills', ',', 'and', 'capital', '.', 'the', 'net', 'effect', '?', 'more', 'new', 'americans', 'from', 'europe', ',', 'fewer', 'from', 'elsewhere', '.', 'about', 'the', 'first', 'outcome', ',', 'watt', '##enberg', 'is', 'explicit', '.', 'the', 'second', 'he', 'leaves', 'to', 'be', 'in', '##fer', '##red', '.', 'in', 'other', 'words', ',', 'he', \"'\", 's', 'pe', '##ssi', '##mist', '##ic', 'or', 'realistic', 'enough', 'to', 'believe', 'that', ',', 'while', 'the', 'first', 'universal', 'nation', 'should', 'go', 'on', 'export', '##ing', 'its', 'ideas', 'ca', 'n', \"'\", 't', '-', '-', 'nor', 'should', 'it', '-', '-', 'try', 'to', 'take', 'in', 'the', 'huddled', ',', 'mobile', 'masses', 'from', 'mexico', 'to', 'the', 'philippines', 'to', 'bangladesh', 'to', 'egypt', ',', 'not', 'if', 'it', 'wants', 'to', 'preserve', 'a', 'se', '##mb', '##lance', 'of', 'peace', 'in', 'its', 'streets', 'and', 'remain', 'number', 'one', '.', 'interesting', '##ly', 'enough', ',', 'congress', 'has', 'now', 're', '##written', 'the', 'immigration', 'laws', 'roughly', 'as', 'watt', '##enberg', 'suggested', '.', 'the', 'number', 'of', 'visas', 'granted', 'yearly', 'has', 'been', 'raised', 'from', '500', ',', '000', 'to', '700', ',', '000', ',', 'with', 'the', 'st', '##ip', '##ulation', 'that', 'most', 'of', 'the', 'added', 'number', 'be', 'not', 'for', 'relatives', 'of', 'earlier', 'immigrants', ',', 'but', 'for', 'skilled', 'or', 'rich', 'people', '.', 'not', 'for', 'the', 'first', 'time', 'in', 'its', 'short', 'history', ',', 'america', 'is', 'trying', 'to', 'reconcile', 'its', 'pie', '##ties', 'with', 'its', 'interests', ',', 'its', 'past', 'with', 'its', 'future', ',', 'its', 'dreams', 'with', 'its', 'nightmares', '.', 'meanwhile', ',', 'laws', 'or', 'no', 'laws', ',', 'the', 'mexican', '##s', ',', 'filipino', '##s', ',', 'and', ',', 'yes', ',', 'the', 'israelis', 'keep', 'see', '##ping', 'and', 'sneaking', 'in', '.', 'israel', 'is', 'the', 'other', 'fantastic', 'country', 'in', 'formation', '.', 'if', 'america', \"'\", 's', 'symbol', 'is', 'the', 'golden', 'door', ',', 'the', 'jewish', 'state', \"'\", 's', 'is', 'the', 'ing', '##ath', '##ering', 'of', 'the', 'exiles', ',', 'an', 'ideal', 'somewhat', 'similar', 'and', 'altogether', 'different', 'the', 'law', 'of', 'return', ',', 'giving', 'all', 'jews', 'the', 'right', 'to', 'enter', 'israel', 'and', 'get', 'citizenship', 'at', 'any', 'time', '.', 'you', 'have', 'to', 'be', 'a', 'very', 'important', 'parole', '-', 'breaker', ',', 'a', 'morton', 'sob', '##ell', 'or', 'meyer', 'lan', '##sky', ',', 'to', 'be', 'turned', 'back', '.', 'otherwise', ',', 'no', 'matter', 'what', 'the', 'burden', ',', 'no', 'matter', 'what', 'the', 'pain', ',', 'cost', ',', 'or', 'confusion', ',', 'no', 'matter', 'what', 'the', 'economists', 'or', 's', '##nob', '##s', 'say', ',', 'all', 'the', 'jews', 'who', 'knock', 'on', 'israel', \"'\", 's', 'doors', 'are', 'welcomed', '.', 'today', ',', 'this', 'means', 'a', 'million', 'or', 'more', 'refugee', '-', 'immigrants', 'from', 'the', 'di', '##sin', '##te', '##grating', 'soviet', 'union', '.', 'the', 'young', 'and', 'the', 'old', ',', 'the', 'wicked', 'and', 'the', 'good', ',', 'the', 'capitalist', '##s', 'and', 'the', 'stalin', '##ists', ',', 'the', 'healthy', 'and', 'the', 'hiv', '-', 'positive', ',', 'are', 'all', 'being', 'allowed', 'to', 'stream', 'in', 'on', 'principle', ',', 'and', 'ben', 'gu', '##rion', 'airport', 'is', 'a', 'throw', '##back', 'to', 'ellis', 'island', 'in', 'the', 'old', 'days', '.', 'whether', 'israel', 'will', 'rise', 'on', 'this', 'tide', 'or', 'drown', 'in', 'it', 'is', 'a', 'hard', 'question', '.', 'what', \"'\", 's', 'clear', 'is', 'that', 'most', 'of', 'these', 'jews', 'are', 'coming', 'here', 'only', 'because', 'the', 'u', '.', 's', '.', 'quota', 'is', 'filled', '.', 'they', \"'\", 're', 'moving', 'to', 'zion', 'not', 'because', 'they', \"'\", 're', 'will', 'take', 'them', 'in', '.', 'similarly', ',', 'the', 'israelis', 'and', 'others', 'who', 'continue', 'to', 'sneak', 'into', 'america', 'do', 'so', 'not', 'because', 'it', \"'\", 's', 'better', 'than', 'ever', ',', 'as', 'native', 'son', 'ben', 'watt', '##enberg', 'would', 'have', 'it', ',', 'or', 'because', 'its', 'prospects', 'are', 'great', ',', 'but', 'because', ',', 'saddam', 'dead', 'or', 'alive', ',', 'it', \"'\", 's', 'the', 'only', 'america', 'around', '.', 'illustration', 'the', 'colonel', ':', 'the', 'life', 'and', 'wars', 'of', 'henry', 'st', '##im', '##son', ',', '1867', '-', '1950', 'philip', 'ter', '##zia', '##n', 'godfrey', 'hodgson', '/', 'alfred', 'a', '.', 'kn', '##op', '##f', '/', '402', '##pp', '.', '$', '24', '.', '95', 'the', 'next', 'time', 'david', 's', '.', 'bro', '##der', 'or', 'r', '.', 'w', '.', 'apple', ',', 'jr', '.', ',', 'or', 'george', 'f', '.', 'will', ',', 'or', 'any', 'other', 'of', 'our', 'pol', '##itic', '##o', '-', 'journalist', '##ic', 'lu', '##mina', '##ries', 'spec', '##ulates', 'in', 'print', 'about', 'george', 'bush', \"'\", 's', '\"', 'vision', 'thing', ',', '\"', 'it', 'would', 'be', 'worth', '##while', 'to', 'refer', 'him', 'to', 'this', 'volume', '.', 'although', 'the', 'british', 'journalist', 'godfrey', 'hodgson', \"'\", 's', 'long', '##awa', '##ited', 'life', 'of', 'henry', 'st', '##im', '##son', 'is', 'infinitely', 'less', 'than', 'any', 'hopeful', 'peru', '##ser', 'has', 'a', 'right', 'to', 'expect', ',', 'it', 'does', 'answer', 'a', 'question', 'that', 'has', 'plagued', 'the', 'city', 'of', 'washington', 'for', 'the', 'past', 'few', 'years', '.', 'for', 'in', 'colonel', 'st', '##im', '##son', 'we', 'find', 'the', 'american', 'equivalent', 'of', 'the', 'aristocratic', 'principle', ':', 'of', 'those', 'to', 'and', 'so', 'on', '.', 'our', 'pop', '##uli', '##st', ',', 'democratic', 'ideals', 'prevent', 'us', 'from', 'thinking', 'comfortably', 'in', 'these', 'terms', ',', 'but', 'there', 'it', 'is', '.', 'no', 'doubt', 'the', 'colonel', \"'\", 's', 'famous', 'speech', 'to', 'george', 'bush', \"'\", 's', 'graduating', 'class', 'at', 'andover', '-', '-', 'the', 'one', 'in', 'which', 'he', 'said', 'that', 'he', 'did', 'n', \"'\", 't', 'pity', 'the', 'boys', 'but', 'en', '##vie', '##d', 'them', ',', 'since', 'providence', 'had', 'deposited', 'them', 'at', 'a', 'moment', 'in', 'time', '(', '1940', ')', 'that', 'gave', 'them', 'the', 'responsibility', 'of', 'choosing', 'between', 'good', 'and', 'evil', 'for', 'the', 'world', '-', '-', 'had', 'the', 'intended', 'effect', '.', 'those', 'who', 'expect', 'our', 'president', \"'\", 's', 'ideology', 'to', 'be', 'revealed', 'in', 'cu', '##omo', '##es', '##que', 'or', '##ations', 'or', 'keynote', 'addresses', 'or', 'soaring', 'declaration', '##s', 'need', 'look', 'no', 'further', 'than', 'the', 'scouting', 'manual', 'of', 'the', 'gentleman', 'squire', ':', 'duty', ',', 'fidelity', ',', 'nobles', '##se', 'ob', '##li', '##ge', ',', 'imp', '##ec', '##cable', 'manners', ',', 'sword', 'at', 'the', 'ready', '.', 'indeed', ',', 'colonel', 'st', '##im', '##son', 'seldom', 'en', '##un', '##cia', '##ted', 'what', 'might', 'pass', 'today', 'for', 'a', 'constitutional', 'creed', 'or', 'a', 'declaration', 'of', 'political', 'principle', '.', 'neither', 'the', 'great', 'society', 'nor', 'the', 'century', 'of', 'the', 'common', 'man', 'would', 'have', 'meant', 'much', 'to', 'this', 'skull', 'and', 'bones', '##ter', '.', 'his', 'was', 'a', 'sharp', ',', 'con', '##st', '##ricted', 'vision', 'of', 'an', 'imperfect', 'world', 'in', 'guidance', 'and', 'some', 'fundamental', 'pre', '##ce', '##pts', '.', 'just', 'as', 'the', 'great', 'mercantile', 'enterprises', 'of', 'the', 'gilded', 'age', 'had', '##lo', '##oked', 'to', 'young', 'st', '##im', '##son', 'for', 'protection', 'and', 'advice', ',', 'so', 'his', 'country', '##men', 'sought', 'old', 'st', '##im', '##son', \"'\", 's', 'guidance', 'in', 'their', 'violent', 'transition', 'to', 'super', '##power', 'status', '.', 'the', 'colonel', ',', 'who', 'ran', 'only', 'once', 'for', 'elect', '##ive', 'office', ',', 'did', 'not', 'necessarily', 'pursue', 'such', 'prestige', ';', 'it', 'was', 'his', 'obligation', 'to', 'assume', 'responsibility', '.', 'this', 'particular', 'tradition', 'in', 'our', 'national', 'political', 'life', 'did', 'not', 'begin', 'with', 'henry', 'st', '##im', '##son', ',', 'but', 'for', 'two', 'generations', 'of', 'american', 'foreign', 'policy', 'he', 'person', '##ified', 'it', '.', 'born', 'in', 'new', 'york', 'city', 'two', 'years', 'after', 'the', 'end', 'of', 'the', 'civil', 'war', ',', 'st', '##im', '##son', 'died', 'three', 'months', 'after', 'the', 'outbreak', 'of', 'fighting', 'in', 'korea', ':', 'from', 'general', 'sherman', 'to', 'dr', '.', 'op', '##pen', '##heimer', 'in', 'one', 'lifetime', '.', 'st', '##im', '##son', \"'\", 's', 'mother', 'died', 'when', 'he', 'was', 'eight', 'years', 'old', ',', 'and', 'his', 'father', ',', 'a', 'banker', 'who', 'switched', 'careers', 'to', 'medicine', 'to', 'dia', '##gno', '##se', 'and', 'treat', 'his', 'ai', '##ling', 'spouse', ',', 'left', 'his', 'orphaned', 'children', 'in', 'the', 'care', 'of', 'their', 'grandparents', 'while', 'he', 'threw', 'himself', 'ob', '##ses', '##sive', '##ly', 'into', 'his', 'medical', 'work', '.', 'henry', 'st', '##im', '##son', 'was', 'a', 'sensitive', ',', 'aus', '##ter', '##e', 'boy', ',', 'the', 'phillips', 'academy', '(', 'andover', ')', ',', 'yale', 'college', ',', 'and', 'harvard', 'law', 'school', '.', 'his', 'childless', 'marriage', 'to', 'a', 'woman', 'of', 'slightly', 'less', 'ex', '##al', '##ted', 'social', 'rank', 'distressed', 'his', 'father', ',', 'but', 'it', 'was', 'a', 'successful', ',', 'possibly', 'even', 'bliss', '##ful', ',', 'match', 'that', 'lasted', 'half', 'a', 'century', '.', 'he', 'settled', 'into', 'the', 'practice', 'of', 'corporate', 'law', 'in', '1890s', 'manhattan', ',', 'rode', 'to', 'hounds', 'in', 'the', 'country', ',', 'and', ',', 'as', 'he', 'slowly', 'descended', 'into', 'early', 'middle', 'age', ',', 'came', 'under', 'the', 'inevitable', 'influence', 'of', 'eli', '##hu', 'root', ',', 'senior', 'partner', 'in', 'his', 'firm', ',', 'and', 'theodore', 'roosevelt', ',', 'the', 'great', 'white', 'chief', 'of', 'his', 'governing', 'tribe', '.', 'to', 'the', 'career', '-', 'minded', 'among', 'us', ',', 'st', '##im', '##son', \"'\", 's', 'life', 'has', 'a', 'rare', 'narrative', 'grande', '##ur', '.', 'once', 'eli', '##hu', 'root', 'was', 'brought', 'into', 'the', 'mckinley', 'administration', 'in', '1899', 'to', 'organize', 'the', 'bureau', '##cratic', 'chaos', 'in', 'the', 'war', 'department', 'after', 'the', 'victory', 'over', 'spain', ',', 'st', '##im', '##son', \"'\", 's', 'star', 'was', 'destined', 'to', 'rise', '.', 'as', 'secretary', 'of', 'state', ',', 'root', 'commended', 'his', 'younger', 'partner', 'to', 'theodore', 'roosevelt', ',', 'who', 'appointed', 'st', '##im', '##son', 'u', '.', 's', '.', 'attorney', 'for', 'the', 'southern', 'district', 'of', 'new', 'york', '.', 'there', 'he', 'assembled', 'an', 'elite', 'brigade', 'of', 'younger', 'often', 'duplicate', '##d', 'since', '.', 'next', ',', 'he', 'presided', 'for', 'two', 'years', 'over', 'william', 'howard', 'taft', \"'\", 's', 'war', 'department', ',', 'and', ',', 'when', 'conflict', 'finally', 'came', 'for', 'the', 'united', 'states', 'in', '1917', ',', 'he', 'spent', 'several', 'happy', 'months', 'as', 'an', 'aging', 'artillery', '##man', 'in', 'france', '.', 'the', 'harding', 'years', 'found', 'him', 'practicing', 'law', ',', 'but', 'in', '1927', ',', 'when', 'calvin', 'cool', '##idge', 'sought', 'a', 'media', '##tor', 'for', 'nicaragua', \"'\", 's', 'insurgency', ',', 'he', 'recruited', 'st', '##im', '##son', '.', 'the', 'arrangements', 'negotiated', 'under', 'the', 'thorn', 'tree', 'at', 'tip', '##ita', '##pa', '-', '-', 'boycott', '##ed', ',', 'of', 'course', ',', 'by', 'augusto', 'sand', '##ino', '-', '-', 'were', 'characteristic', 'of', 'st', '##im', '##son', '.', 'establishing', 'his', 'authority', 'by', 'plain', 'strength', 'of', 'will', ',', 'he', 'was', 'both', 'cunning', 'and', 'wise', ',', 'rigorous', 'and', 'def', '##ere', '##ntial', ',', 'sc', '##rup', '##ulous', 'and', 'candi', '##d', ';', 'and', 'he', 'expected', 'his', 'ga', '##udy', 'players', 'to', 'observe', 'those', 'same', 'standards', '.', 'presumably', ',', 'his', 'actions', 'offended', 'latin', 'temper', '##s', ',', 'but', 'they', 'did', 'appeal', 'to', 'yankee', 'sentiment', ',', 'and', 'cool', '##idge', 'of', 'vermont', 'rewarded', 'st', '##im', '##son', 'of', 'new', 'york', 'with', 'a', 'pro', '##con', '##sul', '##ar', 'plum', ':', 'governor', '-', 'general', 'of', 'the', 'philippines', '.', 'thus', ',', 'on', 'the', 'basis', 'of', 'status', 'and', 'merit', '-', '-', 'and', 'on', 'that', 'basis', 'alone', '-', '-', 'he', 'was', 'herbert', 'hoover', \"'\", 's', 'choice', 'as', 'secretary', ':', 'hoover', 'port', '##ly', ',', 'ph', '##leg', '##matic', ',', 'cynical', ',', 'dei', '##lib', '##erate', ',', 'slow', 'to', 'anger', ',', 'with', 'a', 'memory', 'for', 'slight', '##s', ';', 'st', '##im', '##son', 'gaunt', ',', 'imp', '##et', '##uous', ',', 'instinct', '##ive', ',', 'ideal', '##istic', ',', 'quick', 'to', 'react', 'but', 'swift', 'to', 'forgive', '.', 'out', 'of', 'the', 'japanese', 'invasion', 'of', 'manchu', '##ria', '(', '1931', ')', 'came', 'the', 'st', '##im', '##son', 'doctrine', ',', 'the', 'notion', 'of', 'with', '##holding', 'diplomatic', 'recognition', 'on', 'grounds', 'of', 'principle', '.', 'st', '##im', '##son', '##ism', 'in', 'practice', 'combined', 'wilson', '##ian', 'ideal', '##ism', 'with', 'roosevelt', '##ian', 'action', ',', 'and', 'his', 'ind', '##ign', '##ation', 'over', 'japanese', 'per', '##fi', '##dy', 'was', 'characteristic', '##ally', 'deep', '.', 'the', 'united', 'states', 'should', 'not', 'merely', 'record', 'its', 'disapproval', ',', 'he', 'believed', ',', 'but', 'punish', 'the', 'trans', '##gre', '##sso', '##r', '.', 'but', 'hoover', ',', 'deep', 'in', 'the', 'depression', 'and', 'per', '##ce', '##iving', 'that', 'his', 'influence', 'in', 'japanese', '-', 'occupied', 'north', 'china', 'was', 'not', 'likely', 'to', 'be', 'great', ',', 'saw', 'little', 'point', 'in', 'bela', '##bor', '##ing', 'the', 'issue', '.', 'modern', 'readers', 'will', 'appreciate', 'the', 'sixty', '-', 'year', '-', 'old', 'debates', 'on', 'the', 'efficacy', 'of', 'em', '##bar', '##gos', 'and', 'the', 'varied', 'definitions', 'of', 'american', 'interest', '.', 'but', 'in', 'their', 'differences', 'over', 'manchu', '##ria', ',', 'st', '##im', '##son', 'and', 'hoover', 'were', 'a', 'vision', 'of', 'the', 'future', ':', 'intervention', '##ism', ',', 'isolation', '##ism', ';', 'bundles', 'for', 'britain', ',', 'america', 'first', '.', 'towards', 'the', 'end', 'of', 'his', 'life', ',', 'st', '##im', '##son', 'wrote', 'war', ')', 'with', 'the', 'help', 'of', 'young', 'mc', '##ge', '##org', '##e', 'bun', '##dy', ',', 'the', 'son', 'of', 'a', 'longtime', 'aide', '.', 'four', 'years', 'after', 'st', '##im', '##son', \"'\", 's', 'death', ',', 'richard', 'n', '.', 'current', 'issued', 'a', 'hostile', 'study', '(', 'secretary', 'st', '##im', '##son', ')', ',', 'and', 'in', '1960', ',', 'el', '##ting', 'e', '.', 'mori', '##son', ',', 'editor', 'of', 'theodore', 'roosevelt', \"'\", 's', 'letters', ',', 'published', 'an', 'authorized', 'life', '(', 'turmoil', 'and', 'tradition', ')', '.', 'godfrey', 'hodgson', 'is', 'a', 'journalist', ',', 'not', 'a', 'historian', ',', 'and', 'as', 'far', 'as', 'i', 'can', 'tell', ',', 'he', 'has', 'added', 'nearly', 'nothing', 'to', 'the', 'substance', 'of', 'these', 'earlier', 'volumes', '.', 'moreover', ',', 'he', 'is', 'a', 'brit', '##on', ',', 'and', 'what', 'he', 'might', 'regard', 'as', 'an', 'outsider', \"'\", 's', 'insights', 'are', 'often', 'a', 'stranger', \"'\", 's', 'mis', '##per', '##ception', '##s', '.', 'his', 'judgment', 'of', 'st', '##im', '##son', \"'\", 's', 'character', 'is', 'slightly', 'less', 'def', '##ere', '##ntial', 'than', 'mori', '##son', \"'\", 's', ';', 'meanwhile', ',', 'his', 'view', 'of', 'st', '##im', '##son', \"'\", 's', 'policies', 'and', 'opinions', 'is', 'very', 'nearly', 'identical', 'to', 'current', \"'\", 's', '.', 'he', 'breaks', 'new', 'ground', 'only', 'in', 'reminding', 'us', 'constantly', 'that', 'st', '##im', '##son', 'and', 'his', 'mentor', '##s', '(', 'root', ',', 'taft', ',', 'leonard', 'wood', ')', 'had', 'politically', 'incorrect', 'ideas', 'on', 'race', '.', 'no', 'one', 'can', 'dispute', 'that', 'eli', '##hu', 'root', 'held', 'views', 'about', 'asian', '##s', 'that', 'no', 'st', '##im', '##son', \"'\", 's', 'rational', '##e', 'for', 'the', 'internment', 'of', 'japanese', '-', 'americans', 'in', '1942', 'seems', 'largely', 'un', '##con', '##vin', '##cing', 'fifty', 'years', 'later', '.', 'but', 'so', 'what', '?', 'i', 'should', 'imagine', 'that', 'even', 'godfrey', 'hodgson', \"'\", 's', 'sen', '##si', '##bilities', 'have', 'evolved', 'in', 'the', 'course', 'of', 'decades', ',', 'and', 'most', 'of', 'the', 'offenders', 'in', 'this', 'volume', 'were', 'born', 'when', 'slavery', 'was', 'still', 'extant', '.', 'it', 'is', 'perhaps', 'not', 'coincide', '##ntal', 'that', 'hodgson', 'has', 'formed', 'many', 'of', 'his', 'conclusions', 'on', 'the', 'basis', 'of', 'conversations', 'with', 'such', 'contemporary', 'na', '##bo', '##bs', 'as', 'the', 'bun', '##dy', 'brothers', '(', 'mc', '##ge', '##org', '##e', 'and', 'william', ')', 'and', 'the', 'late', 'king', '##man', 'brewster', '.', 'the', 'echoes', 'of', 'those', 'comfortable', 'anglo', '-', 'american', 'chat', '##s', 'sound', 'heavily', 'in', 'these', 'pages', '.', 'the', 'assertion', 'that', 'st', '##im', '##son', 'was', 'some', 'kind', 'of', 'pioneer', 'new', 'frontiers', '##man', '-', 'a', 'herald', 'tribune', 'republican', 'whose', 'faith', 'has', 'been', 'betrayed', 'in', 'our', 'bush', '/', 'reagan', '##ite', 'times', '-', '-', 'is', 'wholly', 'un', '##pers', '##ua', '##sive', ',', 'thoroughly', 'predictable', ',', 'and', 'patent', '##ly', 'wrong', '.', 'the', 'distance', 'of', 'decades', 'gives', 'hodgson', 'the', 'freedom', 'to', 'psycho', '##anal', '##y', '##ze', 'his', 'dead', 'subject', '.', 'st', '##im', '##son', ',', 'to', 'be', 'sure', ',', 'is', 'eligible', 'for', 'some', 'sessions', 'on', 'the', 'couch', ':', 'the', 'mother', '##less', 'boy', 'with', 'the', 'cold', ',', 'distant', 'father', ';', 'the', 'apparent', 'ste', '##ril', '##ity', ',', 'possibly', 'caused', 'by', 'mum', '##ps', ';', 'the', 'temper', ';', 'st', '##ren', '##uous', 'physical', 'exercise', ';', 'the', 'rigid', 'adherence', 'to', 'his', 'gentleman', \"'\", 's', 'code', ';', 'the', 'imp', '##erson', '##al', 'diaries', ',', 'devoid', 'of', 'reflections', 'on', 'the', 'arts', 'or', 'the', 'flesh', '.', 'but', 're', '##tic', '##ence', 'is', 'not', 'always', 'a', 'means', 'of', 'conceal', '##ing', 'things', ',', 'and', 'orphans', 'sometimes', 'avoid', 'psychic', 'wounds', '.', 'it', 'is', 'entirely', 'possible', 'that', 'st', '##im', '##son', \"'\", 's', 'marriage', 'influenced', 'his', 'public', 'behavior', ',', 'or', 'vice', 'versa', ',', 'but', 'hodgson', \"'\", 's', 'speculation', '##s', 'are', 'obvious', 'and', 'sim', '##pl', '##istic', ',', 'and', 'his', 'only', 'living', 'witnesses', 'knew', 'st', '##im', '##son', 'in', 'old', 'age', ',', 'when', 'history', 'had', 'already', 'wrapped', 'him', 'like', 'a', 'sh', '##roud', '.', 'in', 'the', 'summer', 'of', '1940', ',', 'when', 'france', 'cap', '##it', '##ulated', 'and', 'dunkirk', 'fell', ',', 'justice', 'felix', 'frankfurt', '##er', 'swung', 'into', 'action', '.', 'franklin', 'roosevelt', 'had', 'been', 'anxious', 'to', 'convey', 'some', 'message', 'to', 'britain', 'and', 'hitler', 'with', 'symbolic', 'appointments', 'in', 'his', 'peace', '##time', 'cabinet', ';', 'he', 'was', 'also', 'concerned', 'about', 'the', 'republican', 'national', 'convention', 'opening', 'in', 'philadelphia', '.', 'st', '##im', '##son', ',', 'who', 'had', 'known', 'f', '##dr', 'socially', 'and', 'kept', 'in', 'friendly', 'contact', 'throughout', 'the', 'new', 'deal', ',', 'was', 'the', 'best', '-', 'known', 'republican', 'intervention', '##ist', 'in', 'america', '.', 'the', 'el', '##ope', '##ment', 'of', 'st', '##im', '##son', 'and', 'roosevelt', ',', 'engineered', 'by', 'frankfurt', '##er', ',', 'outraged', 'the', 'republicans', 'but', 'impressed', 'the', 'europeans', ':', 'best', 'means', 'roosevelt', 'had', 'to', 'take', 'sides', 'and', 'prepare', 'america', 'to', 'fight', '.', 'st', '##im', '##son', 'in', 'the', 'war', 'department', ',', 'and', 'frank', 'knox', 'in', 'the', 'navy', 'department', ',', 'were', 'more', 'than', 'mere', 'figure', '##heads', ';', 'but', ',', 'by', 'virtue', 'of', 'their', 'age', 'and', 'the', 'demands', 'of', 'global', 'warfare', ',', 'they', 'were', 'destined', 'to', 'rat', '##ify', ',', 'not', 'initiate', ',', 'u', '.', 's', '.', 'policies', '.', 'by', 'the', 'time', 'st', '##im', '##son', 'presided', 'over', 'the', 'decision', 'to', 'use', 'the', 'atomic', 'bomb', 'against', 'the', 'japanese', ',', 'he', 'was', '78', 'years', 'old', ',', 'weary', ',', 'distracted', ',', 'and', 'very', 'nearly', 'worn', 'out', '.', 'yet', 'it', 'must', 'be', 'said', 'that', 'the', 'process', 'by', 'which', 'he', 'de', '##hm', '##ed', 'the', 'nuclear', 'problem', '-', '-', 'the', 'disco', '##nce', '##rting', 'meeting', 'of', 'human', 'morality', 'and', 'national', 'obligation', '-', '-', 'was', 'exactly', 'the', 'kind', 'of', 'service', 'that', 'st', '##im', '##son', 'rendered', 'best', ',', 'and', 'holds', 'up', 'rather', 'well', '.', 'no', 'american', 'statesman', 'better', 'understood', 'military', 'necessity', ',', 'principle', '##d', 'action', ',', 'the', 'need', 'to', 'arrive', 'at', 'a', 'timely', 'consensus', '.', 'this', 'was', ',', 'as', 'he', 'saw', 'it', ',', 'the', 'dread', 'responsibility', 'for', 'which', 'he', 'had', 'been', 'trained', '.', 'such', 'burden', '##s', 'were', 'un', '##we', '##lco', '##me', ',', 'but', 'always', 'assumed', '-', 'and', 'the', 'model', 'of', 'service', 'that', 'was', 'henry', 'st', '##im', '##son', \"'\", 's', 'death', '.', 'philip', 'ter', '##zia', '##n', 'is', 'editor', 'of', 'the', 'editorial', 'pages', 'at', 'the', 'providence', 'journal', '.', 'by', 'edward', 'nord', '##en', 'edward', 'nord', '##en', 'is', 'a', 'writer', 'living', 'in', 'jerusalem', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fb08535b-6408-490b-ec13-5e52a6e45c08",
        "id": "v14TzEapShve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)\n",
        "evaluate_lstm"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48/48 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29820192356904346, 0.9166666666666666]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S4l1FesjShvg"
      },
      "source": [
        "###### BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxqMpFSqShvg",
        "colab": {}
      },
      "source": [
        "train_labels = train_labels.astype(np.int64)\n",
        "validation_labels = validation_labels.astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QufEyuCGShvh",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vEsIgmVvShvj",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7c495bf6-30d7-4efd-c275-6e6e08e50a02",
        "id": "2otncE2dShvk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ntI2I4vRShvl",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vb0Vm8ftShvm",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eh6KmsWlShvn",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IoVRCa86Shvp",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KkiUzeRDShvq",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b1e27abc-2154-44eb-a06a-26b6e3cbfa56",
        "id": "OcaMIYQQShvr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c6d120c8-5027-4846-ffa5-653d7a8ef917",
        "id": "7kW1zxflShvs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32729432824999094,\n",
              " 0.23155721370130777,\n",
              " 0.21828558202832937,\n",
              " 0.20521679567173123]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1297cdc9-bb57-4e0a-d458-74ac6c30ae53",
        "id": "rBVyZIC2Shvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(period1_test)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4678b23a-6ca7-4916-ce3d-3ba5d5af1d26",
        "id": "X7IIb1mwShvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(period1_test.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = period1_test.sentence.values\n",
        "labels = period1_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length=512\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels.astype(np.int64))\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 29\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8394105a-d35c-435a-b3d0-69300ef60cf7",
        "id": "ApLDyVnaShvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 29 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9732ab1e-eeb3-47b8-d784-54af469c6a00",
        "id": "Kb_h_U6nShvy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (period1_test.label.sum(), len(period1_test.label), (period1_test.label.sum() / len(period1_test.label) * 100.0)))"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 3 of 29 (10.34%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "06d514a6-4063-4ee8-fff5-de554117ac00",
        "id": "OGv6W9m4Shv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "26d1cf0b-9d40-40c3-bb86-37c8c43ad608",
        "id": "I0qqY_81Shv2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2ea188a3-b4c6-4c21-fb44-95d8b4943a12",
        "id": "C0R9wwmUShv3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "39e3d0c4-5b9a-45be-f17e-4e82540060ba",
        "id": "kuJVz34JShv4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './period1/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./period1/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./period1/vocab.txt',\n",
              " './period1/special_tokens_map.json',\n",
              " './period1/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "60dWG5K5ShB2"
      },
      "source": [
        "#### Period 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6ebfda67-3d25-45e5-ddbf-00998239762d",
        "id": "jcggpKM3ShB3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "period2 = data[data.period==2]\n",
        "period2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>these kick-ass activists are kicking ass // w...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>loung ung 's journey photograph preceding pag...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>women find new shelter from the sex industry ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>tulips . dot-coms . hey , manias happen . but...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>as dvds take over more and more of the home v...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3108</th>\n",
              "      <td>we must turn to the past for a film as innoc...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3109</th>\n",
              "      <td>iraqi expatriates are clamoring to cast ball...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>it begins with news reports of freakish ligh...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3111</th>\n",
              "      <td>havre , mont . - surrounded by fields of kne...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3112</th>\n",
              "      <td>some see life as a ladder , a road , a river...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>596 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "405    these kick-ass activists are kicking ass // w...     0       2\n",
              "406    loung ung 's journey photograph preceding pag...     0       2\n",
              "407    women find new shelter from the sex industry ...     0       2\n",
              "412    tulips . dot-coms . hey , manias happen . but...     1       2\n",
              "413    as dvds take over more and more of the home v...     0       2\n",
              "...                                                 ...   ...     ...\n",
              "3108    we must turn to the past for a film as innoc...     0       2\n",
              "3109    iraqi expatriates are clamoring to cast ball...     0       2\n",
              "3110    it begins with news reports of freakish ligh...     0       2\n",
              "3111    havre , mont . - surrounded by fields of kne...     0       2\n",
              "3112    some see life as a ladder , a road , a river...     0       2\n",
              "\n",
              "[596 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-gp8T4IVOHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ca6d809-de97-463f-d2ab-d5dd0592feae"
      },
      "source": [
        "period2.label.sum()/len(period2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08053691275167785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeBeE-8xShB5",
        "colab": {}
      },
      "source": [
        "period2_train, period2_test = train_test_split(period2, random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "09f18dd0-2d72-4db4-d4e2-4e7d4e3eab44",
        "id": "mMTyF0ctShB7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period2_train), len(period2_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "566 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "96654cbf-1461-4fe7-f088-1329281cb43d",
        "id": "EPstvO5DShB9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period2_train.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period2_train.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'section', ':', '10', 'miles', 'square', 'the', 'last', 'low', '-', 'rent', 'office', 'building', 'in', 'downtown', 'd', '.', 'c', '.', 'smoke', '##s', 'out', 'the', 'national', 'organization', 'for', 'women', ',', 'us', '-', 'ukraine', 'foundation', ',', 'a', 'private', 'eye', ',', 'and', 'the', 'washington', 'monthly', '.', 'now', 'it', 'can', 'be', 'told', '.', 'it', 'is', 'not', '-', '-', 'and', 'never', 'has', 'been', '-', '-', 'tricky', 'to', 'sneak', 'into', 'the', 'washington', 'monthly', \"'\", 's', 'downtown', 'office', 'building', 'on', 'the', 'come', '##r', 'of', 'h', 'and', '15th', 'streets', ',', 'one', 'block', 'from', 'the', 'sniper', '##s', 'atop', 'the', 'white', 'house', '.', 'on', 'a', 'grey', 'columbus', 'day', ',', 'as', 'on', 'all', 'federal', 'holidays', ',', 'the', 'monthly', 'was', 'open', 'for', 'business', ',', 'but', 'the', 'building', \"'\", 's', 'front', 'door', 'was', 'bolted', '.', 'when', 'i', 'realized', 'i', \"'\", 'd', 'arrived', 'without', 'my', 'security', 'card', 'to', 'unlock', 'the', 'door', ',', 'i', 'simply', 'tr', '##otted', 'past', 'the', '$', '10', 'sale', 'racks', 'to', 'the', 'left', 'of', 'the', 'main', 'entrance', 'and', 'pushed', 'open', 'the', 'glass', 'door', 'of', 'the', 'ar', '##rington', 'club', 'boutique', ',', 'where', 'britney', 'spears', 'was', 'pu', '##rring', 'over', 'the', 'loud', '##sp', '##eak', '##ers', 'while', 'a', 'single', 'shop', '##per', 'paw', '##ed', 'through', 'racks', 'of', 'string', 'bikini', '##s', 'and', 'se', '##quin', '##ed', 'bust', '##iers', ',', 'occasionally', 'glancing', 'at', 'advertisements', 'posted', 'for', 'various', '\"', 'modeling', '\"', 'agencies', '.', 'i', 'smiled', 'at', 'the', 'halloween', 'witch', 'costume', ',', 'his', 'purple', 'shirt', 'un', '##bu', '##tton', '##ed', 'just', 'enough', 'to', 'reveal', 'a', 'few', 'stray', 'chest', 'hairs', '.', 'and', 'then', 'i', 'turned', 'to', 'my', 'right', ',', 'and', 'pushed', 'open', 'a', 'side', 'door', 'adorned', 'with', 'a', 'poster', 'of', 'bikini', '-', 'clad', 'carmen', 'elect', '##ra', '.', 'i', 'was', 'in', 'the', 'main', 'lobby', '.', 'lest', 'you', 'get', 'the', 'wrong', 'idea', 'about', 'where', 'i', 'work', ',', 'the', 'store', '##front', 'on', 'the', 'opposite', 'side', 'of', 'the', 'main', 'entrance', 'is', 'occupied', 'by', 'the', 'ag', '##ape', 'christian', 'bookstore', ',', 'where', 'floor', '-', 'to', '-', 'ceiling', 'shelves', 'over', '##flow', 'with', 'study', 'bible', '##s', ',', 'greeting', 'cards', ',', 'and', 've', '##gg', '##ie', 'tales', 'dvds', '.', 'unfortunately', ',', 'its', 'door', 'into', 'the', 'lobby', 'is', 'usually', 'locked', '.', 'what', 'the', 'club', 'boutique', 'and', 'the', 'christian', 'bookstore', ',', 'together', 'with', 'the', 'building', \"'\", 's', 'varied', 'office', 'tenants', '-', '-', 'which', 'have', 'recently', 'included', 'the', 'national', 'organization', 'for', 'women', ',', 'the', 'washington', 'area', 'bi', '##cy', '##cl', '##ists', 'association', ',', 'the', 'center', 'on', 'eco', '##tour', '##ism', ',', 'code', 'pink', ',', 'a', 'private', 'eye', ',', 'and', 'the', 'u', '.', 's', '.', '-', 'ukraine', 'foundation', '-', '-', 'have', 'in', 'common', 'is', 'not', 'ideology', 'but', 'the', 'desire', 'to', 'operate', 'in', 'downtown', 'washington', 'and', 'not', 'sp', '##lu', '##rge', 'on', 'rent', '.', 'most', 'tenants', 'pay', 'a', 'little', 'more', 'than', '$', '20', 'per', 'square', 'foot', ',', 'about', 'half', 'the', 'low', 'rent', 'come', 'trade', '##offs', '.', 'the', 'woodward', 'building', 'is', 'old', 'and', 'draft', '##y', ',', 'with', 'elevators', 'that', 'huff', 'and', 'groan', ',', 'often', 'stopping', 'for', 'a', 'short', 'rest', 'between', 'stories', 'or', 'opening', 'mysteriously', 'on', 'floors', 'where', 'no', 'passengers', 'embark', '-', '-', 'according', 'to', 'an', 'engineer', 'from', 'otis', ',', 'no', 'company', 'makes', 'replacement', 'parts', 'for', 'these', 'gilded', 'cages', 'anymore', ',', 'forcing', 'the', 'repair', 'staff', 'to', 'imp', '##rov', '##ise', '.', 'many', 'of', 'us', 'take', 'the', 'stairs', 'just', 'to', 'be', 'safe', '.', 'the', 'bathroom', 'fa', '##uce', '##ts', 'do', 'little', 'more', 'than', 'gu', '##rg', '##le', ',', 'and', 'air', 'conditioning', 'is', 'supplied', 'by', 'crack', '##ling', 'window', 'units', '.', 'for', 'businesses', 'that', 'ca', 'n', \"'\", 't', 'quite', 'afford', 'a', 'typical', 'd', '.', 'c', '.', 'address', 'for', 'their', 'letter', '##head', ',', 'the', 'woodward', 'beats', 'rent', '##ing', 'a', 'post', 'box', 'or', 'shuffling', 'across', 'the', 'river', 'to', 'ross', '##lyn', '.', 'and', 'if', 'you', 'sq', '##uin', '##t', 'past', 'the', 'dust', 'and', 'peeling', 'paint', ',', 'the', 'buildings', 'spacious', 'hallways', ',', 'wrought', 'iron', 'staircase', ',', 'ceiling', 'frescoes', ',', 'and', 'gold', '-', 'trimmed', 'lobby', 'letter', '##box', 'con', '##jure', 'a', 'certain', 'faded', 'elegance', ',', 'reminiscent', 'of', 'the', 'woodward', \"'\", 's', 'original', '1911', 'glory', '.', 'all', 'that', 'is', 'coming', 'to', 'an', 'end', 'this', 'year', ',', 'when', '73', '##3', '15th', 'street', 'will', 'empty', 'so', 'that', 'it', 'can', 'be', 'renovated', 'and', 'to', 'for', '##age', 'elsewhere', 'in', 'washington', \"'\", 's', 'difficult', 'real', 'estate', 'market', '.', 'while', 'most', 'american', 'cities', 'harbor', 'a', 'handful', 'of', 'neighborhood', 'committees', 'and', 'volunteer', 'organizations', ',', 'washington', 'is', 'unique', 'in', 'terms', 'of', 'scale', '.', 'the', 'swirl', 'of', 'political', 'activity', 'in', 'the', 'capital', 'makes', 'the', 'city', 'a', 'mecca', 'for', 'myriad', 'national', 'and', 'local', 'nonprofit', '##s', ',', 'whose', 'leaders', 'often', 'dee', '##m', 'it', 'mission', 'critical', 'to', 'locate', 'their', 'headquarters', '-', '-', 'or', 'at', 'least', 'a', 'branch', '-', '-', 'somewhere', 'within', 'ears', '##hot', 'of', 'capitol', 'hill', 'or', 'the', 'white', 'house', '.', 'in', 'recent', 'decades', ',', 'the', 'woodward', 'building', 'has', 'filled', 'a', 'particular', 'niche', 'in', 'd', '.', 'c', '.', \"'\", 's', 'political', 'ecosystem', ':', 'shelter', '##ing', 'a', 'he', '##ft', '##y', 'slice', 'of', 'the', 'capital', \"'\", 's', 'nonprofit', 'under', '##class', '.', 'its', 'original', 'owners', 'had', 'rather', 'different', 'ambitions', '.', 'the', 'woodward', 'building', ',', 'an', '11', '-', 'story', 'steel', '-', 'flame', 'ed', '##ifice', 'with', 'six', 'marble', 'columns', 'framing', 'one', 'entrance', ',', 'was', 'built', 'for', 'the', 'owner', 'of', 'the', 'woodward', '&', ';', 'lot', '##hr', '##op', 'department', 'stores', 'in', 'the', 'early', 'years', 'of', 'the', '20th', 'century', ',', 'when', 'developers', 'competed', 'to', 'attract', 'the', 'city', \"'\", 's', 'emerging', 'class', 'of', 'lawyers', 'and', 'bankers', 'with', 'marble', '##d', 'lo', '##bbies', 'and', 'beaux', 'arts', 'facades', '.', 'it', 'stands', 'across', 'the', 'street', 'from', 'the', 'us', '.', 'treasury', ',', 'fifteenth', 'street', 'financial', 'district', '.', 'the', 'exterior', 'resembles', 'a', 'triple', '-', 'layer', 'cake', '-', '-', 'three', 'stories', 'of', 'stone', 'form', 'the', 'base', ',', 'six', 'stories', 'of', 'brick', 'in', 'the', 'middle', ',', 'and', 'two', 'stories', 'of', 'ornamental', 'terra', 'cot', '##ta', 'at', 'the', 'top', '.', 'with', 'its', 'gilded', 'store', '##front', '##s', ',', 'well', '-', 'clad', 'elevator', 'operators', ',', 'and', 'spacious', 'offices', 'each', 'with', 'its', 'own', 'large', 'window', 'and', 'trans', '##om', 'above', 'the', 'door', '(', 'nec', '##ess', '##ities', 'in', 'the', 'days', 'before', 'fir', '-', 'conditioning', ')', ',', 'the', 'woodward', 'was', 'then', 'the', 'very', 'definition', 'of', 'po', '##sh', '.', 'a', 'year', 'before', 'the', 'building', 'opened', ',', 'lavish', 'advertisements', 'printed', 'in', 'maroon', 'and', 'gold', 'for', 'prospective', 'tenants', 'proclaimed', 'it', '\"', 'the', 'largest', 'and', 'most', 'imposing', 'office', 'building', 'in', 'washington', '.', '\"', 'the', 'building', 'retained', 'a', 'certain', 'lust', '##er', 'into', 'the', 'late', '1960s', ',', 'housing', 'an', 'upscale', 'men', \"'\", 's', 'shoe', 'store', 'frequented', 'by', 'vice', 'president', 'hubert', 'humphrey', 'and', 'a', 'barber', '##sho', '##p', 'where', 'dick', 'cheney', ',', 'then', 'a', 'young', 'aide', 'to', 'donald', 'rum', '##sf', '##eld', 'at', 'the', 'us', '.', 'office', 'of', 'economic', 'opportunity', ',', 'was', 'a', 'regular', '.', 'neil', 'armstrong', 'once', 'dropped', 'by', 'for', 'a', 'trim', 'on', 'his', 'way', 'to', 'lunch', 'at', 'the', 'white', 'house', '.', 'but', 'in', 'the', '1970s', 'and', '1980s', ',', 'as', 'woodward', 'grew', 'older', 'and', 'un', '##ti', '##dy', ',', 'her', 'belle', 'ep', '##o', '##que', 'beauty', 'worse', 'for', 'the', 'wear', ',', 'her', 'lobby', 'chan', '##del', '##iers', 'collecting', 'dust', '.', 'the', 'well', '-', 'heel', '##ed', 'clients', 'gradually', 'trickle', '##d', 'out', '.', 'the', 'building', 'became', 'known', 'as', 'a', 'last', 'resort', 'for', 'businesses', ',', 'where', 'the', 'rent', 'was', 'cheap', ',', 'maintenance', 'was', 'minimal', ',', 'and', 'the', 'leases', 'contained', 'one', '-', 'year', 'termination', 'clauses', '.', 'after', 'his', 'failed', 'presidential', 'bid', ',', 'john', 'anderson', 'found', 'himself', '$', '600', ',', '000', 'in', 'debt', 'and', 'downs', '##ized', 'from', 'his', 'price', '##r', 'georgetown', 'offices', 'to', 'the', 'woodward', 'building', '.', 'in', 'the', '1980s', ',', 'jesse', 'jackson', 'ran', 'his', 'rainbow', '-', 'push', 'campaign', 'downtown', 'on', 'a', 'budget', 'from', 'the', 'building', '.', 'eventually', ',', 'the', 'jewelry', 'boutique', 'and', 'shoe', 'store', 'moved', 'out', ';', 'the', 'bikini', 'shop', 'moved', 'in', '.', 'the', 'law', 'firms', 'moved', 'out', ';', 'the', 'center', 'on', 'eco', '##tour', '##ism', 'moved', 'in', '.', 'just', 'as', 'other', 'downtown', 'buildings', 'were', 'adding', 'underground', 'parking', 'garage', '##s', ',', 'the', 'woodward', 'building', 'converted', 'a', 'section', 'of', 'its', 'ground', 'level', 'to', 'a', 'bike', 'room', '.', 'two', 'decades', 'ago', ',', 'there', 'remained', 'a', 'handful', 'of', 'un', '##ren', '##ova', '##ted', 'historic', 'buildings', 'in', 'downtown', 'washington', '.', 'with', 'their', 'fan', '##ciful', 'faa', '##des', ',', 'column', '##ed', 'entrances', ',', 'crack', '##ling', 'ra', '##dia', '##tors', ',', 'and', 'below', '-', 'market', 'rents', ',', 'these', 'ed', '##ifice', '##s', 'became', 'if', 'they', 'do', 'n', \"'\", 't', 'always', 'do', 'well', '.', 'one', 'by', 'one', ',', 'each', 'has', 'undergone', 'a', 'make', '##over', ':', 'the', 'southern', 'building', 'in', '1987', ';', 'the', 'evening', 'star', 'building', 'in', '1990', ';', 'the', 'investment', 'building', 'in', '1997', ';', 'the', 'bowen', 'building', 'in', '2005', '.', 'many', 'of', 'the', 'woodward', 'building', \"'\", 's', 'final', 'tenants', 'are', 'themselves', 'refugees', 'from', 'prior', 'renovations', ',', 'including', 'the', 'bi', '##cy', '##lists', \"'\", 'association', 'and', 'the', 'be', '##sp', '##eck', '##led', 'private', 'eye', 'on', 'the', 'seventh', 'floor', 'with', 'a', 'pen', '##chan', '##t', 'for', 'bell', '##owing', 'from', 'his', 'open', 'door', '.', 'the', 'monthly', 'itself', 'arrived', 'in', 'similar', 'fashion', 'in', '2001', ',', 'soon', 'after', 'our', 'then', '-', 'office', 'building', 'on', 'connecticut', 'avenue', '-', '-', 'whose', 'tenants', 'included', 'tv', '-', 'free', 'america', ',', 'the', 'student', 'environmental', 'news', '##wire', ',', 'and', 'the', 'women', \"'\", 's', 'collective', '-', '-', 'was', 'gut', '##ted', 'to', 'make', 'way', 'for', 'a', 'ladies', \"'\", 'clothing', 'store', ',', 'ann', 'taylor', 'loft', '.', 'the', 'jack', '##hammer', '##s', 'had', 'begun', 'churning', 'up', 'the', 'basement', 'while', 'we', 'were', 'still', 'typing', 'away', 'upstairs', ',', 'our', 'computers', 'sh', '##roud', '##ed', 'in', 'plastic', 'sheet', '##ing', 'to', 'protect', 'them', 'from', 'the', 'clouds', 'of', 'dust', '.', 'after', 'inspecting', 'row', 'houses', 'in', 'columbia', 'heights', 'and', 'office', 'suites', 'in', 'ross', '##lyn', ',', 'neither', 'of', 'which', 'seemed', 'washington', 'enough', 'for', 'the', 'washington', 'monthly', ',', 'our', 'it', \"'\", 's', 'hard', 'to', 'get', 'more', 'washington', 'than', 'this', '.', 'at', 'least', 'once', 'a', 'day', ',', 'our', 'offices', 'res', '##ound', 'with', 'sirens', 'from', 'official', 'motor', '##cade', '##s', 'para', '##ding', 'below', 'along', '15th', 'street', '.', 'various', 'protesters', 'file', 'by', 'along', 'the', 'same', 'road', ',', 'en', 'route', 'to', 'lafayette', 'park', 'or', 'the', 'national', 'mall', ',', 'interrupting', 'our', 'editorial', 'pow', '-', 'wow', '##s', 'with', 'chants', 'and', 'drums', '.', 'in', 'the', 'months', 'before', 'the', 'invasion', 'of', 'iraq', ',', 'the', 'lobby', 'became', 'an', 'ad', 'hoc', 'staging', 'area', 'for', 'various', 'demonstrations', ',', 'while', 'code', 'pink', 'organized', 'marches', 'out', 'of', 'its', 'fifth', '-', 'floor', 'office', ',', 'essentially', 'a', 'large', 'broom', 'closet', ',', 'with', \"'\", '\"', 'stop', 'the', 'war', '\"', 'and', '\"', 'no', 'blood', 'for', 'oil', '\"', 'banners', 'un', '##fur', '##led', 'for', 'painting', 'in', 'the', 'adjacent', 'hallways', '.', 'earlier', 'this', 'year', ',', 'when', 'a', 'man', 'parked', 'his', 'van', 'at', 'the', 'end', 'of', 'our', 'block', 'and', 'claimed', 'to', 'have', 'a', 'bomb', 'inside', ',', 'we', 'peered', 'out', 'of', 'our', 'windows', 'to', 'find', 'that', 'the', 'streets', 'had', 'been', 'cord', '##oned', 'off', ',', 'bomb', 'squads', 'sent', 'in', ',', 'and', 'seemingly', 'every', 'other', 'building', 'on', 'the', 'block', 'evacuated', '.', 'soon', ',', 'though', ',', 'the', 'woodward', 'will', 'indeed', 'be', 'empty', '.', 'the', 'hallways', 'are', 'already', 'for', 'sale', 'are', 'posted', 'in', 'the', 'lobby', '.', 'the', 'monthly', 'is', 'packing', 'up', 'for', 'the', 'international', 'building', ',', 'a', 'more', 'recently', 'renovated', 'and', 'more', 'expensive', '(', 'though', 'not', 'too', 'expensive', ')', 'building', 'a', 'few', 'blocks', 'away', '.', 'meanwhile', ',', 'there', \"'\", 's', 'talk', 'of', 'dona', '##ting', 'the', 'old', 'elevators', 'to', 'the', 'smithsonian', '.', 'photo', '(', 'black', '&', ';', 'white', ')', 'by', 'christina', 'larson', 'christina', 'larson', 'is', 'the', 'managing', 'editor', 'of', 'the', 'washington', 'monthly', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wn5fmlF6ShB_",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bae196b9-3a0b-4758-9249-dd4bda0d4460",
        "id": "sf5wXUQWShCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period2_train), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "566 566 566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AmwBDXpYShCB",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.05)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UUDjPP6eShCE"
      },
      "source": [
        "###### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kzHNIek6ShCF",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a94ce76e-f4c3-4e1d-e214-895b80ea1823",
        "id": "FWmals6oShCH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Using Sigmoid Activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='sigmoid'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05e8853a-bf38-4b26-e2bd-7ce93cddbf62",
        "id": "I6YoeSHfShCI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 537 samples, validate on 29 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "537/537 [==============================] - 4s 8ms/step - loss: 0.5440 - acc: 0.9050 - val_loss: 0.2680 - val_acc: 0.9310\n",
            "Epoch 2/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.2973 - acc: 0.9181 - val_loss: 0.2579 - val_acc: 0.9310\n",
            "Epoch 3/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.2845 - acc: 0.9181 - val_loss: 0.2514 - val_acc: 0.9310\n",
            "Epoch 4/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.2824 - acc: 0.9181 - val_loss: 0.2519 - val_acc: 0.9310\n",
            "Epoch 5/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.2795 - acc: 0.9181 - val_loss: 0.2523 - val_acc: 0.9310\n",
            "Epoch 6/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.2737 - acc: 0.9181 - val_loss: 0.2517 - val_acc: 0.9310\n",
            "Epoch 7/10\n",
            "537/537 [==============================] - 4s 7ms/step - loss: 0.2643 - acc: 0.9181 - val_loss: 0.2532 - val_acc: 0.9310\n",
            "Epoch 8/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.2405 - acc: 0.9181 - val_loss: 0.2596 - val_acc: 0.9310\n",
            "Epoch 9/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.1887 - acc: 0.9181 - val_loss: 0.2742 - val_acc: 0.9310\n",
            "Epoch 10/10\n",
            "537/537 [==============================] - 3s 6ms/step - loss: 0.1033 - acc: 0.9553 - val_loss: 0.3214 - val_acc: 0.9310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea32af2d-1cb9-4f28-dbcd-88fa5eab27cf",
        "id": "4rYHj0wnShCK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period2_test.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period2_test.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'nobody', \"'\", 's', 'paying', 'much', 'attention', 'to', 'the', 'activity', 'in', 'the', 'tucked', '-', 'away', 'corner', 'of', 'car', '##ib', '##ou', 'coffee', 'on', 'peach', '##tree', 'road', '.', 'na', '##ges', '##h', 'ku', '##ku', '##no', '##or', ',', 'a', 'film', 'director', 'who', 'lives', 'in', 'brook', '##haven', ',', 'is', 'nearly', 'pinned', 'to', 'the', 'shop', \"'\", 's', 'camel', '-', 'colored', 'wall', '.', 'he', \"'\", 's', 'getting', 'his', 'picture', 'taken', 'by', 'a', 'journal', '-', 'constitution', 'photographer', '.', 'the', 'car', '##ib', '##ou', 'staff', 'keeps', 'on', 'making', 'mo', '##cha', '##s', 'and', 'la', '##ttes', '.', 'the', 'few', 'afternoon', 'patrons', 'hardly', 'look', 'up', 'from', 'their', 'java', 'cups', '.', 'obviously', ',', 'to', 'them', ',', 'it', \"'\", 's', 'not', 'as', 'if', 'this', 'guy', 'is', 'ba', '##z', 'lu', '##hr', '##mann', '(', '\"', 'mo', '##ulin', 'rouge', '\"', ')', ',', 'christopher', 'nolan', '(', '\"', 'me', '##mento', '\"', ')', ',', 'spike', 'lee', 'or', 'even', 'one', 'of', 'those', 'coe', '##n', 'brothers', '.', 'it', 'would', 'be', 'a', 'different', 'story', 'on', 'the', 'other', 'side', 'of', 'the', 'planet', '.', 'in', 'india', ',', 'where', 'ku', '##ku', '##no', '##or', 'comes', 'from', ',', 'movie', 'fans', 'would', 'surround', 'him', '.', 'they', 'would', 'aggressively', 'try', 'to', 'touch', 'him', '.', 'they', 'would', 'beg', 'him', 'to', 'visit', 'their', 'homes', '.', 'at', '35', ',', 'ku', '##ku', '##no', '##or', '(', 'his', 'full', 'name', 'is', 'pronounced', 'ne', '##y', '-', 'gay', '##sh', 'cuckoo', '-', 'nor', ')', 'recently', 'wrapped', 'shooting', 'on', 'his', 'fourth', 'film', '-', '-', '\"', 'three', 'walls', ',', '.', 'with', 'successively', 'better', '-', 'known', 'low', '-', 'budget', 'flick', '##s', '\"', 'rock', '##ford', ',', '\"', '\"', 'hyderabad', 'blues', '\"', 'and', '\"', 'bollywood', 'calling', '\"', 'under', 'his', 'belt', ',', 'ku', '##ku', '##no', '##or', 'is', 'a', 'celebrity', 'in', 'india', '.', 'he', \"'\", 's', 'also', 'at', 'the', 'forefront', 'of', 'a', 'new', 'genre', 'of', 'indian', 'movies', 'dubbed', '\"', 'hi', '##ng', '##lish', '.', '\"', 'they', 'have', 'dialogue', 'that', 'switches', 'back', 'and', 'forth', 'between', 'english', 'and', 'hindi', 'and', 'plots', 'hon', '##ed', 'more', 'to', 'a', 'western', ',', 'linear', 'style', 'of', 'storytelling', 'as', 'opposed', 'to', 'the', 'break', '-', 'into', '-', 'song', 'bollywood', 'extra', '##va', '##gan', '##za', '##s', 'that', 'dominate', 'indian', 'filmmaking', '.', 'ku', '##ku', '##no', '##or', 'says', 'he', 'makes', 'hi', '##ng', '##lish', 'movies', 'because', ',', 'living', 'now', 'in', 'the', 'united', 'states', ',', 'it', \"'\", 's', 'what', 'he', 'knows', '.', 'the', 'films', 'also', 'have', 'found', 'an', 'app', '##re', '##cia', '##tive', 'audience', '.', '\"', 'they', 'do', 'very', 'well', ',', 'but', 'it', 'is', 'a', 'small', 'market', ',', '\"', 'he', 'says', '.', '\"', 'it', \"'\", 's', 'restricted', 'to', 'urban', 'audiences', 'in', 'india', '.', '\"', 'that', \"'\", 's', 'changing', ',', 'too', '.', 'worldwide', 'acceptance', 'of', 'all', 'forms', 'of', 'indian', 'movies', '-', '-', 'including', 'the', 'new', 'hi', '##ng', '##lish', 'films', ',', 'traditional', 'indian', 'art', 'films', 'and', 'the', 'mega', '##wat', '##t', 'musical', 'epic', '##s', 'known', 'as', 'bollywood', 'films', '-', '-', 'is', 'ed', '##ging', 'upward', '.', 'since', 'its', 'late', '-', 'february', 'release', ',', 'director', 'earned', 'nearly', '$', '4', '.', '5', 'million', 'in', 'u', '.', 's', '.', 'theaters', 'in', 'just', 'five', 'weeks', '.', '(', 'that', 'tops', 'the', 'full', 'run', 'of', 'darren', 'ar', '##ono', '##fs', '##ky', \"'\", 's', '2000', 'art', '-', 'house', 'knockout', '\"', 'requiem', 'for', 'a', 'dream', '\"', 'and', 'almost', 'matches', 'the', '2000', 'take', 'of', 'alejandro', 'gonzalez', 'ina', '##rri', '##tu', \"'\", 's', 'critical', 'wonder', '\"', 'amore', '##s', 'per', '##os', '.', '\"', ')', 'with', '\"', 'monsoon', 'wedding', ',', '\"', 'nair', '-', '-', 'who', 'earlier', 'made', '\"', 'mississippi', 'mas', '##ala', ',', '\"', 'starring', 'two', '-', 'time', 'oscar', 'winner', 'den', '##zel', 'washington', '-', '-', 'became', 'the', 'first', 'female', 'director', 'to', 'win', 'at', 'the', 'venice', 'film', 'festival', '.', 'her', 'movie', 'is', 'drawing', 'strong', 'crowds', 'locally', 'at', 'george', 'le', '##fo', '##nt', \"'\", 's', 'garden', 'hills', 'theater', ',', 'where', 'it', 'will', 'probably', 'play', 'through', 'this', 'month', '.', 'it', 'also', 'has', 'been', 'playing', 'at', 'regal', \"'\", 's', 'hollywood', '24', 'and', 'the', 'marie', '##tta', 'star', 'cinemas', '.', '\"', 'monsoon', 'wedding', '\"', 'is', 'a', 'compelling', ',', 'modern', '-', 'day', 'family', 'drama', 'set', 'in', 'new', 'delhi', 'in', 'the', 'days', 'leading', 'up', 'to', 'an', 'arranged', 'marriage', '.', 'shot', 'on', 'a', 'me', '##ager', 'budget', 'of', 'about', '$', '1', 'million', ',', 'it', 'could', 'make', 'more', 'than', '$', '7', '.', '5', 'million', 'in', 'the', 'united', 'states', ',', 'which', 'would', 'easily', 'make', 'it', 'the', 'top', 'import', 'ever', 'from', 'south', 'asia', '.', '\"', 'mira', 'is', 'of', 'theatrical', 'distribution', 'for', 'usa', 'films', ',', 'which', 'is', 'handling', 'the', 'movie', 'in', 'the', 'united', 'states', '.', '\"', 'her', 'film', 'demonstrates', 'the', 'cosmopolitan', 'nature', 'of', 'indian', 'culture', 'and', 'does', 'it', 'with', 'a', 'story', 'involving', 'love', ',', 'family', ',', 'commitment', 'and', 'truth', '.', '\"', 'at', 'the', 'same', 'time', '\"', 'monsoon', 'wedding', '\"', 'is', 'scoring', ',', 'atlanta', \"'\", 's', 'high', 'museum', 'of', 'art', 'is', 'holding', 'its', 'first', 'film', 'festival', 'of', 'india', ',', 'focusing', 'on', 'traditional', 'art', 'films', '.', 'with', 'screenings', 'every', 'saturday', 'in', 'april', 'in', 'rich', 'auditorium', ',', 'the', 'museum', 'has', 'already', 'shown', 'master', 'filmmaker', 'sat', '##ya', '##jit', 'ray', \"'\", 's', '1977', 'historical', 'drama', '\"', 'the', 'chess', 'players', '.', '\"', 'coming', 'this', 'saturday', 'is', '\"', 'the', 'cloud', 'capped', 'star', ',', '\"', 'director', 'ri', '##t', '##wick', 'g', '##hat', '##ak', \"'\", 's', '1960', 'tale', 'of', 'a', 'middle', '-', 'class', 'refugee', 'family', 'struggling', 'to', 'survive', 'in', 'calcutta', '.', 'and', 'coming', 'to', 'u', '.', 's', '.', 'theaters', 'later', 'this', 'year', 'or', 'in', '2003', 'will', 'be', 'mira', '##max', \"'\", 's', 'art', '##y', '\"', 'the', 'warrior', ',', '\"', 'director', 'as', '##if', 'ka', '##pad', '##ia', \"'\", 's', 'first', 'feature', ',', 'a', 'cinema', '##to', '##g', '-', 'rap', '##hy', '-', 'heavy', 'historical', 'drama', 'set', 'in', 'northern', 'india', ',', 'and', 'the', 'spiritual', 'love', 'story', '\"', 'the', 'sam', '##sar', '##a', ',', '\"', 'from', 'pan', 'na', '##lin', ',', 'a', 'self', '-', 'taught', 'director', 'from', 'the', 'several', 'indian', 'films', 'that', 'make', 'a', 'bit', 'of', 'a', 'splash', ',', '\"', 'says', 'aria', '##nna', 'bo', '##cco', ',', 'senior', 'vice', 'president', 'of', 'acquisitions', 'for', 'mira', '##max', '.', 'the', 'ones', 'arriving', 'now', '\"', 'happen', 'to', 'be', 'very', 'good', 'movies', ',', '\"', 'she', 'says', '.', '\"', 'it', \"'\", 's', 'hard', 'to', 'say', 'if', 'they', 'are', 'different', 'from', 'the', 'norm', ',', 'but', 'they', 'are', 'crossing', 'over', 'better', '.', '\"', 'already', 'this', 'year', ',', 'the', 'sp', '##lish', '-', 'splash', '##y', 'bollywood', 'market', 'has', 'made', 'one', 'of', 'its', 'biggest', 'waves', 'ever', 'in', 'the', 'west', '.', '\"', 'la', '##ga', '##an', ':', 'once', 'upon', 'a', 'time', 'in', 'india', ',', '\"', 'a', 'nearly', 'four', '-', 'hour', ',', 'pg', '-', 'rated', 'epic', 'involving', 'six', 'extravagant', 'musical', 'numbers', 'and', 'a', 'game', 'of', 'cricket', ',', 'became', 'only', 'the', 'third', 'indian', 'movie', 'to', 'be', 'nominated', 'for', 'an', 'academy', 'award', 'for', 'best', 'foreign', 'language', 'film', '.', 'though', 'it', 'lost', 'the', 'oscar', 'to', 'bosnia', '-', 'her', '##ze', '##go', '##vin', '##ia', \"'\", 's', 'war', '-', 'is', '-', 'pe', '##ll', '-', 'mel', '##l', 'satire', '\"', 'no', 'man', \"'\", 's', 'land', ',', '\"', '\"', 'la', '##ga', '##an', \"'\", 's', '\"', 'nomination', 'was', 'enough', 'to', 'spark', 'spontaneous', 'singing', 'by', 'the', 'film', \"'\", 's', 'actors', 'outside', 'star', '-', 'producer', 'aa', '##mir', 'khan', \"'\", 's', 'office', 'in', 'bombay', '.', 'the', 'oscar', 'nod', '\"', 'is', 'a', 'major', 'breakthrough', ',', '\"', 'indian', 'film', 'critic', 'sub', '##has', '##h', 'j', '##ha', 'recently', 'told', 'the', 'can', 'hold', 'its', 'head', 'high', 'and', 'reach', 'out', 'to', 'the', 'world', '.', '\"', 'booming', 'market', 'traditional', 'bollywood', '-', '-', 'the', 'world', \"'\", 's', 'largest', 'movie', 'industry', ',', 'reportedly', 'selling', '100', 'million', 'tickets', 'a', 'week', 'in', 'india', '-', '-', 'is', 'as', 'different', 'from', 'hi', '##ng', '##lish', 'films', 'and', 'indian', 'art', 'fare', 'as', 'hollywood', \"'\", 's', '\"', 'sing', '##in', \"'\", 'in', 'the', 'rain', '\"', 'is', 'from', 'the', 'independent', '\"', 'in', 'the', 'bedroom', '\"', 'and', 'the', 'outsider', 'artist', '##ry', 'of', 'richard', 'link', '##late', '##r', \"'\", 's', '\"', 'waking', 'life', '.', '\"', 'bollywood', '(', 'the', 'word', 'is', 'an', 'ama', '##lga', '##m', 'of', '\"', 'bombay', '\"', 'and', '\"', 'hollywood', '\"', ')', 'makes', 'nearly', '1', ',', '000', 'flick', '##s', 'a', 'year', '.', 'most', 'are', 'a', 'mixture', 'of', 'violence', ',', 'amour', 'and', 'bus', '##by', 'berkeley', ',', 'feeding', 'penn', '##ies', 'from', 'heaven', 'to', 'hundreds', 'of', 'millions', 'of', 'indians', ',', 'many', 'living', 'in', 'poverty', '.', 'indian', 'audiences', 'typically', '\"', 'do', 'n', \"'\", 't', 'want', 'to', 'pay', ',', 'in', 'some', 'instances', ',', 'a', 'fourth', 'of', 'their', 'month', \"'\", 's', 'salary', 'for', 'a', 'ticket', 'and', 'see', 'their', 'own', 'lives', 'up', 'on', 'the', 'screen', ',', '\"', 'ku', '##ku', '##no', '##or', 'explains', '.', '\"', 'they', \"'\", 'd', 'rather', 'have', 'a', 'film', 'transport', 'them', 'to', 'switzerland', 'for', 'a', 'song', 'and', 'norway', 'for', 'another', '.', 'it', \"'\", 's', 'much', 'never', 'seen', 'a', 'bollywood', 'movie', ',', 'chances', 'are', 'you', \"'\", 've', 'gotten', 'a', 'taste', 'of', 'one', '.', 'lu', '##hr', '##mann', 'has', 'talked', 'about', 'how', 'bollywood', 'filmmaking', 'influenced', 'his', 'own', 'oscar', '-', 'winning', '\"', 'mo', '##ulin', 'rouge', '.', '\"', 'you', 'can', 'see', 'it', 'in', 'the', 'nonstop', 'eye', 'candy', 'and', 'the', 'fr', '##illy', ',', 'fr', '##ivo', '##lous', 'musical', 'numbers', '.', 'last', 'year', \"'\", 's', 'oscar', '-', 'nominated', '\"', 'ghost', 'world', '\"', 'opened', 'with', 'a', 'musical', 'clip', 'from', 'the', '1965', 'bollywood', 'go', '-', 'go', '-', 'get', '##ter', '\"', 'gum', '##na', '##am', ',', '\"', 'with', 'that', 'movie', \"'\", 's', 'big', 'hair', ',', 'gold', 'lame', 'and', 'z', '##orro', '-', 'masked', 'hull', '##aba', '##lo', '##o', 'dancers', '.', 'in', 'metro', 'atlanta', ',', 'bollywood', 'films', 'are', 'the', 'staple', 'of', 'galaxy', 'cinema', 'international', 'in', 'g', '##win', '##nett', 'county', 'and', 'bollywood', 'cinema', 'in', 'stone', 'mountain', '.', 'both', 'theaters', 'are', 'popular', 'with', 'the', 'large', 'indian', 'population', 'in', 'north', 'georgia', ',', 'including', 'the', 'estimated', '50', ',', '000', 'who', 'live', 'in', 'g', '##win', '##nett', '.', 'the', 'stone', 'mountain', 'theater', 'frequently', 'debuts', 'bollywood', 'films', 'at', 'the', 'same', 'time', 'they', \"'\", 're', 'released', 'in', 'india', '.', 'bollywood', 'cinema', 'is', 'now', 'screening', '\"', 'aa', '##nk', '##hen', ',', '\"', 'which', 'debuted', 'here', 'april', '5', ',', 'the', 'same', 'day', 'as', 'its', 'big', 'premiere', 'in', 'malaysia', 'to', 'usher', 'in', 'the', 'latest', 'indian', 'international', 'film', 'academy', 'awards', '.', '\"', 'aa', '##nk', '##hen', '\"', 'is', 'in', 'the', 'indian', 'government', \"'\", 's', 'attempt', 'to', 'halt', 'gangster', 'involvement', 'in', 'the', 'industry', '.', 'in', 'recent', 'months', ',', 'one', 'bollywood', 'star', \"'\", 's', 'home', 'was', 'burned', 'and', 'indian', 'police', 'shot', 'and', 'killed', 'four', 'gunmen', 'who', 'reportedly', 'had', 'been', 'plotting', 'to', 'murder', '\"', 'la', '##ga', '##an', \"'\", 's', '\"', 'khan', ',', 'the', 'film', \"'\", 's', 'producer', 'and', 'lead', 'actor', ',', 'and', 'others', 'if', 'the', 'movie', \"'\", 's', 'overseas', 'distribution', 'rights', 'were', 'n', \"'\", 't', 'handed', 'over', 'to', 'their', 'gang', 'leader', '.', '\"', 'some', 'of', 'the', 'bollywood', 'actors', 'i', 'know', ',', 'they', 'all', 'drive', 'with', 'police', 'escorts', ',', '\"', 'ku', '##ku', '##no', '##or', 'says', '.', '\"', 'they', 'come', 'to', 'a', 'party', 'with', 'two', 'cop', 'cars', 'in', 'front', 'and', 'a', 'motorcycle', 'in', 'back', '.', 'that', \"'\", 's', 'their', 'life', 'now', '.', 'we', 'were', 'having', 'coffee', 'with', 'one', 'of', 'the', 'actors', ',', 'and', 'outside', 'there', 'were', 'two', 'gunmen', 'standing', 'there', 'with', 'arms', '.', '\"', 'the', 'piracy', 'factor', 'usa', \"'\", 's', 'foley', 'says', 'piracy', 'has', 'also', 'been', 'a', 'problem', 'with', 'indian', 'films', '.', 'pirate', '##d', 'dvds', 'of', '\"', 'monsoon', 'wedding', '\"', 'probably', 'hit', 'america', ',', 'especially', 'new', 'york', ',', 'at', 'the', 'same', 'time', 'the', 'movie', 'opened', 'in', 'theaters', '.', '\"', 'it', \"'\", 's', 'just', 'in', '##fur', '##iating', ',', '\"', 'foley', 'finding', 'a', 'cure', 'for', 'the', 'common', 'cold', '.', '\"', 'of', 'all', 'the', 'recent', 'indian', 'films', ',', '\"', 'wedding', '\"', 'is', 'the', 'one', 'scoring', 'most', 'with', 'americans', '.', '\"', 'i', 'think', 'that', 'it', 'is', 'a', 'hi', '##ng', '##lish', 'film', 'helps', 'a', 'lot', ',', '\"', 'foley', 'says', '.', '\"', 'when', 'the', 'dialogue', 'suddenly', 'slips', 'from', 'english', 'to', 'hindi', ',', 'there', 'is', 'a', 'natural', '##ism', 'with', 'it', '.', 'i', 'have', 'to', 'say', 'that', 'sub', '##titles', 'in', 'all', 'foreign', 'films', 'are', 'often', 'annoying', 'as', 'hell', '.', 'they', 'are', 'distracting', '.', 'the', 'beauty', 'in', \"'\", 'monsoon', 'wedding', \"'\", 'is', 'that', 'the', 'language', 'is', 'not', 'a', 'barrier', ',', 'and', 'the', 'visuals', 'are', 'stunning', '.', '\"', 'for', 'his', 'part', ',', 'ku', '##ku', '##no', '##or', 'might', 'take', 'his', 'own', 'indian', 'filmmaking', 'to', 'fresh', 'ground', 'with', 'his', 'next', 'project', '.', 'he', \"'\", 's', 'considering', 'shooting', 'a', 'movie', 'entirely', 'in', 'america', '.', 'it', 'would', 'be', 'a', 'comedy', 'called', '\"', 'a', 'touch', 'of', 'ill', '##og', '##ic', '\"', 'and', 'involve', 'second', '-', 'generation', 'indians', 'living', 'in', 'the', 'states', '.', 'a', 'recent', 'letter', 'in', 'the', 'online', 'screen', 'magazine', ',', 'which', 'covers', 'the', 'entertainment', 'business', 'in', 'india', ',', 'expressed', 'one', 'movie', '##go', '##er', \"'\", 's', 'view', 'of', 'modern', 'indian', 'filmmaking', '.', '\"', 'though', 'hi', '##ng', '##lish', 'films', 'these', 'up', ',', '\"', 'wrote', 'j', '##yo', '##ti', '##ran', '##jan', 'bis', '##wal', 'of', 'ang', '##ul', ',', 'india', '.', '\"', 'when', 'most', 'producers', 'and', 'directors', 'of', 'bollywood', 'are', 'churning', 'out', 'hum', '##drum', 'movies', ',', 'every', 'now', 'and', 'then', 'we', 'have', 'people', 'like', 'na', '##ges', '##h', 'ku', '##ku', '##no', '##or', 'trying', 'their', 'best', 'to', 'dish', 'out', 'something', 'special', '.', '\"', 'mira', 'nair', ',', 'director', 'of', 'the', 'hit', 'drama', '\"', 'monsoon', 'wedding', ',', '\"', 'is', 'being', 'hailed', 'as', 'a', 'trail', '##bla', '##zer', 'of', 'indian', 'filmmaking', '.', '/', 'usa', 'films', 'photo', ':', 'director', 'na', '##ges', '##h', 'ku', '##ku', '##no', '##or', ':', 'a', 'low', '-', 'key', 'life', 'in', 'brook', '##haven', ',', 'but', 'a', 'celebrity', 'in', 'india', '.', '/', 'joey', 'ivan', '##sco', '/', 'staff', 'photo', ':', 'va', '##sund', '##hara', 'das', 'portrays', 'the', 'bride', 'in', 'an', 'arranged', 'marriage', 'in', 'new', 'delhi', 'in', '\"', 'monsoon', 'wedding', ',', '\"', 'director', 'mira', 'nair', \"'\", 's', '\"', 'hi', '##ng', '##lish', '\"', 'drama', 'that', 'has', 'earned', 'nearly', '$', '4', '.', '5', 'million', 'in', 'u', '.', 's', '.', 'theaters', 'in', 'five', 'weeks', '.', 'photo', ':', 'india', 'on', '##screen', '>', '\"', 'monsoon', 'wedding', '\"', '-', '-', 'aj', '##c', 'film', 'critic', 'eleanor', 'ring', '##el', 'gillespie', 'says', ',', '\"', 'r', '.', 's', '.', 'v', '.', 'p', '.', 'this', 'wedding', '.', 'a', 'family', 'gathering', 'for', 'a', 'lavish', 'punjabi', 'wedding', 'in', 'new', 'delhi', 'provides', 'an', 'ex', '##uber', '##ant', 'illustration', 'of', 'the', 'clash', 'between', 'indian', 'tradition', 'and', 'western', 'modernization', '.', '\"', 'director', 'mira', 'nair', 'employs', 'bits', 'of', 'bollywood', 'musical', '##ity', 'that', 'inter', '##we', '##aves', 'english', 'and', 'hindi', ',', 'with', 'moderate', 'use', 'of', 'sub', '##titles', '.', 'at', 'le', '##fo', '##nt', 'garden', 'hills', '.', 'call', '404', '-', '266', '-', '220', '##2', 'for', 'prices', 'and', 'showtime', '##s', '.', '>', '\"', 'aa', '##nk', '##hen', '\"', '-', '-', 'a', 'traditional', 'bollywood', 'film', 'involving', 'high', 'style', ',', 'sunglasses', ',', 'music', 'and', ',', 'of', 'course', ',', 'danger', '.', 'at', 'bollywood', 'cinema', '.', '54', '##32', 'memorial', 'drive', ',', 'stone', 'mountain', '.', 'call', '770', '-', '43', '##8', '-', '64', '##64', 'for', 'prices', ',', 'showtime', '##s', 'and', 'other', 'films', '.', '>', '\"', 'company', '\"', '-', '-', 'stars', 'malayalam', '(', 'south', 'indian', ')', 'cinema', 'veteran', 'mohan', '##lal', '.', 'at', 'galaxy', 'cinema', 'international', '(', 'which', 'features', 'movies', 'from', 'india', ',', 'pakistan', ',', 'bangladesh', ',', 'china', ',', 'taiwan', ',', 'korea', ',', 'iran', 'and', 'other', 'countries', ')', '.', 'green', \"'\", 's', 'corner', 'shopping', 'center', ',', '49', '##75', 'jimmy', 'carter', 'boulevard', ',', 'nor', '##cross', '.', 'call', '770', '-', '93', '##1', '-', '345', '##6', 'for', 'prices', ',', 'showtime', '##s', 'and', 'other', 'films', '.', 'on', 'television', '>', '\"', 'asian', 'variety', 'show', '\"', '(', '10', 'a', '.', 'm', '.', 'saturdays', 'on', 'w', '##p', '##xa', ')', '-', '-', 'bollywood', 'movie', 'preview', '##s', ',', 'celebrity', 'interviews', 'and', 'box', 'office', 'reports', '.', 'suggested', 'on', 'dvd', '/', 'video', ':', '>', '\"', 'la', '##ga', '##an', ':', 'once', 'upon', 'a', 'time', 'in', 'india', '\"', '-', '-', 'this', 'three', '-', 'hour', '-', 'and', '-', '45', '-', 'minute', 'bollywood', 'epic', 'was', 'nominated', 'this', 'year', 'for', 'an', 'academy', 'award', 'for', 'best', 'foreign', 'against', 'british', 'rulers', 'on', 'a', 'game', 'of', 'cricket', '.', '>', '\"', 'sal', '##aa', '##m', 'bombay', '!', '\"', '-', '-', 'director', 'mira', 'nair', \"'\", 's', '1988', 'drama', 'about', 'children', 'begging', 'on', 'the', 'streets', 'of', 'bombay', '.', 'un', '##rated', '.', 'it', 'was', 'nominated', 'for', 'an', 'oscar', 'for', 'best', 'foreign', 'film', 'in', '1989', '.', '>', '\"', 'path', '##er', 'pan', '##chal', '##i', ',', '\"', '\"', 'ap', '##ara', '##jit', '##o', '\"', 'and', '\"', 'the', 'world', 'of', 'ap', '##u', '\"', '-', '-', 'director', 'sat', '##ya', '##jit', 'ray', \"'\", 's', 'critically', 'acclaimed', 'trilogy', 'depicting', 'the', 'life', 'of', 'ap', '##u', ',', 'from', 'boy', 'in', 'an', 'impoverished', 'family', 'in', 'a', 'bengali', 'village', 'to', 'man', 'and', 'father', '.', 'the', 'films', 'were', 'released', ',', 'respectively', ',', 'in', '1955', ',', '1956', 'and', '1959', '.', 'all', 'three', 'are', 'available', 'on', 'video', 'at', 'www', '.', 'amazon', '.', 'com', '.', 'onstage', '>', 'heart', 'th', '##ro', '##bs', 'live', '-', '-', 'bollywood', 'stars', 'frequently', 'tour', 'onstage', 'in', 'america', 'in', 'a', 'kind', 'of', 'mill', '##i', '-', 'van', '##ill', '##i', 'extra', '##va', '##gan', '##za', '(', 'bollywood', 'stars', 'rarely', 'sing', 'their', 'own', 'songs', ';', 'they', 'lip', '-', 'sync', '##h', ')', '.', 'this', 'event', 'features', 'hr', '##ith', '##ik', 'ro', '##shan', ',', 'ka', '##reen', '##a', 'kapoor', ',', 'kari', '##sma', 'kapoor', ',', 'arjun', 'ramp', '##al', 'and', 'others', '.', '9', ':', '15', 'p', '.', 'm', '.', 'may', '17', 'at', 'hi', '##fi', 'buys', 'amp', '##hit', '##heater', '.', 'tickets', 'range', 'from', '$', '45', 'to', '$', '150', '.', '770', '-', '90', '##8', '-', '04', '##0', '##4', ',', 'www', '.', 'wow', '##now', '.', 'com', '.', 'on', 'the', 'web', '>', 'www', '.', 'wow', '##now', '.', 'com', '-', '-', 'local', 'web', 'site', 'events', 'and', 'movie', 'info', ',', 'with', 'links', 'to', 'more', '.', '>', 'www', '.', 'bollywood', '##world', '.', 'com', '-', '-', 'photos', ',', 'movie', 'preview', '##s', ',', 'music', 'and', 'chat', 'rooms', '.', '>', 'www', '.', 'cha', '##lo', '##cine', '##ma', '.', 'com', '-', '-', 'movie', 'chat', 'rooms', ',', 'news', ',', 'movie', 'updates', 'and', 'star', 'profiles', '.', '>', '*', '*', '25', ';', '2010', ';', 'tool', '##ong', '-', '-', 'a', 'place', 'to', 'submit', 'your', 'own', 'reviews', 'of', 'selected', 'indian', 'movies', '.', '>', '*', '*', '29', ';', '203', '##7', ';', 'tool', '##ong', '-', '-', 'official', 'web', 'site', 'for', '\"', 'monsoon', 'wedding', '.', '\"', '-', '-', 'bob', 'long', '##ino', 'graphic', ':', 'india', 'on', 'film', '>', 'film', 'festival', 'of', 'india', ':', 'the', 'high', 'museum', 'of', 'art', 'presents', 'its', 'first', 'indian', 'film', 'festival', '.', 'screenings', 'are', 'at', '8', 'p', '.', 'm', '.', 'saturdays', 'in', 'april', ',', 'including', '\"', 'the', 'cloud', 'capped', 'star', '\"', '(', 'in', 'bengali', 'with', 'sub', '##titles', ')', 'on', 'saturday', 'and', '\"', 'in', 'search', 'of', 'famine', '\"', '(', 'in', 'bengali', 'with', 'sub', '##titles', ')', 'on', 'april', '27', '.', '$', '4', '-', '$', '5', '.', 'rich', 'auditorium', ',', 'wood', '##ruff', 'arts', 'center', ',', 'peach', '##tree', 'and', '15th', 'streets', '.', '404', '-', '73', '##3', '-', '45', '##70', ',', 'www', '.', 'high', '.', 'org', '.', '>', 'more', 'inside', ':', 'on', 'the', 'big', 'screen', ',', 'on', 'dvd', '/', 'video', ',', 'and', 'on', 'the', 'web', '.', 'l', '##5', '>', 'check', 'the', 'international', 'calendar', 'in', 'wednesday', \"'\", 's', 'atlanta', '&', ';', 'the', 'world', 'section', 'for', 'happening', '##s', 'in', 'the', 'metro', 'area', 'or', 'go', 'to', 'aj', '##c', '.', 'com', '/', 'world', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a9a284e7-2f93-46ad-b70b-1cb5e3834118",
        "id": "RxSTCjztShCL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)\n",
        "evaluate_lstm"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r30/30 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2344624549150467, 0.9333333373069763]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J9fdwQn2ShCN"
      },
      "source": [
        "###### BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kFQq6suOShCN",
        "colab": {}
      },
      "source": [
        "train_labels = train_labels.astype(np.int64)\n",
        "validation_labels = validation_labels.astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FWvpGd__ShCP",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wr1fYu59ShCQ",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f47984ce-1b90-4b0e-c4ff-fc7115559082",
        "id": "WdNPduAmShCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nu8DC1nWShCU",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qGkruXPfShCV",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXIWwPEDShCW",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yzg-rndXShCX",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lS_9fabZShCY",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d57ade45-6d02-4111-a8c9-a68e83717b6e",
        "id": "PywYsWW8ShCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a498b3a9-5248-488f-e07e-0704b9042c90",
        "id": "O5AZ1kNGShCb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3214767671683255,\n",
              " 0.2836875056519228,\n",
              " 0.27270139227895174,\n",
              " 0.2581087201833725]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0e2ffc71-768a-445e-99f9-de1e20559b05",
        "id": "SMEKk3itShCc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(period2_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2814121f-bca5-4364-c26d-a6950a0ae240",
        "id": "B7hjj8vgShCd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(period2_test.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = period2_test.sentence.values\n",
        "labels = period2_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length=512\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels.astype(np.int64))\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fc12eb14-4e68-43b0-8d7d-35e7f51d10c5",
        "id": "P_SjHzu1ShCe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 30 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5f09e088-c231-4d3e-c22b-3fedd24afd30",
        "id": "m8LMmDZIShCf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (period2_test.label.sum(), len(period2_test.label), (period2_test.label.sum() / len(period2_test.label) * 100.0)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 2 of 30 (6.67%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a5721c8c-ec50-415d-dbb7-bd92aff8d701",
        "id": "cBJ9OGndShCg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "39a3db44-b12b-45e3-f6f0-3eecf5266dd2",
        "id": "zxhAe3SfShCh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "62753c29-9633-4af2-9800-a2af21b195f9",
        "id": "b6WpBViOShCi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2db55108-9b0b-48bf-c817-612724f94ad0",
        "id": "u4vDkiIQShCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './period2/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./period2/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./period2/vocab.txt',\n",
              " './period2/special_tokens_map.json',\n",
              " './period2/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H-hrj_v1SgIi"
      },
      "source": [
        "#### Period 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e90e67a9-ef33-422b-d436-31ac9a4fb50b",
        "id": "CrVThghsSgIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "period3 = data[data.period==3]\n",
        "period3"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>once again , ideology trumps good science and...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>women overcame years of gender apartheid-and ...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>again , rape surfaces as an international war...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>although they seem day-and-night different , ...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>bishop t.d . jakes wants his flock not only t...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3364</th>\n",
              "      <td>(end-video-clip) (end-video-clip) (end-video-...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3365</th>\n",
              "      <td>a recent u.s. supreme court ruling that nonci...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3366</th>\n",
              "      <td>las vegas -- aces are aces , but pairs of nin...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3367</th>\n",
              "      <td>brown # bread molasses and salted butterscotc...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3374</th>\n",
              "      <td>rome - iran for the first time joined a u.s.-...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>379 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "567    once again , ideology trumps good science and...     0       3\n",
              "568    women overcame years of gender apartheid-and ...     0       3\n",
              "569    again , rape surfaces as an international war...     0       3\n",
              "575    although they seem day-and-night different , ...     0       3\n",
              "578    bishop t.d . jakes wants his flock not only t...     0       3\n",
              "...                                                 ...   ...     ...\n",
              "3364   (end-video-clip) (end-video-clip) (end-video-...     0       3\n",
              "3365   a recent u.s. supreme court ruling that nonci...     0       3\n",
              "3366   las vegas -- aces are aces , but pairs of nin...     0       3\n",
              "3367   brown # bread molasses and salted butterscotc...     0       3\n",
              "3374   rome - iran for the first time joined a u.s.-...     0       3\n",
              "\n",
              "[379 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7jcWyoLY6F2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67b5fb17-4033-4e7a-f1d1-394d92247e1a"
      },
      "source": [
        "period3.label.sum()/len(period3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09234828496042216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5mqLdT2pSgIm",
        "colab": {}
      },
      "source": [
        "period3_train, period3_test = train_test_split(period3, random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ebbecca0-b70e-4580-9f2e-18e2f9727b8e",
        "id": "EWJ3J0lbSgIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period3_train), len(period3_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "360 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "01a980fa-274d-4cdf-8f89-ff4d23ec34b4",
        "id": "wPKD_L9XSgIr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period3_train.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period3_train.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'in', 'july', ',', 'lebanon', \"'\", 's', 'leadership', 'finally', 'formed', 'a', 'so', '-', 'member', 'cabinet', ',', 'with', 'the', 'hezbollah', '-', 'led', 'opposition', 'gaining', 'decisive', 'new', 'powers', '.', 'this', 'is', 'the', 'nation', \"'\", 's', 'first', 'united', 'government', 'since', 'november', '2006', ',', 'when', 'an', 'opposition', 'walk', '##out', 'triggered', 'an', '18', '##mont', '##h', 'political', 'crisis', ',', 'including', 'six', 'months', 'during', 'which', 'parliament', '-', 'unable', 'to', 'pass', 'legislation', 'for', 'more', 'than', 'a', 'year', '-', 'could', 'n', \"'\", 't', 'agree', 'on', 'a', 'president', '.', 'women', 'have', 'been', 'hit', 'hard', 'by', 'this', 'crisis', ',', 'the', 'expansion', 'of', 'their', 'rights', 'put', 'on', 'hold', '.', 'but', 'the', 'women', \"'\", 's', 'movement', ',', 'eager', 'to', 'make', 'gender', 'equality', 'a', 'priority', 'for', 'this', 'new', 'cabinet', ',', 'has', 'already', 'mobilized', '-', 'email', '##ing', 'supporters', ',', 'drafting', 'letters', 'to', 'ministers', 'and', 'organizing', 'a', 'sit', '-', 'in', 'before', 'parliament', '-', 'to', 'raise', 'awareness', ',', 'especially', 'regarding', 'equal', 'nationality', 'rights', '.', 'women', 'in', 'lebanon', 'have', 'been', 'unable', 'to', 'share', 'citizenship', 'with', 'their', 'husbands', 'or', 'children', 'since', '1943', ',', 'when', 'lebanon', 'won', 'independence', 'from', 'france', '.', 'lina', 'ab', '##ou', '-', 'ha', '##bib', 'says', 'that', 'since', 'the', '2005', 'assassination', 'of', 'former', 'prime', 'minister', 'raf', '##ik', 'hari', '##ri', 'and', 'subsequent', 'political', 'instability', ',', '\"', 'it', \"'\", 's', 'as', 'if', 'they', \"'\", 've', 'the', 'government', 'been', 'given', 'a', 'legitimate', 'excuse', 'to', 'go', 'into', 'a', 'coma', '.', '\"', 'ab', '##ou', '-', 'ha', '##bib', 'development', 'action', '(', 'cr', '##t', '##d', '-', 'a', ')', ',', 'which', 'runs', 'the', 'middle', 'east', 'regional', 'campaign', '\"', 'my', 'nationality', ':', 'a', 'right', 'for', 'me', 'and', 'my', 'family', '.', '\"', 'cr', '##t', '##d', '-', 'a', 'also', 'tries', 'to', 'change', 'family', '-', 'status', 'laws', '(', 'each', 'of', 'lebanon', \"'\", 's', 'officially', 'recognized', '17', 'religious', 'sect', '##s', 'leg', '##is', '##lates', 'its', 'own', 'personal', 'and', 'family', 'codes', ')', 'and', 'works', 'toward', 'rural', 'women', \"'\", 's', 'economic', 'empowerment', '.', 'many', 'lebanese', 'accused', 'syria', 'of', 'involvement', 'in', 'hari', '##ri', \"'\", 's', 'assassination', ',', 'which', 'syrian', 'president', 'basha', '##r', 'al', '-', 'ass', '##ad', 'denies', ',', 'but', 'mass', 'anti', '-', 'syrian', 'rallies', 'led', 'ass', '##ad', 'to', 'withdraw', 'troops', 'from', 'lebanon', 'in', '2005', 'for', 'the', 'first', 'time', 'in', '29', 'years', '.', 'this', 'was', 'the', 'last', 'foreign', 'military', 'presence', 'to', 'depart', ',', 'and', 'feminist', '##s', 'believed', 'the', 'newfound', 'sovereignty', 'would', 'mean', 'that', 'women', \"'\", 's', 'political', 'time', 'had', 'come', '.', 'but', 'plans', 'halted', 'when', 'hezbollah', 'captured', 'two', 'israeli', 'soldiers', 'in', 'july', '2006', ';', 'israel', 're', '##tal', '##iated', 'and', '34', 'days', 'of', 'fighting', 'ensued', ',', 'dec', '##imating', 'lebanon', \"'\", 's', 'infrastructure', '.', 'after', ',', 'five', 'cabinet', 'members', 'aligned', 'with', 'the', 'opposition', 'resigned', ',', 'para', '##ly', '##zing', 'the', 'government', ',', 'reign', '##iting', 'fears', 'of', 'civil', 'war', '.', '\"', 'the', 'lebanese', 'situation', 'has', 'never', 'been', 'easy', 'for', 'women', '.', 'the', 'issues', 'should', 'remain', 'on', 'the', 'agenda', ',', 'cr', '##t', '##d', '-', 'a', 'refused', 'to', 'be', 'silent', '.', 'now', 'she', 'is', 'optimistic', ',', 'since', 'the', 'new', 'interior', 'minister', ',', 'human', '-', 'rights', 'lawyer', 'z', '##yi', '##ad', 'bar', '##oud', ',', 'is', 'a', '\"', 'staunch', 'supporter', 'of', 'the', 'nationality', 'campaign', '.', '\"', 'ironically', 'lebanon', 'once', 'had', 'a', 'big', 'women', \"'\", 's', 'movement', '.', 'author', 'za', '##yna', '##b', 'fa', '##w', '##wa', '##z', 'al', '-', 'ami', '##li', '(', '1860', '-', '1914', ')', ',', 'who', 'published', 'a', 'who', \"'\", 's', 'who', 'of', '45', '##5', 'women', 'famous', 'in', 'their', 'societies', ',', 'was', 'one', 'of', 'the', 'first', 'arab', '-', 'muslim', 'women', 'to', 'question', 'the', 'veil', 'and', 'protest', 'poly', '##gam', '##y', '.', 'women', 'won', 'suffrage', 'in', '1953', ',', 'earlier', 'than', 'in', 'most', 'arab', 'countries', '.', 'there', 'were', '25', 'feminist', 'journals', 'published', 'during', 'the', '1920s', '.', 'women', 'still', 'walk', 'through', 'beirut', 'in', 'tank', 'tops', 'as', 'well', 'as', 'black', 'head', 'scar', '##ves', '.', 'but', 'such', 'liberties', 'are', 'dec', '##ei', '##ving', '.', '\"', 'women', \"'\", 's', 'rights', 'are', 'on', 'the', 'back', 'burn', '##er', 'more', 'than', 'ever', '.', 'the', 'presidential', 'address', 'by', 'newly', 'elected', 'michel', 'sl', '##eim', '##an', 'in', 'may', 'did', 'not', 'mention', 'anything', 'at', 'all', 'about', 'women', \"'\", 's', 'issues', ',', '\"', 'notes', 'dr', '.', 'dim', '##a', 'da', '##bb', '##ous', '-', 'sense', '##nig', ',', 'director', 'at', 'the', 'institute', 'for', 'women', \"'\", 's', 'studies', 'in', 'the', 'arab', 'world', ',', 'established', 'at', 'the', 'lebanese', 'american', 'university', 'hi', '1973', '.', 'lebanese', 'feminism', '\"', 'is', 'leader', ';', 'everything', 'is', 'now', 'on', 'hold', ',', '\"', 'adds', 'professor', 'anita', 'nas', '##sar', ',', 'the', 'institute', \"'\", 's', 'assistant', 'director', '.', 'a', 'rise', 'in', 'fundamental', '##ist', 'islam', 'has', 'made', 'it', 'harder', 'for', 'feminist', 'groups', ',', 'who', 'face', 'conservative', 'opposition', 'that', 'labels', 'activists', '\"', 'import', '##ers', 'of', 'western', 'ideas', '.', '\"', 'since', 'the', 'nationality', 'campaign', 'began', ',', 'regimes', 'many', 'lebanese', 'regard', 'as', 'more', 'oppressive', ',', 'such', 'as', 'algeria', ',', 'egypt', 'and', 'morocco', ',', 'have', 'reformed', 'their', 'nationality', 'laws', '.', 'excuses', 'against', 'reforming', 'lebanese', 'law', 'indicate', 'a', 'fear', 'of', 'altered', 'demographics', 'in', 'a', 'nation', 'formed', 'on', 'a', 'pre', '##car', '##ious', 'balance', 'between', 'muslims', 'and', 'christians', ',', 'plus', 'a', 'concern', 'that', 'reset', '##tling', 'palestinian', 'refugees', 'wed', 'to', 'lebanese', 'women', 'would', 'deny', 'them', 'right', 'of', 'return', 'should', 'palestinians', 'and', 'israelis', 'reach', 'a', 'deal', '.', 'this', 'fear', 'does', 'n', \"'\", 't', 'extend', 'to', 'palestinian', 'women', 'being', 'granted', 'citizenship', 'once', 'they', 'marry', 'a', 'lebanese', 'man', '.', 'violence', 'against', 'women', 'is', 'another', 'issue', 'ignored', 'during', 'political', 'strife', '.', 'rash', '##a', 'mo', '##um', '##ne', '##h', ',', 'co', '-', 'coordinator', 'of', 'a', 'campaign', 'by', 'the', 'group', 'ka', '##fa', '(', 'enough', 'violence', 'and', 'exploitation', ')', ',', 'was', 'quoted', 'in', 'the', 'daily', 'star', 'newspaper', 'saying', ',', '\"', 'there', 'is', 'no', 'current', 'legislation', 'to', 'protect', 'women', 'from', 'domestic', 'and', 'women', 'find', 'themselves', 'banging', 'their', 'heads', 'against', 'a', 'legal', 'brick', 'wall', '.', '\"', 'lebanon', 'signed', 'the', 'u', '.', 'n', '.', \"'\", 's', 'convention', 'on', 'the', 'elimination', 'of', 'all', 'forms', 'of', 'discrimination', 'against', 'women', '(', 'ce', '##da', '##w', ')', ',', 'but', 'the', 'ce', '##da', '##w', 'committee', 'has', 'called', 'on', 'the', 'nation', 'to', 'en', '##act', 'anti', '-', 'domestic', '-', 'violence', 'laws', '.', 'the', 'committee', 'also', 'expressed', 'concern', 'about', 'three', 'lebanese', 'penal', '##code', 'articles', ',', 'which', 'mit', '##igate', 'guilt', 'in', 'the', 'case', 'of', 'so', '-', 'called', 'honor', 'crimes', ',', 'tolerate', 'marital', 'rape', 'and', 'allow', 'for', 'charges', 'to', 'be', 'dropped', 'in', 'rape', 'cases', '.', 'though', 'no', 'current', 'statistics', 'on', 'domestic', 'violence', 'in', 'lebanon', 'exist', ',', 'a', '2006', 'study', 'reported', 'family', 'violence', 'as', 'a', 'common', ',', 'significant', 'health', 'issue', '.', 'there', 'is', 'lip', '-', 'service', 'support', 'for', 'women', 'from', 'politicians', ',', 'itself', 'uncommon', 'in', 'many', 'arab', 'countries', ',', 'but', 'nobody', 'will', 'act', '.', 'g', '##has', '##san', 'mo', '##uk', '##hei', '##ber', ',', 'an', 'independent', 'member', 'and', 'vocal', 'supporter', 'for', 'changing', 'disc', '##rim', '##inatory', 'laws', ',', 'says', 'women', \"'\", 's', 'rights', 'are', 'one', 'of', 'many', 'issues', 'stuck', 'on', 'the', 'back', 'burn', '##er', 'until', 'the', 'politics', 'are', 'sorted', 'through', ':', '\"', 'it', \"'\", 's', 'not', 'just', 'gender', 'that', \"'\", 's', 'being', 'she', '##lved', ',', 'it', \"'\", 's', 'anti', '##cor', '##rup', '##tion', 'laws', ',', 'economic', 'reforms', ',', 'living', 'conditions', '.', '\"', 'for', 'politics', 'is', 'n', \"'\", 't', 'a', 'valid', 'justification', 'for', 'the', 'slow', 'speed', 'of', 'gender', 'reforms', '.', 'there', 'are', 'only', 'six', 'women', 'in', 'parliament', ',', 'most', 'of', 'them', 'widows', 'or', 'members', 'of', 'political', 'dynasties', '(', 'hari', '##ri', \"'\", 's', 'sister', ',', 'bahia', ',', 'is', 'minister', 'of', 'education', ',', 'the', 'cabinet', \"'\", 's', 'sole', 'woman', ')', '.', 'mp', 'g', '##hin', '##wa', 'ja', '##llo', '##ul', 'did', 'introduce', 'a', 'bill', 'to', 'change', 'the', 'nationality', 'law', 'in', '2006', ',', 'but', 'she', \"'\", 's', 'been', 'quiet', 'on', 'the', 'issue', 'since', 'the', 'war', '.', 'the', 'women', 'mps', '\"', 'are', 'not', 'effective', ',', 'they', \"'\", 're', 'just', 'following', 'their', 'men', 'and', 'have', 'no', 'independent', 'opinions', ',', '\"', 'says', 'ik', '##bal', 'al', '-', 'shay', '##eb', 'g', '##hane', '##m', ',', 'former', 'vice', 'president', 'of', 'the', 'lebanese', 'council', 'of', 'women', ',', 'an', 'umbrella', 'ngo', 'for', 'over', '170', 'women', \"'\", 's', 'groups', '.', 'g', '##hane', '##m', 'has', 'worked', 'to', 'introduce', 'a', 'gender', 'quota', 'for', 'parliament', 'since', 'the', 'mid', '-', '1990s', '.', '\"', 'it', \"'\", 's', 'almost', 'impossible', 'now', 'because', 'of', 'the', 'situation', ',', 'but', 'we', 'ca', 'n', \"'\", 't', 'wait', 'on', 'the', 'time', 'being', 'right', ',', '\"', 'g', '##hane', '##m', 'says', '.', '\"', 'people', 'are', 'watching', 'tv', ',', 'watching', 'politics', '.', 'we', \"'\", 're', 'trying', 'our', 'best', 'to', 'awake', '##n', 'them', 'to', 'their', 'rights', '.', 'once', 'we', 'get', 'through', ',', 'they', 'say', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0XyaF9YsSgIt",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "137953a3-12d4-49fc-9ba1-5ec6782bfebe",
        "id": "nFTQM0GqSgIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period3_train), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "360 360 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mBzvQATkSgIw",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.05)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SBjfF62TSgIz"
      },
      "source": [
        "###### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MuyusfM9SgI0",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e7ab4b00-1977-47df-c2d8-f4eccba95e2a",
        "id": "bwSwM2wbSgI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Using Sigmoid Activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='sigmoid'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4806e5b9-d39d-471e-cb05-2c6d6dde5973",
        "id": "T5mvNCFdSgI3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 342 samples, validate on 18 samples\n",
            "Epoch 1/10\n",
            "342/342 [==============================] - 3s 9ms/step - loss: 0.6645 - acc: 0.8801 - val_loss: 0.5758 - val_acc: 0.8889\n",
            "Epoch 2/10\n",
            "342/342 [==============================] - 2s 7ms/step - loss: 0.3832 - acc: 0.9123 - val_loss: 0.3572 - val_acc: 0.8889\n",
            "Epoch 3/10\n",
            "342/342 [==============================] - 2s 7ms/step - loss: 0.2952 - acc: 0.9123 - val_loss: 0.3493 - val_acc: 0.8889\n",
            "Epoch 4/10\n",
            "342/342 [==============================] - 2s 6ms/step - loss: 0.2971 - acc: 0.9123 - val_loss: 0.3518 - val_acc: 0.8889\n",
            "Epoch 5/10\n",
            "342/342 [==============================] - 2s 6ms/step - loss: 0.2966 - acc: 0.9123 - val_loss: 0.3534 - val_acc: 0.8889\n",
            "Epoch 6/10\n",
            "342/342 [==============================] - 2s 6ms/step - loss: 0.2917 - acc: 0.9123 - val_loss: 0.3472 - val_acc: 0.8889\n",
            "Epoch 7/10\n",
            "342/342 [==============================] - 2s 6ms/step - loss: 0.2824 - acc: 0.9123 - val_loss: 0.3552 - val_acc: 0.8889\n",
            "Epoch 8/10\n",
            "342/342 [==============================] - 2s 7ms/step - loss: 0.2733 - acc: 0.9123 - val_loss: 0.3462 - val_acc: 0.8889\n",
            "Epoch 9/10\n",
            "342/342 [==============================] - 2s 6ms/step - loss: 0.2542 - acc: 0.9123 - val_loss: 0.3384 - val_acc: 0.8889\n",
            "Epoch 10/10\n",
            "342/342 [==============================] - 2s 6ms/step - loss: 0.2165 - acc: 0.9123 - val_loss: 0.3201 - val_acc: 0.8889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fef43664-b991-4a67-ee36-3e8a40595abb",
        "id": "viELpk13SgI5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period3_test.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period3_test.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'in', 'white', 'house', 'meetings', 'beginning', 'monday', ',', 'president', 'ho', '##s', '##ni', 'mu', '##bara', '##k', 'of', 'egypt', 'is', 'expected', 'to', 'tell', 'the', 'obama', 'administration', 'that', 'arab', 'nations', 'want', 'peace', ',', 'but', 'are', 'unwilling', 'to', 'ab', '##ide', 'mr', '.', 'obama', \"'\", 's', 'call', 'to', 'make', 'good', '-', 'faith', 'concessions', 'to', 'israel', 'until', 'israel', 'takes', 'tangible', 'steps', 'like', 'freezing', 'settlements', ',', 'an', 'egyptian', 'official', 'said', '.', '#', 'as', 'part', 'of', 'its', 'effort', 'to', 'res', '##us', '##cit', '##ate', 'the', 'peace', 'process', ',', 'the', 'obama', 'administration', 'has', 'asked', 'arab', 'countries', 'to', 'make', 'small', 'but', 'symbolic', 'gestures', 'to', 'normal', '##ize', 'relations', 'with', 'israel', ',', 'like', 'allowing', 'planes', 'to', 'fly', 'through', 'their', 'airspace', 'or', 'improving', 'cultural', 'ties', '.', 'the', 'administration', 'has', 'also', 'asked', 'israel', 'to', 'freeze', 'all', 'growth', 'in', 'settlements', '.', '#', 'so', 'far', ',', 'neither', 'side', 'has', 'agreed', 'to', 'mr', '.', 'obama', \"'\", 's', 'proposed', 'first', 'steps', ',', 'and', 'so', 'the', 'president', 'is', 'expected', 'to', 'look', 'to', 'mr', '.', 'mu', '##bara', '##k', 'for', 'help', 'in', 'breaking', 'the', 'latest', 'middle', 'east', 'dead', '##lock', ',', 'regional', 'analysts', 'said', '.', '#', 'mr', '.', 'mu', '##bara', '##k', 'flew', 'from', 'cairo', 'to', 'washington', 'on', 'saturday', 'for', 'his', 'first', 'american', 'visit', 'in', 'five', 'years', ',', 'accompanied', 'by', 'foreign', 'minister', 'ahmed', 'ab', '##ou', '##l', 'g', '##hei', '##t', 'and', 'gen', '.', 'omar', 'sul', '##eim', '##an', ',', 'chief', 'of', 'egypt', \"'\", 's', 'intelligence', 'president', 'joseph', 'r', '.', 'bid', '##en', 'jr', '.', 'and', 'other', 'officials', ',', 'and', 'is', 'to', 'meet', 'with', 'mr', '.', 'obama', 'on', 'tuesday', '.', '#', 'mr', '.', 'mu', '##bara', '##k', 'will', 'tell', 'mr', '.', 'obama', 'that', 'from', 'the', 'arab', 'perspective', ',', 'the', 'best', 'way', 'to', 'build', 'confidence', 'is', 'to', 'press', 'israel', 'to', 'freeze', 'settlements', ',', 'implement', 'an', 'economic', 'plan', 'to', 'improve', 'life', 'in', 'the', 'west', 'bank', ',', 'ease', 'pressure', 'on', 'gaza', 'and', 'agree', 'to', 'negotiate', 'with', 'all', 'issues', 'on', 'the', 'table', ',', 'including', 'the', 'status', 'of', 'jerusalem', 'and', 'refugees', ',', 'said', 'ambassador', 'ho', '##ssa', '##m', 'za', '##ki', ',', 'spokesman', 'for', 'egypt', \"'\", 's', 'foreign', 'ministry', '.', '#', \"'\", \"'\", 'if', 'they', 'do', 'this', 'and', 'engage', 'immediately', 'in', 'negotiations', 'with', 'abu', 'maze', '##n', ',', 'this', 'is', 'a', 'recipe', 'for', 'open', '##ness', 'and', 'the', 'arabs', 'will', 'make', 'the', 'gestures', 'needed', ',', \"'\", 'mr', '.', 'za', '##ki', 'said', ',', 'referring', 'to', 'mahmoud', 'abbas', ',', 'president', 'of', 'the', 'palestinian', 'authority', '.', \"'\", \"'\", 'but', 'they', 'do', 'n', \"'\", 't', 'want', 'to', 'make', 'this', 'first', 'step', '.', 'they', 'are', 'demanding', 'the', 'arabs', 'make', 'the', 'first', 'step', '.', 'the', 'arabs', 'should', 'not', 'make', 'the', 'first', 'step', '.', 'they', 'are', 'the', 'occupying', 'power', '.', 'the', 'occupation', 'must', 'end', '.', \"'\", '#', 'in', 'many', 'ways', ',', 'mr', '.', 'an', 'old', 'script', ',', 'as', 'arabs', 'and', 'israelis', 'argue', 'which', 'side', 'should', 'go', 'first', ',', 'arab', 'states', 'rev', '##ert', 'to', 'their', 'old', 'roles', 'in', 'the', 'region', ',', 'and', 'the', 'united', 'states', 'temper', '##s', 'its', 'criticism', 'of', 'egypt', \"'\", 's', 'political', 'and', 'human', 'rights', 'record', 'in', 'return', 'for', 'egypt', \"'\", 's', 'regional', 'cooperation', '.', '#', 'during', 'the', 'bush', 'years', ',', 'the', 'region', \"'\", 's', 'more', 'radical', 'forces', ',', 'those', 'against', 'the', 'peace', 'process', ',', 'had', 'the', 'upper', 'hand', ',', 'including', 'iran', ',', 'syria', 'and', 'hamas', ',', 'the', 'militant', 'palestinian', 'group', 'that', 'now', 'controls', 'the', 'gaza', 'strip', '.', '#', 'but', 'while', 'the', 'dynamics', 'of', 'the', 'region', 'are', 'always', 'fluid', ',', 'the', 'tone', ',', 'at', 'the', 'moment', ',', 'appears', 'to', 'favor', 'those', 'in', 'the', 'peace', 'camp', ',', 'regional', 'analysts', 'said', '.', 'that', 'shift', 'has', 'been', 'attributed', 'in', 'part', 'to', 'iran', 'being', 'distracted', 'by', 'the', 'internal', 'political', 'tu', '##mu', '##lt', 'over', 'its', 'disputed', 'presidential', 'election', ',', 'and', 'mr', '.', 'obama', \"'\", 's', 'outreach', 'to', 'the', 'muslim', 'world', ',', 'especially', 'his', 'speech', 'in', 'cairo', ',', 'which', 'has', 'won', 'the', 'president', ',', 'if', 'not', 'the', 'united', 'states', ',', 'popular', 'good', 'will', '.', '#', \"'\", \"'\", 'the', 'ex', '##tre', '##mist', 'forces', 'now', 'in', 'the', 'region', 'are', 'to', 'some', 'extent', 'ambassador', 'to', 'the', 'united', 'states', 'and', 'chairman', 'of', 'the', 'egyptian', 'council', 'for', 'foreign', 'affairs', '.', \"'\", \"'\", 'if', 'you', 'notice', 'for', 'instance', 'hamas', ',', 'hamas', \"'\", 's', 'discourse', 'has', 'begun', 'to', 'soft', '##en', 'a', 'bit', '.', 'if', 'you', 'notice', 'syria', ',', 'now', 'syria', 'is', 'actually', 'cooper', '##ating', 'with', 'the', 'united', 'states', '.', \"'\", '#', 'mr', '.', 'mu', '##bara', '##k', \"'\", 's', 'visit', 'also', 'signals', 'an', 'effort', 'to', 're', '-', 'establish', 'egypt', 'as', 'the', 'united', 'states', 'chief', 'strategic', 'arab', 'ally', 'after', 'years', 'of', 'tension', 'and', 'an', '##imo', '##sity', 'between', 'washington', 'and', 'cairo', '.', 'mr', '.', 'mu', '##bara', '##k', 'had', 'refused', 'to', 'visit', 'washington', 'in', 'protest', 'over', 'president', 'bush', \"'\", 's', 'middle', 'east', 'policies', ',', 'the', 'invasion', 'of', 'iraq', 'and', 'the', 'public', 'criticism', 'of', 'egypt', \"'\", 's', 'political', 'and', 'human', 'rights', 'record', '.', '#', 'the', 'bush', 'administration', 'effectively', 'side', '##lined', 'egypt', ',', 'turning', 'more', 'to', 'saudi', 'arabia', 'as', 'a', 'regional', 'force', 'to', 'counter', 'the', 'growing', 'influence', 'of', 'iran', 'and', 'push', 'forward', 'a', 'peace', 'initiative', 'that', 'the', 'saudi', 'king', 'initially', 'sponsored', ',', 'political', 'analysts', 'here', 'said', '.', 'but', 'today', ',', 'the', 'saudi', '##s', 'have', 'stepped', 'back', '.', '#', 'political', 'analysts', 'said', 'that', 'mr', '.', 'obama', 'was', 'pressing', 'saudi', 'arabia', 'to', 'take', 'the', 'lead', 'in', 'offering', 'so', '-', 'called', 'confidence', '-', 'building', 'measures', 'to', 'israel', ',', 'but', 'the', 'peace', 'initiative', 'endorsed', 'by', 'all', '22', 'members', 'of', 'the', 'arab', 'league', '.', '#', \"'\", \"'\", 'saudi', 'arabia', 'will', 'not', 'accept', 'to', 'take', 'any', 'steps', 'before', 'israel', 'shows', 'that', 'it', 'wants', 'peace', 'to', 'be', 'its', 'first', 'choice', ',', \"'\", 'said', 'anwar', 'maj', '##id', 'es', '##h', '##ki', ',', 'chairman', 'of', 'the', 'middle', 'east', 'center', 'for', 'strategic', 'and', 'legal', 'studies', ',', 'a', 'research', 'center', 'in', 'ji', '##dda', ',', 'saudi', 'arabia', '.', \"'\", \"'\", 'president', 'mu', '##bara', '##k', 'will', 'listen', 'to', 'the', 'demands', 'of', 'the', 'united', 'states', 'and', 'then', 'present', 'the', 'arab', 'point', 'of', 'view', 'in', 'this', 'regard', '.', \"'\", '#', 'egypt', 'remains', 'weighed', 'down', 'by', 'domestic', 'problems', ',', 'including', 'high', 'unemployment', ',', 'widespread', 'poverty', 'and', 'uncertainty', 'over', 'who', 'will', 'replace', 'mr', '.', 'mu', '##bara', '##k', ',', 'who', 'is', '81', 'and', 'in', 'his', '28th', 'year', 'in', 'office', '.', 'but', 'the', 'white', 'house', 'appears', 'to', 'have', 'calculated', 'that', 'egypt', ',', 'as', 'the', 'largest', 'arab', 'nation', 'with', 'regional', 'goals', 'similar', 'to', 'washington', \"'\", 's', ',', 'remains', 'the', 'best', 'place', 'to', 'turn', '.', '#', \"'\", \"'\", 'the', 'united', 'states', 'has', 'to', 'have', 'a', 'regional', 'power', 'to', 'coordinate', 'its', 'policies', 'with', 'and', 'egypt', 'can', 'not', 'be', 'a', 'regional', 'power', 'without', 'the', 'united', 'states', ',', \"'\", 'said', 'mr', '.', 'reed', '##y', ',', 'some', 'kind', 'of', 'a', 'complementary', 'relationship', '.', \"'\", '#', 'mor', '##de', '##chai', 'ked', '##ar', ',', 'a', 'former', 'israeli', 'military', 'intelligence', 'officer', 'and', 'now', 'a', 'lecturer', 'at', 'bar', '-', 'il', '##an', 'university', 'in', 'jerusalem', ',', 'said', 'that', 'the', 'meeting', 'between', 'mr', '.', 'mu', '##bara', '##k', 'and', 'mr', '.', 'obama', 'was', 'important', ',', 'but', 'that', 'their', 'discussion', 'should', 'focus', 'on', 'what', 'israel', 'insists', 'is', 'the', 'primary', 'problem', 'in', 'the', 'region', ',', 'iran', \"'\", 's', 'nuclear', 'program', 'and', 'its', 'regional', 'ambitions', '.', '#', \"'\", \"'\", 'the', 'issue', 'is', 'iran', 'and', 'what', 'seems', 'to', 'be', 'an', 'american', 'reluctance', 'to', 'take', 'care', 'of', 'this', 'problem', ',', 'which', 'is', 'much', 'more', 'meaningful', 'for', 'mu', '##bara', '##k', 'than', 'israel', ',', \"'\", 'he', 'said', '.', '#', 'ambassador', 'za', '##ki', ',', 'of', 'the', 'egyptian', 'foreign', 'ministry', ',', 'said', 'that', 'the', 'presidents', 'would', 'address', 'many', 'issues', 'besides', 'the', 'peace', 'process', ',', 'including', 'iran', ',', 'sudan', 'and', 'ex', '##tre', '##mism', '.', '#', 'but', 'on', 'the', 'peace', 'process', ',', 'he', 'said', ',', 'egypt', \"'\", 's', 'opinion', 'is', 'un', '##sha', '##ka', '##ble', '.', '#', \"'\", \"'\", 'we', 'think', 'this', 'huge', 'gap', 'of', 'confidence', 'requires', 'movement', 'from', 'the', 'israelis', 'first', ',', \"'\", 'he', 'said', '.', \"'\", \"'\", 'then', 'the', 'arabs', 'are', 'willing', 'to', 'make', 'gestures', '.', 'this', 'is', 'the', 'way', 'arabs', 'see', 'it', '.', \"'\", \"'\", '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05938035-83fa-46bd-de1f-62ac8b9e7e15",
        "id": "oojN2hSqSgI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)\n",
        "evaluate_lstm"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r19/19 [==============================] - 0s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4725702106952667, 0.8421052694320679]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w7DmrLbUSgI8"
      },
      "source": [
        "###### BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9lagDQejSgI9",
        "colab": {}
      },
      "source": [
        "train_labels = train_labels.astype(np.int64)\n",
        "validation_labels = validation_labels.astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p4EF7MYPSgI_",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dyBM_hklSgJH",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a4b01e35-5753-4436-e7f6-638a6d531163",
        "id": "jbmpjEs4SgJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r6CTERA3SgJK",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SZlzKlCkSgJL",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4O0xUj9aSgJO",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oJzwcXCASgJP",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Tatn5iLSgJQ",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "22974917-8b9b-454a-c5d0-26a21831d184",
        "id": "zpkDFlBdSgJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b26de931-2964-421a-dc62-f551db155791",
        "id": "MtRyTDVnSgJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37997413900765503,\n",
              " 0.30528532578186557,\n",
              " 0.29806255075064575,\n",
              " 0.2937267639420249]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eeb87083-1504-4890-bec1-7136528ebaeb",
        "id": "KhxYtQXPSgJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(period3_test)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f6e80fb9-30ce-442f-8e01-8dbe09dd1df9",
        "id": "xF0Cku-ISgJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(period3_test.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = period3_test.sentence.values\n",
        "labels = period3_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length=512\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels.astype(np.int64))\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 19\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cb569b94-46ad-4fdd-cd6c-5915d64140f3",
        "id": "mHi_VjtoSgJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 19 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a2bfdb1a-2029-45aa-b927-4072e41d1f92",
        "id": "-z2SgfwzSgJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (period3_test.label.sum(), len(period3_test.label), (period3_test.label.sum() / len(period3_test.label) * 100.0)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 3 of 19 (15.79%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4VqLMW0aF6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7e71504-9de2-4d92-d63a-4bb8943824a3"
      },
      "source": [
        "true_labels"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e8fd28da-fdee-4cd2-9e60-753d61d42988",
        "id": "D9QmHD4sSgJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c18b324c-2703-42ec-a4e5-f2a9ecc39312",
        "id": "iX81r2BYSgJb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0de3dc73-a5d2-4032-a8ec-212ce1fc50f4",
        "id": "9pIhz-YtSgJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13c3a66b-2e13-48e1-f749-ce6736a5742a",
        "id": "C9xQPWD8SgJg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './period3/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./period3/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./period3/vocab.txt',\n",
              " './period3/special_tokens_map.json',\n",
              " './period3/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ldEZ-pK1SciT"
      },
      "source": [
        "#### Period 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7a1ffb0f-7001-414c-de0c-ccadb71ed454",
        "id": "2RHTJgHQSciV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "period4 = data[data.period==4]\n",
        "period4"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>calls for the creation of a no-fly zone over ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>he 's not here in \" welcome to the cafeteria ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>as a graduate student in theology , i lived i...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>get ready for more undernourished infants , d...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>sunday , march 27 exodus 17:1-7 translated li...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3987</th>\n",
              "      <td>!kelly-mcevers# oh , thanks , nice to meet yo...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3988</th>\n",
              "      <td>!margaret-warner# and i 'm margaret warner . ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3989</th>\n",
              "      <td>!judy-woodruff# the syrian opposition , u.n ....</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3990</th>\n",
              "      <td>!charlie-rose : good morning . it is friday d...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3991</th>\n",
              "      <td>!lee-cowan# this morning , spiritual leaders ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>514 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  period\n",
              "738    calls for the creation of a no-fly zone over ...     0       4\n",
              "739    he 's not here in \" welcome to the cafeteria ...     0       4\n",
              "740    as a graduate student in theology , i lived i...     0       4\n",
              "741    get ready for more undernourished infants , d...     1       4\n",
              "742    sunday , march 27 exodus 17:1-7 translated li...     1       4\n",
              "...                                                 ...   ...     ...\n",
              "3987   !kelly-mcevers# oh , thanks , nice to meet yo...     0       4\n",
              "3988   !margaret-warner# and i 'm margaret warner . ...     0       4\n",
              "3989   !judy-woodruff# the syrian opposition , u.n ....     0       4\n",
              "3990   !charlie-rose : good morning . it is friday d...     0       4\n",
              "3991   !lee-cowan# this morning , spiritual leaders ...     0       4\n",
              "\n",
              "[514 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPdeFc0AaS0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05f90af5-32ce-44ce-975c-9e8744c90639"
      },
      "source": [
        "period4.label.sum()/len(period4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1828793774319066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eNBB2YCGSciY",
        "colab": {}
      },
      "source": [
        "period4_train, period4_test = train_test_split(period4, random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "539e42b0-706a-41cb-87bb-dfbc7a2d5e81",
        "id": "X-x0X3acScia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period4_train), len(period4_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "488 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "81eb19bb-c836-4be2-9296-4e2592abb1fa",
        "id": "AdsG1UfRScid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period4_train.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period4_train.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'essay', 'the', 'politics', '#', 'the', 'labour', 'leader', \"'\", 's', 'mission', 'to', 'res', '##ha', '##pe', 'capitalism', 'in', 'britain', '#', 'it', 'was', 'the', 'kind', 'of', 'challenge', 'that', 'made', 'tony', 'blair', 'and', 'gordon', 'brown', 'wince', '.', '\"', 'when', 'are', 'you', 'going', 'to', 'bring', 'back', 'socialism', '?', '\"', 'called', 'the', 'man', 'in', 'the', 'crowd', 'to', 'ed', 'mil', '##iba', '##nd', 'at', 'a', 'rally', 'in', 'brighton', 'on', 'the', 'eve', 'of', 'his', 'party', \"'\", 's', 'annual', 'conference', 'in', 'september', '.', 'the', 'labour', 'leader', 'paused', 'before', 'offering', 'a', 'response', 'that', 'his', 'two', 'predecessors', 'would', 'not', 'have', 'dared', 'deliver', ':', '\"', 'that', \"'\", 's', 'what', 'we', \"'\", 're', 'doing', ',', 'sir', '.', '\"', '#', 'when', ',', 'later', 'that', 'week', ',', 'mil', '##iba', '##nd', 'used', 'his', 'conference', 'speech', 'to', 'call', 'for', 'a', 'freeze', 'in', 'energy', 'bills', ',', 'conservatives', 'needed', 'no', 'more', 'evidence', 'to', 'declare', 'that', 'ed', 'had', 'definitive', '##ly', 'coloured', 'himself', 'red', '.', 'he', 'was', 'portrayed', 'by', 'hostile', 'media', 'as', 'a', 'left', '-', 'wing', 'fan', '##atic', ',', 'dragging', 'his', 'party', 'back', 'to', 'the', '1970s', 'with', 'price', 'controls', 'and', 'assaults', 'on', 'free', 'enterprise', '.', 'yet', 'the', 'attack', 'on', 'energy', 'companies', 'was', 'popular', '.', 'it', 'steered', 'the', 'conversation', 'about', 'britain', \"'\", 's', 'economy', 'away', 'from', 'budget', 'discipline', '-', '-', 'where', 'the', 'tori', '##es', 'feel', 'secure', '-', '-', 'to', 'in', 'need', 'of', 'a', 'response', 'to', 'what', 'mil', '##iba', '##nd', 'had', 'branded', '\"', 'the', 'cost', 'of', 'living', 'crisis', '\"', '.', '#', 'the', 'labour', 'leader', 'followed', 'up', 'that', 'success', 'by', 'shuffling', 'his', 'shadow', 'cabinet', ',', 'promoting', 'young', 'mps', 'from', 'the', '2010', 'parliamentary', 'intake', 'and', 'side', '##lining', 'veterans', '.', 'in', 'westminster', ',', 'this', 'was', 'interpreted', 'as', 'a', 'consolidation', 'of', 'power', ',', 'el', '##eva', '##ting', 'a', 'cad', '##re', 'of', '\"', 'mil', '##iba', '##ndi', '##tes', '\"', 'largely', 'at', 'the', 'expense', 'of', '\"', 'blair', '##ites', '\"', '.', 'when', 'he', 'was', 'first', 'elected', 'to', 'the', 'leadership', ',', 'mil', '##iba', '##nd', 'had', 'promised', 'to', '\"', 'turn', 'the', 'page', '\"', 'on', 'new', 'labour', '.', 'now', 'it', 'seemed', 'the', 'whole', 'book', 'had', 'been', 'burned', '.', '#', 'is', 'mil', '##iba', '##nd', 'writing', 'the', 'sequel', '-', '-', 'or', 'has', 'his', 'leadership', 'been', 'an', 'exercise', 'in', 'sm', '##ud', '##ging', 'the', 'line', 'between', '\"', 'old', '\"', 'and', '\"', 'new', '\"', 'labour', 'scripts', '?', 'the', 'purpose', 'of', 'this', 'essay', 'is', 'not', 'to', 'judge', 'whether', 'he', 'has', 'found', 'the', 'formula', 'for', 'winning', 'the', 'next', 'election', '.', 'no', 'one', 'knows', 'that', '.', 'i', 'contend', 'simply', 'that', ',', 'beneath', 'the', 'fr', '##oth', 'of', 'volatile', 'daily', 'news', ',', 'a', 'coherent', 'pattern', 'is', 'disc', '##ern', '##ible', 'in', 'the', 'labour', 'leader', \"'\", 's', 'actions', '.', 'there', 'is', 'a', 'consistent', 'analysis', 'of', 'what', 'remedy', '.', 'with', 'labour', 'leading', 'in', 'opinion', 'polls', '18', 'months', 'before', 'an', 'election', ',', 'it', 'is', 'worth', 'taking', 'seriously', 'the', 'possibility', 'that', 'britain', 'will', 'one', 'day', 'be', 'governed', 'by', 'a', 'creed', 'called', 'mil', '##iba', '##ndi', '##sm', '.', '#', 'this', 'is', 'not', 'a', 'view', 'widely', 'held', 'in', 'westminster', '.', 'even', 'many', 'labour', 'mps', 'are', 'sc', '##ept', '##ical', '.', 'the', 'standard', 'version', 'of', 'mil', '##iba', '##nd', \"'\", 's', 'story', 'is', 'that', 'he', 'got', 'the', 'job', 'by', 'sub', '##ter', '##fu', '##ge', '-', '-', 'gaming', 'a', 'faulty', 'election', 'process', 'with', 'the', 'help', 'of', 'trade', 'union', 'bosses', '-', '-', 'and', 'was', 'promptly', 'exposed', 'as', 'lacking', 'an', 'agenda', '.', 'he', 'has', 'been', 'forced', 'ever', 'since', 'to', 'rely', 'on', 'di', '##s', '##jo', '##int', '##ed', 'tactical', 'man', '##oe', '##u', '##vres', 'to', 'stay', 'in', 'the', 'game', '.', 'in', 'this', 'view', ',', 'his', 'posture', '##s', 'have', 'been', 'dictated', 'less', 'by', 'conviction', 'than', 'by', 'the', 'need', 'to', 'manage', 'tensions', 'between', 'factions', 'of', 'left', 'and', 'right', ',', 'blair', '##ites', 'and', 'the', 'rest', '.', '#', 'mil', '##iba', '##nd', \"'\", 's', 'friends', 'con', '##cede', 'that', 'the', 'circumstances', 'of', 'his', 'election', 'put', 'him', 'at', 'a', 'disadvantage', '.', 'he', 'had', 'no', 'base', 'in', 'the', 'parliamentary', 'party', ',', 'no', 'political', 'machine', '.', 'he', 'was', 'pill', '##ori', '##ed', 'in', 'the', 'media', 'from', 'day', 'one', '.', 'compromise', 'was', 'needed', 'to', 'secure', 'his', 'position', '.', 'that', 'does', 'not', 'pre', '##cl', '##ude', 'the', 'closest', 'advisers', 'have', 'always', 'insisted', 'that', 'there', 'is', 'a', 'long', '-', 'term', 'plan', 'and', 'that', 'its', 'outline', 'would', 'be', 'visible', 'in', 'time', 'for', 'a', '2015', 'general', 'election', '.', 'are', 'they', 'right', '?', '#', 'the', 'first', 'step', 'in', 'understanding', 'mil', '##iba', '##ndi', '##sm', 'is', 'to', 'recognise', 'why', 'a', 'page', 'needed', 'turning', 'after', 'the', 'new', 'labour', 'era', ',', 'which', 'had', ',', 'after', 'all', ',', 'achieved', 'three', 'successive', 'election', 'victories', '.', 'the', 'defeat', 'of', '2010', 'marked', 'the', 'end', 'of', 'a', 'journey', 'that', 'began', 'with', 'the', 'trauma', 'of', 'neil', 'kin', '##nock', \"'\", 's', 'defeat', 'by', 'john', 'major', 'in', '1992', '.', 'that', 'result', 'taught', 'a', 'generation', 'of', 'rising', 'labour', 'stars', '-', '-', 'blair', ',', 'brown', 'and', 'peter', 'man', '##del', '##son', '-', '-', 'that', 'society', ',', 'not', 'just', 'the', 'economy', ',', 'had', 'been', 'ref', '##ashi', '##oned', 'by', 'tory', 'rule', '.', 'the', 'opposition', 'would', 'be', 'locked', 'out', 'of', 'power', 'for', 'as', 'long', 'as', 'it', 'denied', 'that', 'liberated', 'markets', 'had', 'made', 'people', 'prosperous', '.', 'labour', 'could', 'not', 'ignore', 'the', 'aspirations', 'of', 'a', 'self', '-', 'made', 'middle', 'class', ',', 'nor', 'urge', 'it', 'to', 'pay', 'higher', 'taxes', 'for', 'the', 'sake', 'of', 'equality', '.', '#', 'in', '2002', ',', 'at', 'a', 'reception', 'in', 'hampshire', ',', 'margaret', 'thatcher', 'was', 'asked', 'what', 'she', 'considered', 'her', 'greatest', 'achievement', '.', 'she', 'replied', 'our', 'opponents', 'to', 'change', 'their', 'minds', '.', '\"', 'she', 'was', 'half', '-', 'right', '.', 'labour', \"'\", 's', 'mind', 'was', 'altered', 'by', 'fear', 'of', 'perpetual', 'defeat', 'but', 'the', 'party', \"'\", 's', 'heart', 'was', 'anguish', '##ed', '.', 'the', 'new', 'labour', 'government', 'ex', '##uded', 'anxiety', 'at', 'the', 'thought', 'of', 'being', 'exposed', 'as', 'explicitly', 'left', 'wing', '.', 'blair', 'never', 'lacked', 'self', '-', 'confidence', 'but', 'labour', 'governed', 'with', 'the', 'jump', '##iness', 'of', 'an', 'occupying', 'army', '.', 'under', 'gordon', 'brown', ',', 'that', 'brittle', '##ness', ',', 'inca', '##rna', '##te', 'in', 'the', 'prime', 'minister', ',', 'became', 'a', 'governing', 'pathology', '.', '#', 'in', 'that', 'context', ',', 'the', 'financial', 'crisis', 'that', 'erupted', 'in', '2007', 'was', 'a', 'moment', 'of', 'both', 'ex', '##ult', '##ation', 'and', 'existent', '##ial', 'panic', 'for', 'labour', '.', 'the', 'cause', 'of', 'the', 'cal', '##ami', '##ty', 'appeared', 'to', 'be', 'turbo', '-', 'charged', 'market', 'capitalism', 'run', 'am', '##ok', 'on', 'a', 'global', 'scale', '.', 'it', 'might', 'have', 'been', 'a', 'moment', 'of', 'vin', '##dication', 'for', 'the', 'centre', 'left', 'but', 'blair', \"'\", 's', 'and', 'brown', \"'\", 's', 'successes', 'had', 'been', 'pre', '##dicated', 'on', 'partial', 'surrender', 'to', 'the', 'enemy', 'ideology', '.', 'so', 'labour', \"'\", 's', 'demo', '##ral', '##isation', 'was', 'more', 'complex', 'than', 'the', 'usual', 'disappointment', 'at', 'having', 'been', 'beaten', '.', 'the', 'formula', 'that', 'had', 'been', 'devised', 'in', 'the', 'mid', '-', '1990s', 'for', 'escaping', 'opposition', 'was', 'felt', 'to', 'be', 'obsolete', 'at', 'best', ',', 'treason', '##ous', 'at', 'worst', 'wood', ',', 'an', 'oxford', 'academic', 'and', 'downing', 'street', 'adviser', 'under', 'brown', ',', 'was', 'sitting', 'in', 'the', 'front', 'room', 'of', 'ed', 'mil', '##iba', '##nd', \"'\", 's', 'home', 'in', 'north', 'london', 'conducting', 'a', 'post', '-', 'mort', '##em', 'on', 'the', 'defeat', '.', 'the', 'question', 'the', 'two', 'men', 'gr', '##apple', '##d', 'with', 'was', 'this', ':', 'was', 'there', 'an', 'appetite', 'in', 'britain', 'for', 'a', 'new', 'political', 'offering', 'from', 'the', 'left', '-', '-', 'one', 'that', 'could', 'find', 'mass', 'appeal', 'with', 'radical', 'determination', 'to', 'fix', 'broken', 'markets', ';', 'one', 'that', 'would', 'promise', 'to', 'work', 'on', 'behalf', 'of', 'the', 'consumer', ',', 'not', 'simply', 'strengthen', 'the', 'hand', 'of', 'the', 'state', '?', 'the', 'conversation', 'ranged', 'from', 'the', 'monopoly', '-', 'bust', '##ing', 'crusade', '##s', 'of', 'the', 'us', 'presidents', 'william', 'howard', 'taft', 'and', 'teddy', 'roosevelt', 'to', 'germany', \"'\", 's', 'postwar', 'reconstruction', 'under', 'chancellor', 'konrad', 'aden', '##auer', '.', 'might', 'an', 'optimistic', 'story', 'of', 'what', 'the', 'british', 'left', 'did', 'next', 'be', 'salvage', '##d', 'from', 'its', 'crushing', 'defeat', '?', '#', 'fast', '-', 'forward', 'to', 'the', 'summer', 'of', '2013', 'and', 'wood', ',', 'by', 'then', 'en', '##nob', '##led', 'as', 'a', 'peer', ',', 'along', 'with', 'mil', '##iba', '##nd', 'and', 'his', 'chief', 'speech', '-', 'writer', ',', 'marc', 'ste', '##ars', '(', 'another', 'oxford', 'academic', ')', ',', 'were', 'dividing', 'their', 'time', 'between', 'the', 'same', 'front', 'room', 'and', 'the', 'nearby', 'ka', '##len', '##dar', 'caf', 'in', 'high', '-', 'gate', ',', 'channel', '##ling', 'the', 'spirit', 'address', '.', '#', 'the', 'critics', 'are', 'wrong', 'to', 'say', 'that', 'mil', '##iba', '##nd', \"'\", 's', 'project', 'is', 'erratic', 'or', 'hastily', 'assembled', '.', '(', 'if', 'anything', ',', 'the', 'charge', 'that', 'it', 'is', 'too', 'determined', '##ly', 'intellectual', 'is', 'more', 'fitting', '.', ')', 'mil', '##iba', '##ndi', '##sm', 'takes', 'a', 'deep', 'perspective', ',', 'charting', 'long', 'political', 'cycles', 'from', 'the', 'postwar', 'period', 'to', 'the', 'present', 'day', '.', 'the', 'first', 'phase', 'encompasses', 'a', 'period', 'of', 'consensus', 'about', 'active', 'state', 'management', 'of', 'the', 'economy', ',', 'whitehall', '-', 'led', 'intervention', 'and', 'support', 'for', 'the', 'welfare', 'state', '.', 'that', 'un', '##rave', '##lled', 'in', 'the', '1970s', '-', '-', 'a', 'decade', 'of', 'inter', '##re', '##gn', '##um', 'marked', 'by', 'economic', 'st', '##ag', '##nation', 'and', 'inc', '##on', '##clusive', 'elections', '.', 'the', 'breakthrough', 'was', 'achieved', 'by', 'margaret', 'thatcher', '.', 'her', '1979', 'victory', 'was', 'narrowly', 'won', 'but', '-', '-', 'helped', 'by', 'splits', 'on', 'the', 'left', 'and', 'suicidal', 'labour', 'opposition', '-', '-', 'it', 'evolved', 'into', 'ideological', 'he', '##ge', '##mony', '.', 'individual', 'ambition', ',', 'released', 'from', 'state', 'control', ',', 'would', 'be', 'the', 'motor', 'of', 'progress', '.', 'riches', 'would', 'trickle', 'down', 'the', 'social', 'hierarchy', '.', '#', 'this', 'phase', 'lasts', 'up', 'to', 'the', 'collapse', 'of', 'lehman', 'brothers', 'bank', 'in', 'september', '2008', '.', 'new', 'labour', ',', 'in', 'this', 'view', ',', 'limited', 'its', 'ambition', 'to', 'com', '##pen', '##sat', '##ing', 'the', 'losers', 'from', 'the', 'new', 'consensus', 'with', 'revenues', 'skimmed', 'stealth', '-', '-', 'an', 'approach', 'made', 'easy', 'in', 'the', 'short', 'term', 'since', 'the', 'city', 'of', 'london', 'served', 'as', 'a', 'cash', 'cow', '.', 'that', 'option', 'closed', 'when', 'boom', 'turned', 'to', 'bust', '.', '#', 'according', 'to', 'this', 'analysis', ',', 'the', 'coalition', 'is', 'another', 'inter', '##re', '##gn', '##um', '.', 'mil', '##iba', '##nd', 'sees', 'david', 'cameron', 'engaged', 'in', 'a', 'futile', 'effort', 'to', 'breathe', 'life', 'into', 'the', 'corpse', 'of', 'an', 'expired', 'doctrine', '.', 'he', 'as', '##cr', '##ibe', '##s', 'to', 'himself', 'the', 'role', 'that', 'thatcher', 'once', 'played', ',', 'appearing', 'at', 'first', 'as', 'an', 'unlikely', 'leader', ',', 'dogg', '##edly', 'pursuing', 'ideas', 'that', 'threaten', 'to', 'disrupt', 'a', 'com', '##pl', '##ace', '##nt', 'orthodoxy', '.', 'just', 'as', 'the', 'iron', 'lady', 'once', 'anticipated', 'the', 'swing', 'of', 'the', 'pendulum', 'away', 'from', 'su', '##ff', '##ocating', 'stat', '##ism', ',', 'mil', '##iba', '##nd', 'believes', 'it', 'is', 'swinging', 'away', 'from', 'market', 'fe', '##tish', '##ism', '.', 'or', ',', 'rather', ',', 'he', 'thinks', 'it', 'has', 'the', 'potential', 'to', 'move', 'in', 'that', 'direction', '.', '\"', 'sometimes', 'you', 'have', 'to', 'push', 'the', 'pendulum', ',', '\"', 'mil', '##iba', '##nd', 'once', 'told', 'me', '.', '#', 'his', 'confidence', 'is', 'fuel', '##led', 'by', 'evidence', 'that', 'the', 'trickle', '-', 'down', 'mechanism', 'stopped', 'working', 'well', 'before', 'the', 'bubble', 'burst', '.', 'this', 'point', 'was', 'the', 'central', 'argument', 'in', 'his', 'conference', 'speech', 'this', 'year', '.', '\"', 'for', 'generations', 'in', 'britain', ',', 'when', 'the', 'economy', 'grew', 'along', 'the', 'way', 'that', 'vital', 'link', 'between', 'the', 'growing', 'wealth', 'of', 'the', 'country', 'and', 'your', 'family', 'finances', 'was', 'broken', ',', '\"', 'he', 'said', '.', '#', 'average', 'wages', 'began', 'st', '##ag', '##nat', '##ing', 'around', '2003', ',', 'even', 'as', 'the', 'economy', 'looked', 'bu', '##oya', '##nt', 'in', 'national', 'statistics', '.', 'for', 'many', 'households', ',', 'there', 'has', 'been', 'a', 'relative', 'decline', 'in', 'income', ',', 'which', 'was', 'masked', 'by', 'treasury', 'tax', 'credits', 'and', 'rising', 'personal', 'debt', '.', 'benefits', '(', 'a', 'substantial', 'portion', 'of', 'which', 'goes', 'to', 'people', 'in', 'work', ')', 'and', 'credit', 'cards', 'covered', 'up', 'for', 'the', 'systemic', 'failure', 'of', 'our', 'economic', 'model', '.', 'those', 'at', 'the', 'top', 'of', 'the', 'income', 'scale', 'were', 'spared', 'the', 'squeeze', 'as', 'the', 'proceeds', 'of', 'growth', 'flowed', 'upwards', 'to', 'a', 'narrow', 'wealthy', 'elite', '.', '#', 'according', 'to', 'the', 'resolution', 'foundation', ',', 'a', 'think', 'tank', 'that', 'has', 'heavily', 'influenced', 'mil', '##iba', '##nd', \"'\", 's', 'thinking', ',', 'the', 'richest', '1', 'per', 'cent', 'now', 'takes', 'home', '10', '##p', 'in', 'every', 'pound', 'of', 'income', 'earned', 'in', 'the', 'uk', '.', 'the', 'bottom', 'half', 'shares', '18', '##p', 'in', 'every', 'pound', '.', 'most', 'people', \"'\", 's', 'spending', 'power', 'has', 'been', 'eroded', 'by', 'inflation', '.', 'net', 'income', 'for', 'low', '-', 'to', '-', 'middle', '-', 'earning', 'households', 'has', 'fallen', 'by', 'an', 'average', 'of', '7', '.', '5', 'per', 'cent', 'in', ',', 'is', 'a', 'structural', 'mal', '##fu', '##nction', 'in', 'the', 'economy', ',', 'not', 'just', 'a', 'consequence', 'of', 'recession', '.', 'it', 'shows', 'little', 'sign', 'of', 'aba', '##ting', 'even', 'as', 'growth', 'returns', '.', 'conservatives', \"'\", 'celebrations', 'of', 'economic', 'recovery', 'risk', 'looking', 'like', 'the', 'triumph', '##al', 'march', 'of', 'a', 'class', 'that', 'was', 'ins', '##ulated', 'from', 'the', 'pri', '##vation', '##s', 'of', 'the', 'down', '##turn', '.', 'whereas', 'thatcher', \"'\", 's', 'success', 'was', 'achieved', 'by', 'tapping', 'into', 'the', 'aspirations', 'of', 'a', 'rising', 'middle', 'class', ',', 'mil', '##iba', '##nd', 'sees', 'the', 'future', 'of', 'politics', 'as', 'belonging', 'to', 'the', 'party', 'that', 'addresses', 'the', 'an', '##xie', '##ties', 'of', 'a', '\"', 'squeezed', 'middle', '\"', 'that', 'fears', 'downward', 'mobility', '.', 'for', 'most', 'of', 'the', 'postwar', 'era', ',', 'each', 'generation', 'has', 'presumed', 'that', 'it', 'will', 'do', 'better', 'than', 'the', 'last', '-', '-', 'its', 'children', 'will', 'be', 'better', 'housed', 'and', 'school', '##ed', 'and', 'have', 'better', 'jobs', '.', 'that', 'expectation', 'has', 'gone', '.', '#', 'if', 'that', 'is', 'the', 'diagnosis', ',', 'what', 'is', 'the', 'remedy', '?', 'so', 'far', ',', 'mil', '##iba', '##nd', \"'\", 's', 'programme', 'is', 'largely', 'emblem', '##atic', ',', 'although', 'he', 'has', 'been', 'more', 'candi', '##d', 'about', 'his', 'intentions', 'than', 'previous', 'opposition', 'leaders', 'at', 'equivalent', 'stages', 'in', 'their', 'bids', 'for', 'election', '.', 'the', 'agenda', 'can', 'be', 'divided', 'into', 'three', 'parts', '.', '#', 'responsible', 'capitalism', '#', 'a', 'party', 'conference', 'that', 'his', 'mission', 'was', 'to', 'restore', 'morality', 'to', 'business', '.', 'he', 'drew', 'a', 'distinction', 'between', '\"', 'predators', '\"', 'and', '\"', 'producers', '\"', ':', 'those', 'who', 'get', 'rich', 'by', 'exploitation', 'and', 'those', 'who', 'add', 'value', 'to', 'society', '.', 'that', 'rhetoric', 'was', 'abandoned', 'as', 'it', 'became', 'clear', 'that', 'mps', 'were', 'sq', '##ui', '##rmin', '##g', 'when', 'asked', 'to', 'flesh', 'out', 'the', 'line', 'with', 'concrete', 'examples', '.', 'yet', 'the', 'concept', 'survives', 'in', 'mil', '##iba', '##nd', \"'\", 's', 'attack', 'on', 'big', 'energy', 'companies', ',', 'his', 'pledge', '##s', 'to', 'crack', 'down', 'on', 'tax', 'avoidance', 'and', 'his', 'plans', 'for', 'more', 'regulation', 'of', 'pay', '##day', 'lend', '##ers', '.', 'he', 'believes', 'that', 'people', 'feel', 'powerless', 'before', 'mighty', 'corporations', '.', '#', 'this', 'also', 'expresses', 'a', 'structural', 'weakness', 'in', 'the', 'economy', '.', 'the', 'great', 'liberal', '##isation', 'programme', 'of', 'the', '1980s', 'and', '1990s', 'transferred', 'public', 'assets', 'to', 'private', 'hands', 'on', 'the', 'pre', '##sum', '##ption', 'that', 'competitive', 'market', 'forces', 'would', 'improve', 'performance', '.', 'that', 'process', ',', 'argue', 'the', 'mil', '##iba', '##ndi', '##tes', ',', 'was', 'only', 'half', '-', 'completed', '.', 'shareholders', 'got', 'their', 'divide', '##nds', 'but', 'customers', 'did', 'not', 'get', 'better', 'service', 'and', 'regulators', 'were', 'weak', '.', '#', 'as', 'a', 'result', 'of', 'the', 'thatcher', '##ite', 'settlement', ',', 'politicians', 'surrendered', 'control', 'over', 'things', 'that', 'people', 'dee', '##m', 'essential', 'in', 'their', 'daily', 'lives', '-', '-', 'trains', 'citizens', 'at', 'the', 'mercy', 'of', 'companies', 'that', 'enjoyed', 'near', '-', 'mono', '##polis', '##tic', 'positions', 'or', 'quasi', '##car', '##tel', '##s', 'in', 'broken', 'markets', '.', 'alongside', 'beef', '##ier', 'regulation', ',', '\"', 'responsible', 'capitalism', '\"', 'means', 'changing', 'the', 'way', 'companies', 'are', 'run', 'so', 'that', 'employees', 'are', 'better', 'treated', 'and', 'better', 'paid', '.', 'it', 'means', ',', 'for', 'example', ',', 'favour', '##ing', 'small', 'businesses', 'in', 'the', 'tax', 'system', 'and', 'offering', 'incentives', 'for', 'bosses', 'who', 'pay', 'the', '\"', 'living', 'wage', '\"', '.', '#', 'pre', '##dis', '##tri', '##bution', '#', 'the', 'offer', 'to', 'make', 'capitalism', 'more', 'ethical', 'was', 'der', '##ided', 'as', 'un', '##work', '##able', '.', 'mil', '##iba', '##nd', \"'\", 's', 'next', 'big', 'idea', 'was', 'rid', '##ic', '##uled', 'as', 'un', '##pro', '##no', '##unce', '##able', '.', 'the', 'term', '\"', 'pre', '##dis', '##tri', '##bution', '\"', 'was', 'coined', 'by', 'the', 'yale', 'professor', 'jacob', 'hacker', ',', 'who', 'defines', 'it', 'in', 'its', 'simplest', 'terms', 'as', '\"', 'making', 'markets', 'work', 'for', 'the', 'middle', 'class', '\"', '(', 'although', 'in', 'the', 'us', '\"', 'middle', 'class', '\"', 'is', 'a', 'broader', 'concept', 'that', 'includes', 'many', 'of', 'what', 'in', 'britain', 'would', 'be', 'thought', 'of', 'as', 'working', '-', 'class', 'households', ')', '.', '#', 'the', 'conventional', 'left', '-', 'wing', 'approach', 'to', 'correct', '##ing', 'inequality', 'has', 'been', 'the', 'real', '##lo', '##cation', 'of', 'resources', 'from', 'rich', 'to', 'poor', '.', 'this', 'is', 'problematic', 'in', 'various', 'ways', '.', 'first', ',', 'at', 'a', 'time', 'of', 'budget', 'scar', '##city', ',', 'the', 'resources', 'to', 'achieve', 'n', \"'\", 't', 'much', 'public', 'consent', 'for', 'a', 'system', 'in', 'which', 'the', 'state', 'con', '##fi', '##sca', '##tes', 'money', 'from', 'some', 'people', 'to', 'give', 'it', 'to', 'their', 'neighbours', ',', 'regardless', 'of', 'how', 'needy', 'they', 'are', '.', 'third', ',', 'it', 'deals', 'with', 'the', 'symptoms', ',', 'not', 'the', 'causes', '.', 'redistribution', 'alone', 'can', 'not', 'counter', '##act', 'the', 'forces', 'that', 'drive', 'inequality', 'in', 'a', 'liberal', '##ised', 'market', 'economy', '.', '#', 'the', '\"', 'pre', '##dis', '##tri', '##bution', '\"', 'approach', 'puts', 'the', 'emphasis', 'on', 'changing', 'the', 'structures', 'that', 'make', 'it', 'hard', 'for', 'people', 'without', 'privileged', 'backgrounds', 'to', 'get', 'ahead', '.', 'that', 'could', 'take', 'the', 'form', 'of', 'universal', 'child', '##care', ',', 'so', 'more', 'women', 'can', 'take', 'jobs', '.', 'it', 'means', 'rec', '##og', '##nis', '##ing', 'the', 'central', 'role', 'that', 'a', 'housing', 'shortage', 'plays', 'in', 'holding', 'back', 'many', 'people', \"'\", 's', 'life', 'chances', '.', '#', 'this', ',', 'say', 'the', 'mil', '##iba', '##ndi', '##tes', ',', 'is', 'about', 'much', 'more', 'than', 'spending', 'priorities', '.', 'it', 'requires', 'what', 'stewart', 'wood', 'has', 'called', '\"', 'a', 'supply', '##side', 'revolution', 'from', 'the', 'left', '\"', '.', 'it', 'is', 'a', 'counterpart', 'to', 'the', 'conservative', 'assertion', 'that', 'employee', 'protection', 'st', '##if', '##les', 'enterprise', 'and', 'must', 'be', 'stripped', 'away', 'for', 'the', 'good', 'of', 'economic', 'efficiency', '.', 'the', 'pre', '##dis', '##tri', '##bu', '##tative', 'reply', 'is', 'that', 'productivity', 'is', 'boosted', 'when', 'staff', 'feel', 'secure', ',', 'rewarded', 'and', '#', 'one', 'nation', '#', 'public', 'di', '##sa', '##ffe', '##ction', 'with', 'the', 'coalition', 'has', 'not', 'translated', 'into', 'automatic', 'support', 'for', 'labour', '.', 'much', 'of', 'it', 'has', 'been', 'channel', '##led', 'to', 'uk', '##ip', '.', 'that', 'is', 'partly', 'because', 'labour', 'is', 'tar', '##nished', 'as', 'just', 'another', 'face', 'of', 'a', 'disc', '##red', '##ited', 'political', 'establishment', '.', 'it', 'was', 'also', 'inevitable', 'that', 'a', 'crisis', 'in', 'liberal', 'global', '##isation', 'would', 'generate', 'a', 'backlash', 'in', 'favour', 'of', 'ins', '##ular', 'nationalism', '.', 'in', 'aus', '##ter', '##e', 'times', ',', 'people', 'become', 'more', 'jealous', 'in', 'preserving', 'the', 'privileges', 'they', 'have', '-', '-', 'fearing', 'competition', 'from', 'job', '-', 'seeking', 'immigrants', ';', 'res', '##enting', 'sharing', 'public', 'services', 'with', 'outsiders', '.', '#', 'in', 'mil', '##iba', '##nd', \"'\", 's', 'view', ',', 'the', 'conservatives', 'are', 'also', 'gaming', 'the', 'politics', 'of', 'fear', 'and', 'division', ',', 'blaming', 'foreign', '\"', 'benefit', 'tourists', '\"', 'for', 'pressure', 'on', 'the', 'nhs', ',', 'for', 'example', ',', 'and', 'pretending', 'that', 'there', 'is', 'a', 'neat', 'division', 'between', 'indus', '##tri', '##ous', 'workers', 'whose', 'taxes', 'fund', 'the', 'welfare', 'state', 'and', 'idle', 'lay', '##ab', '##outs', 'who', 'milk', 'it', '.', 'rec', '##og', '##nis', '##ing', 'that', 'he', 'can', 'not', 'win', 'an', 'arms', 'race', 'in', 'that', 'kind', 'of', 'rhetoric', ',', 'the', 'labour', 'leader', 'wants', 'to', 'present', 'himself', 'as', 'the', 'author', 'of', 'a', 'different', ',', 'inclusive', 'politics', '.', '#', 'the', '\"', 'one', 'nation', '\"', 'rhetoric', 'is', 'an', 'attempt', 'the', 'welfare', 'state', 'as', 'part', 'of', 'a', 'national', 'story', '.', 'the', 'institutions', 'founded', 'under', 'clement', 'at', '##tle', '##e', ',', 'mil', '##iba', '##nd', 'argues', ',', 'are', 'cher', '##ished', 'fixtures', 'of', 'british', 'identity', '.', 'he', 'has', 'been', 'influenced', 'in', 'this', 'respect', 'by', 'the', 'young', 'australian', 'writer', 'tim', 'so', '##ut', '##ph', '##om', '##mas', '##ane', ',', 'whose', 'work', 'was', 'brought', 'to', 'the', 'labour', 'leader', \"'\", 's', 'attention', 'by', 'jon', 'cr', '##ud', '##das', ',', 'head', 'of', 'the', 'party', \"'\", 's', 'policy', 'review', '.', '#', 'so', '##ut', '##ph', '##om', '##mas', '##ane', ',', 'born', 'to', 'a', 'refugee', 'family', ',', 'believes', 'the', 'left', 'must', '\"', 'reclaim', 'patriot', '##ism', '\"', ',', 'resisting', 'liberal', 'sq', '##ue', '##ami', '##sh', '##ness', 'about', 'even', 'discussing', 'identity', 'politics', '.', 'too', 'often', ',', 'that', 'has', 'surrendered', 'the', 'flag', 'to', 'b', '##lim', '##pis', '##h', 'reaction', 'on', 'the', 'right', '.', '#', 'in', 'policy', 'terms', ',', '\"', 'one', 'nation', '\"', 'translates', 'as', 'a', 'determination', 'to', 'address', 'those', 'issues', 'of', 'social', 'division', 'on', 'which', 'labour', 'is', 'least', 'trusted', 'by', 'voters', '-', '-', 'immigration', 'and', 'welfare', '-', '-', 'in', 'a', 'manner', 'consistent', 'with', 'the', 'party', \"'\", 's', 'self', '-', 'image', 'as', 'a', 'champion', 'of', 'fairness', ',', 'defender', 'of', 'the', 'vulnerable', 'and', 'bastion', 'against', 'prejudice', '.', 'so', ',', 'for', 'example', ',', 'the', 'hostility', 'to', 'welfare', 'spending', 'must', 'be', 'met', 'by', 'asserting', 'that', 'job', 'creation', 'and', 'higher', 'wages', 'reduce', 'the', 'benefits', 'bill', 'more', 're', '##lia', '##bly', 'than', 'should', 'be', 'neutral', '##ised', 'by', 'building', 'homes', ',', 'not', 'by', 'blaming', 'foreigners', 'for', 'jumping', 'council', 'house', 'queue', '##s', '.', 'to', 'sc', '##ept', '##ics', ',', 'that', 'is', 'more', 'eva', '##sion', 'than', 'confrontation', 'of', 'the', 'issues', '.', 'for', 'the', 'mil', '##iba', '##ndi', '##tes', ',', 'it', 'is', 'acknowledging', 'public', 'anxiety', 'without', 'ind', '##ul', '##ging', 'big', '##ot', '##ry', '.', '#', 'the', 'ambition', 'not', 'to', 'fight', 'on', 'terms', 'dictated', 'by', 'the', 'right', 'comes', 'back', 'to', 'mil', '##iba', '##nd', \"'\", 's', 'critique', 'of', 'new', 'labour', ',', 'specifically', 'its', 'habit', 'of', 'taking', 'internal', 'opposition', 'as', 'proof', 'of', 'modern', '##ising', 'vi', '##go', '##ur', '.', 'even', 'self', '-', 'styled', 'blair', '##ites', 'con', '##cede', 'that', 'there', 'was', 'a', 'tendency', 'to', 'see', 'the', 'approval', 'of', 'the', 'tory', 'press', 'and', 'the', 'outrage', 'of', 'the', 'left', 'as', 'badges', 'of', 'honour', '.', 'mil', '##iba', '##nd', 'rejected', 'that', 'approach', 'in', 'the', '2010', 'leadership', 'contest', 'when', 'he', 'declared', ',', '\"', 'as', 'your', 'leader', ',', 'i', 'will', 'never', 'leave', 'this', 'party', 'behind', '.', '\"', '#', 'his', 'determination', 'to', 'honour', 'that', 'pledge', 'is', 'beyond', 'doubt', '.', 'to', 'his', 'critics', ',', 'that', 'has', 'led', 'to', 'craven', 'compromise', 'with', 'militant', 'leftist', '##s', 'whose', 'instincts', 'drive', 'labour', 'into', 'the', 'electoral', 'wilderness', '.', 'his', 'friends', 'point', 'out', 'that', 'a', 'leader', 'who', 'is', 'not', 'suspected', 'of', 'being', 'ashamed', 'of', 'his', 'party', 'ends', 'up', 'with', 'much', 'more', 'room', 'for', 'man', '##oe', '##u', '##vre', 'over', 'who', 'in', 'opposition', 'tried', 'to', '\"', 'deco', '##nta', '##minate', '\"', 'his', 'party', \"'\", 's', 'toxic', 'brand', 'with', 'posture', '##s', 'that', 'outraged', 'the', 'tory', 'right', '.', ')', '#', 'for', 'mil', '##iba', '##nd', ',', 'being', 'trusted', 'by', 'the', 'rank', 'and', 'file', 'as', 'an', 'em', '##bo', '##diment', 'of', 'traditional', 'labour', 'values', 'is', 'about', 'more', 'than', 'job', 'security', '.', 'one', 'part', 'of', 'his', 'agenda', 'that', 'gets', 'little', 'attention', 'but', 'that', 'aide', '##s', 'insist', 'is', 'central', 'to', 'the', 'project', 'is', 'the', 'transformation', 'of', 'the', 'party', 'from', 'a', 'rusty', 'bureau', '##cratic', 'app', '##arat', 'to', 'a', 'network', 'of', 'grass', '-', 'roots', 'activists', '.', 'that', 'work', 'is', 'led', 'by', 'ar', '##nie', 'graf', ',', 'the', '69', '-', 'year', '-', 'old', 'american', 'pioneer', 'of', '\"', 'community', 'organising', '\"', ',', 'about', 'which', 'mil', '##iba', '##nd', 'is', 'evangelical', '.', 'the', 'principle', 'is', 'to', 'win', 'political', 'support', 'street', 'by', 'street', ',', 'focusing', 'on', 'hyper', '-', 'local', 'issues', 'and', 'engaging', 'people', 'who', 'would', 'otherwise', 'never', 'go', 'near', 'a', 'constituency', 'party', 'meeting', '.', '#', 'graf', \"'\", 's', 'approach', 'is', 'intended', 'as', 'a', 'remedy', 'to', 'ap', '##athy', '.', 'mil', '##iba', '##nd', 'has', 'described', 'voters', \"'\", 'pe', '##ssi', '##mist', '##ic', 'surrender', 'to', 'the', 'status', 'quo', 'as', 'a', 'greater', 'threat', 'to', 'his', 'prospects', 'of', 'winning', 'an', 'election', 'than', 'active', 'support', 'for', 'the', 'conservative', 'party', '.', 'earlier', 'this', 'year', ',', 'the', 'labour', 'leader', 'told', 'me', ':', '\"', 'the', 'right', 'wins', 'a', 'way', 'out', 'of', 'their', 'problems', '.', 'we', 'win', 'when', 'we', 'convince', 'people', 'that', 'there', 'is', 'a', 'way', 'and', 'that', 'we', 'can', 'set', 'britain', 'in', 'the', 'right', 'direction', '.', '\"', '#', 'mil', '##iba', '##nd', \"'\", 's', 'critics', 'can', 'think', 'of', 'many', 'greater', 'obstacles', 'to', 'a', 'labour', 'victory', '.', 'chief', 'among', 'them', 'is', 'an', 'economic', 'revival', 'that', 'might', 'allow', 'the', 'tori', '##es', 'to', 'claim', 'vin', '##dication', 'for', 'their', 'aus', '##ter', '##ity', 'programme', '.', 'if', 'wages', 'begin', 'to', 'recover', 'in', 'real', 'terms', 'along', 'with', 'national', 'gdp', ',', 'a', 'central', 'plank', 'of', 'the', 'mil', '##iba', '##nd', 'platform', 'will', 'cr', '##umble', 'beneath', 'him', '.', 'his', 'advisers', 'are', 'confident', 'that', 'wo', 'n', \"'\", 't', 'happen', '.', 'tori', '##es', 'in', 'the', 'treasury', 'predict', 'it', 'will', '.', 'ed', 'balls', \"'\", 's', 'commitment', 'to', 'the', 'intellectual', 'ten', '##ets', 'of', 'mil', '##iba', '##ndi', '##sm', 'is', 'notorious', '##ly', 'doubtful', ',', 'although', 'his', 'determination', 'to', 'get', 'labour', 'elected', 'with', 'mil', '##iba', '##nd', 'as', 'leader', 'is', 'un', '##quest', '##ioned', '.', 'as', 'a', 'pair', ',', 'the', 'two', 'eds', 'are', 'trusted', 'less', 'than', 'george', 'osborne', 'and', 'cameron', 'as', 'steward', '##s', 'of', 'the', 'economy', '.', '#', 'there', 'are', 'still', 'gaps', 'in', 'mil', '##iba', '##nd', \"'\", 's', 'programme', '.', 'his', 'account', 'of', 'how', 'labour', 'would', 'champion', 'hard', '-', 'pressed', 'consumers', 'against', 'wicked', 'corporate', 'interests', 'is', 'not', 'matched', 'by', 'a', 'determination', 'to', 'reform', 'the', 'public', 'sector', '.', 'of', 'the', 'state', '.', 'his', 'vision', 'of', 'party', 'reform', 'risks', 'being', 'lost', 'in', 'back', '-', 'room', 'ha', '##gg', '##ling', 'with', 'the', 'trade', 'union', 'leaders', 'who', 'finance', 'the', 'whole', 'labour', 'show', '.', 'his', 'personal', 'ratings', ',', 'while', 'improving', ',', 'are', 'still', 'below', 'the', 'levels', 'that', 'usually', 'indicate', 'momentum', 'towards', 'downing', 'street', '.', '#', 'it', 'has', 'always', 'been', 'easy', 'to', 'list', 'the', 'ways', 'in', 'which', 'politicians', 'might', 'fail', 'but', 'it', 'is', 'getting', 'harder', 'to', 'write', 'mil', '##iba', '##nd', 'off', '.', 'he', 'has', 'displayed', 'a', 'ten', '##ac', '##ity', 'that', 'di', '##sor', '##ient', '##ates', 'his', 'enemies', '.', 'conservative', 'attacks', 'are', 'contradictory', '.', 'he', 'is', 'weak', 'yet', 'dangerous', ';', 'ridiculous', 'yet', 'sinister', '.', 'the', 'latest', 'tory', 'line', 'is', 'that', 'he', 'is', 'a', 'con', 'artist', ',', 'offering', 'fl', '##im', '##sy', 'pop', '##uli', '##sm', 'in', 'the', 'face', 'of', 'complex', 'problems', '.', 'so', 'they', 'recognise', 'at', 'least', 'that', 'the', 'left', 'can', 'be', 'popular', '.', '#', 'the', 'conservatives', 'should', 'consider', 'also', 'the', 'possibility', 'that', 'what', 'mil', '##iba', '##nd', 'says', 'expresses', 'something', 'more', 'substantial', 'than', 'a', 'retreat', 'to', 'the', 'pre', '-', 'blair', 'era', 'of', 'labour', 'politics', '.', 'mil', '##iba', '##ndi', '##sm', 'is', 'not', 'a', 'complete', 'doctrine', 'but', 'it', 'is', 'much', 'more', 'than', 'nostalgia', 'for', 'the', 'kind', 'of', 'high', '-', 'taxi', '##ng', ',', 'spend', '##th', '##rift', 'social', 'democracy', 'that', 'its', 'opponents', 'want', 'it', 'to', 'be', '.', 'it', 'does', 'not', 'thatcher', '##ism', '.', 'the', 'times', 'pose', 'a', 'different', 'challenge', '.', 'blair', 'was', 'confronting', 'a', 'mis', '##mat', '##ch', 'between', 'a', 'party', 'that', 'did', 'n', \"'\", 't', 'like', 'the', 'way', 'the', 'economy', 'worked', 'and', 'a', 'public', 'that', 'largely', 'did', '.', 'mil', '##iba', '##nd', 'is', 'reaching', 'out', 'to', 'a', 'public', 'that', 'does', 'n', \"'\", 't', 'like', 'the', 'way', 'the', 'economy', 'works', 'but', 'doubts', 'the', 'capacity', 'of', 'any', 'party', 'to', 'fix', 'it', '.', '#', 'if', 'it', 'pays', 'off', ',', 'the', 'reward', 'is', 'a', 'mandate', 'to', 'res', '##ha', '##pe', 'british', 'politics', 'on', 'terms', 'chosen', 'by', 'the', 'left', 'in', 'a', 'way', 'more', 'profound', 'than', 'blair', 'and', 'brown', 'did', '.', 'it', 'would', 'ref', '##ute', 'the', 'idea', '-', '-', 'the', 'new', 'labour', 'ne', '##uro', '##sis', '-', '-', 'that', 'britain', 'is', 'innate', '##ly', 'conservative', 'and', 'that', 'e', '##gal', '##itarian', '##ism', 'must', 'be', 'smug', '##gled', 'past', 'the', 'electorate', '.', '#', 'no', 'one', 'claims', 'that', 'mil', '##iba', '##nd', 'hides', 'his', 'agenda', '.', 'no', 'one', 'should', 'have', 'been', 'surprised', 'that', 'he', 'called', 'himself', 'a', 'socialist', 'one', 'afternoon', 'in', 'brighton', '.', 'he', 'has', 'said', 'it', 'all', 'along', '.', 'days', 'after', 'winning', 'the', 'labour', 'leadership', ',', 'he', 'told', 'the', 'bbc', 'of', 'his', 'plan', ':', '\"', 'it', 'is', 'my', 'form', 'of', 'socialism', 'which', 'is', 'a', 'more', 'fair', ',', 'more', 'just', ',', 'more', 'equal', 'society', '.', 'and', 'that', 'is', 'the', '.', '\"', '#', 'he', 'has', 'been', 'true', 'to', 'his', 'word', '.', 'he', 'has', 'taken', 'labour', 'on', 'the', 'journey', 'that', 'he', 'outlined', 'three', 'years', 'ago', 'and', 'brought', 'it', 'within', 'sight', 'of', 'power', '.', 'ed', 'mil', '##iba', '##nd', \"'\", 's', 'tough', '##est', 'challenge', 'now', 'is', 'to', 'turn', 'his', 'ideas', 'into', 'a', 'campaign', 'that', 'persuade', '##s', 'enough', 'of', 'the', 'country', 'to', 'abandon', 'its', 'present', 'course', 'and', 'follow', 'him', 'further', 'down', 'that', 'same', 'path', '.', '#', '\"', 'basically', ',', 'the', 'bitch', 'did', 'n', \"'\", 't', 'text', 'me', 'back', 'so', 'i', \"'\", 'm', 'blank', '##in', \"'\", 'her', '\"', '#', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj9QQGUcScig",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a2448b4c-8e77-4dca-dfbb-9d4cb566b36c",
        "id": "FZRzxEcsScij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(period4_train), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "488 488 488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hDzBTPE8Scil",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.05)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wjlwp9rFScin"
      },
      "source": [
        "###### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MXyPYKfoScin",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f4123911-3a53-40a2-d61d-d594dcfa6837",
        "id": "CZ5JPKWfScip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Using Sigmoid Activation\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='sigmoid'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "23ce5b55-a115-40a6-c9a8-47b426af4306",
        "id": "EJjlmlPtScis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,\n",
        "                              validation_data=(validation_inputs, validation_labels),\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 463 samples, validate on 25 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "463/463 [==============================] - 4s 9ms/step - loss: 0.6388 - acc: 0.7927 - val_loss: 0.5171 - val_acc: 0.8400\n",
            "Epoch 2/10\n",
            "463/463 [==============================] - 3s 7ms/step - loss: 0.4991 - acc: 0.8164 - val_loss: 0.4698 - val_acc: 0.8400\n",
            "Epoch 3/10\n",
            "463/463 [==============================] - 3s 6ms/step - loss: 0.4686 - acc: 0.8164 - val_loss: 0.4395 - val_acc: 0.8400\n",
            "Epoch 4/10\n",
            "463/463 [==============================] - 3s 6ms/step - loss: 0.4556 - acc: 0.8164 - val_loss: 0.4443 - val_acc: 0.8400\n",
            "Epoch 5/10\n",
            "463/463 [==============================] - 3s 7ms/step - loss: 0.3969 - acc: 0.8164 - val_loss: 0.4423 - val_acc: 0.8400\n",
            "Epoch 6/10\n",
            "463/463 [==============================] - 3s 6ms/step - loss: 0.2342 - acc: 0.8747 - val_loss: 0.6707 - val_acc: 0.8400\n",
            "Epoch 7/10\n",
            "463/463 [==============================] - 3s 6ms/step - loss: 0.1743 - acc: 0.9849 - val_loss: 0.6746 - val_acc: 0.8400\n",
            "Epoch 8/10\n",
            "463/463 [==============================] - 3s 6ms/step - loss: 0.0737 - acc: 0.9957 - val_loss: 0.6077 - val_acc: 0.8000\n",
            "Epoch 9/10\n",
            "463/463 [==============================] - 3s 7ms/step - loss: 0.0252 - acc: 0.9978 - val_loss: 0.7409 - val_acc: 0.8000\n",
            "Epoch 10/10\n",
            "463/463 [==============================] - 3s 6ms/step - loss: 0.0119 - acc: 0.9978 - val_loss: 0.8462 - val_acc: 0.7200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f180a10e-738e-4708-e424-d86cec9a601f",
        "id": "C_DTUoYdSciu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = period4_test.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = period4_test.label.values\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', '!', 'gwen', '-', 'if', '##ill', '#', 'president', 'obama', 'addresses', 'the', 'nation', 'tonight', 'in', 'his', 'sixth', 'state', 'of', 'the', 'union', ',', 'and', 'for', 'the', 'first', 'time', 'before', 'a', 'house', 'and', 'senate', 'under', 'republican', 'control', '.', 'good', 'evening', '.', 'i', \"'\", 'm', 'gwen', 'if', '##ill', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'i', \"'\", 'm', 'judy', 'wood', '##ruff', '.', 'we', 'bring', 'you', 'special', 'coverage', 'from', 'the', 'u', '.', 's', '.', 'capitol', 'and', 'analysis', 'from', 'mark', 'shields', 'and', 'david', 'brooks', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'plus', ':', 'islamic', 'state', 'militants', 'threaten', 'to', 'kill', 'two', 'japanese', 'hostages', 'unless', 'paid', 'a', '$', '200', 'million', 'ransom', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'a', 'california', 'public', 'school', 'shifts', 'its', 'focus', 'to', 'life', 'lessons', ',', 'preparing', 'students', 'to', 'go', 'on', 'to', 'college', 'and', 'careers', 'in', 'medicine', '.', '!', 'preston', '-', 'thomas', ',', '-', 'pr', '#', 'we', 'have', 'kids', 'that', 'go', 'to', 'every', 'major', 'hospital', 'in', 'oakland', ',', 'and', 'they', 'are', 'starting', 'to', 'take', 'some', 'of', 'the', 'real', '-', 'world', 'practical', 'skills', 'that', 'they', 'are', 'learning', 'on', 'the', 'internship', '##s', ',', 'but', 'also', 'the', 'things', 'that', 'we', \"'\", 're', 'talking', 'about', 'in', 'class', ',', 'and', 'they', 'are', 'starting', 'to', 'synth', '##es', '##ize', 'that', 'into', 'the', 'vision', 'that', 'they', 'see', 'for', 'themselves', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'those', 'are', 'some', 'of', 'the', 'stories', 'we', \"'\", 're', 'covering', 'on', 'tonight', \"'\", 's', '\"', 'pbs', 'news', '##ho', '##ur', '.', '\"', '(', 'break', ')', '!', 'judy', '-', 'wood', '##ruff', '#', 'congress', 'and', 'the', 'country', ',', 'laying', 'out', 'his', 'view', 'of', 'the', 'state', 'of', 'the', 'union', 'at', 'the', 'start', 'of', 'a', 'new', 'year', '.', 'he', 'spent', 'this', 'day', 'putting', 'finishing', 'touches', 'on', 'many', 'of', 'the', 'proposals', 'he', \"'\", 's', 'already', 'made', 'public', ',', 'while', 'republicans', 'read', '##ied', 'their', 'own', 'arguments', '.', 'we', 'will', 'hear', 'from', 'both', 'sides', 'after', 'our', 'summary', 'of', 'the', 'day', \"'\", 's', 'other', 'news', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'and', 'that', 'summary', 'begins', 'with', 'new', 'signs', 'that', 'bi', '##partisan', '##ship', 'could', 'yet', 'be', 'elusive', ',', 'two', 'new', 'presidential', 'veto', 'threats', 'aimed', 'at', 'a', 'pair', 'of', 'republican', '-', 'sponsored', 'bills', '.', 'one', 'would', 'ban', 'abortion', '##s', 'after', 'the', 'first', '20', 'weeks', 'of', 'pregnancy', '.', 'and', 'the', 'other', 'would', 'mandate', 'decisions', 'on', 'oil', 'and', 'gas', 'pipeline', '##s', 'within', '12', 'months', 'of', 'being', 'proposed', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'an', 'oil', 'pipeline', 'break', 'in', 'montana', 'has', 'forced', 'the', 'town', 'of', 'glen', '##di', '##ve', 'to', 'start', 'truck', '##ing', 'in', 'bottled', 'water', '.', 'about', '50', ',', '000', 'gallons', 'of', 'crude', 'oil', 'spilled', 'into', 'the', 'yellowstone', 'river', 'on', 'saturday', '.', 'the', 'town', 'draws', 'its', 'water', 'from', 'the', 'river', ',', 'and', 'tests', 'have', 'confirmed', 'hazardous', 'levels', 'of', 'benz', '##ene', ',', 'a', 'cancer', '-', 'causing', 'component', 'in', 'oil', '.', 'it', \"'\", 's', 'unclear', 'how', 'long', 'the', 'water', 'will', 'be', 'foul', '##ed', '.', 'amid', 'a', 'heightened', 'alert', 'since', 'the', 'paris', 'attacks', '.', 'there', 'was', 'no', 'immediate', 'indication', 'that', 'the', 'five', 'had', 'any', 'connection', 'to', 'terrorism', '.', 'instead', ',', 'officials', 'in', 'two', 'towns', 'in', 'southern', 'france', 'said', 'they', 'were', 'tied', 'to', 'other', 'crimes', '.', 'a', 'cache', 'of', 'explosives', 'was', 'found', 'as', 'well', '.', '!', 'gilles', '-', 'soul', '##ie', ',', '-', 'dir', '#', 'what', 'i', 'can', 'tell', 'you', 'is', 'that', 'we', 'have', 'found', 'during', 'a', 'raid', 'at', 'one', 'person', \"'\", 's', 'place', 'some', 'explosive', 'material', ',', 'extremely', 'dangerous', '.', 'for', 'the', 'moment', ',', 'with', 'the', 'investigations', 'we', 'carried', 'out', ',', 'we', 'have', 'not', 'determined', 'if', 'they', 'have', 'a', 'current', 'bombing', 'project', '.', 'we', 'are', 'talking', 'about', 'charges', 'of', 'fabrication', 'and', 'possession', 'of', 'explosive', 'material', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'also', 'today', ',', 'four', 'men', 'suspected', 'of', 'having', 'links', 'to', 'one', 'of', 'the', 'paris', 'gunmen', ',', 'am', '##ed', '##y', 'co', '##uli', '##bal', '##y', ',', 'were', 'charged', 'in', 'paris', '.', 'co', '##uli', '##bal', '##y', 'died', 'earlier', 'this', 'month', 'in', 'a', 'shoot', '-', 'out', 'with', 'french', 'police', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'jury', 'selection', 'has', 'begun', 'in', 'new', 'york', 'for', 'a', 'saudi', 'man', 'linked', 'to', 'the', '1998', 'u', '.', 's', '.', 'embassy', 'bombings', 'in', 'africa', '.', 'khalid', 'al', '-', 'fa', '##w', '##wa', '##z', 'was', 'extra', '##dit', '##ed', 'from', 'britain', 'in', '2012', '.', 'he', \"'\", 's', 'accused', 'of', 'con', '##sp', '##iring', 'with', 'al', '-', 'q', '##aid', '##a', 'in', 'planning', 'the', 'guilty', '.', 'a', 'third', 'died', 'before', 'he', 'could', 'be', 'brought', 'to', 'trial', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'shi', '##ite', 'rebels', 'in', 'yemen', 'seized', 'the', 'presidential', 'palace', 'today', ',', 'after', 'taking', 'over', 'state', 'media', '.', 'they', 'also', 'shell', '##ed', 'the', 'home', 'of', 'the', 'u', '.', 's', '.', '-', 'backed', 'president', 'abd', '-', 'ra', '##bb', '##u', 'mans', '##our', 'had', '##i', ',', 'but', 'he', 'was', 'un', '##hur', '##t', '.', 'the', 'rebel', 'leader', 'demanded', 'that', 'had', '##i', 'quickly', 'implement', 'a', 'peace', 'accord', 'broker', '##ed', 'by', 'the', 'united', 'nations', '.', 'but', 'a', 'spokesman', 'for', 'u', '.', 'n', '.', 'secretary', '-', 'general', 'ban', 'ki', '-', 'moon', 'condemned', 'the', 'violence', '.', '!', 'stephane', '-', 'du', '##jar', '##ric', ',', '#', 'the', 'secretary', '-', 'general', 'is', 'gravely', 'concerned', 'about', 'the', 'deteriorating', 'situation', 'in', 'yemen', '.', 'the', 'secretary', '-', 'general', 'calls', 'on', 'all', 'sides', 'to', 'immediately', 'cease', 'all', 'hostilities', ',', 'exercise', 'maximum', 'restraint', ',', 'and', 'take', 'the', 'necessary', 'steps', 'to', 'restore', 'full', 'authority', 'to', 'the', 'legitimate', 'government', 'institutions', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'the', 'u', '.', 'n', '.', 'security', 'council', 'held', 'an', 'emergency', 'meeting', 'this', 'afternoon', 'and', 'issued', 'a', 'statement', ',', 'saying', 'president', 'had', '##i', 'is', '-', '-', 'quote', '-', '-', '\"', 'the', 'legitimate', 'authority', '\"', 'in', 'yemen', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'investigators', 'in', 'indonesia', 'have', 'concluded', 'an', 'air', '##asia', 'plane', 'was', 'climbing', 'much', 'too', 'fast', 'before', 'it', 'crashed', 'last', 'month', '.', 'the', 'transport', 'minister', 'said', 'today', 'the', 'jet', 'was', 'rising', 'at', '6', ',', '000', 'feet', 'a', 'minute', '.', 'it', 'could', 'have', 'caused', 'the', 'plane', 'to', 'stall', '.', 'the', 'pilots', 'had', 'asked', 'to', 'climb', 'to', 'avoid', 'a', 'storm', ',', 'but', 'ground', 'controllers', 'denied', 'permission', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'in', 'economic', 'news', ',', 'the', 'international', 'monetary', 'fund', 'lowered', 'its', 'forecast', 'of', 'global', 'growth', 'for', 'the', 'next', 'two', 'years', '.', 'and', 'china', 'reported', 'its', 'growth', 'in', '2014', 'was', 'the', 'slow', '##est', 'in', 'nearly', 'a', 'quarter', '-', 'century', ',', 'at', '7', '.', '4', 'percent', '.', 'the', 'global', 'news', 'held', 'wall', 'street', 'mostly', 'in', 'check', '.', 'the', 'dow', 'jones', 'industrial', 'average', 'gained', 'just', 'three', 'points', 'to', 'close', 'at', '1751', '##5', '.', 'the', 'nas', '##da', '##q', 'rose', '20', 'points', 'to', 'close', 'near', '46', '##55', '.', 'and', 'the', 's', '&', 'amp', ';', 'p', '500', 'added', 'three', 'to', 'finish', 'at', '202', '##2', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'there', \"'\", 's', 'good', 'news', 'today', 'about', 'tigers', '.', 'their', 'numbers', 'in', 'the', 'wild', 'in', 'india', 'have', 'increased', 'nearly', 'a', 'third', 'since', '2010', '.', 'a', 'government', 'report', 'today', 'cited', 'images', 'collected', 'at', 'nearly', '10', ',', '000', 'camera', 'traps', '.', 'india', 'is', 'home', 'to', '2', ',', '200', 'tigers', ',', '70', 'percent', 'of', 'the', 'world', \"'\", 's', 'population', '.', 'the', 'country', 'has', 'pledged', 'to', 'construct', 'new', 'preserves', 'to', 'keep', 'the', 'big', 'cats', 'in', 'their', 'natural', 'habitat', '.', '!', 'prakash', '-', 'java', '##dek', '##ar', ',', '#', 'we', 'have', 'pro', '##active', '##ly', 'decided', 'that', 'we', 'will', 'so', 'that', 'the', 'animals', 'will', 'live', 'happily', 'in', 'forests', 'and', 'people', 'will', 'live', 'happily', 'along', 'the', 'side', 'of', 'the', 'forest', 'also', ',', 'and', 'no', 'one', 'will', 'int', '##rud', '##e', 'upon', 'anybody', \"'\", 's', 'territory', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'the', 'world', 'wildlife', 'fund', 'estimates', 'the', 'world', 'lost', '97', 'percent', 'of', 'its', 'tigers', 'in', 'the', 'last', 'century', '.', 'still', 'to', 'come', 'on', 'the', '\"', 'news', '##ho', '##ur', '\"', ':', 'from', 'the', 'white', 'house', 'to', 'capitol', 'hill', ',', 'complete', 'coverage', 'of', 'the', 'president', \"'\", 's', 'state', 'of', 'the', 'union', 'address', ';', 'then', ',', 'islamic', 'state', 'militants', 'make', 'ransom', 'demands', 'for', 'two', 'japanese', 'hostages', ';', 'a', 'divided', 'supreme', 'court', 'weighs', 'in', 'on', 'whether', 'judicial', 'candidates', 'should', 'be', 'able', 'to', 'personally', 'raise', 'campaign', 'cash', ';', 'jury', 'selection', 'begins', 'in', 'the', 'trial', 'of', 'james', 'holmes', ',', 'the', 'man', 'charged', 'with', 'the', '2012', 'mass', 'shooting', 'at', 'a', 'colorado', 'movie', 'theater', ';', 'and', 'connecting', 'the', 'classroom', 'to', 'promising', 'careers', 'in', 'medicine', 'and', 'bio', '##tech', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'it', 'is', 'one', 'of', 'the', 'few', 'explicit', 'tasks', 'the', 'u', '.', 's', '.', 'constitution', 'requires', 'of', 'each', 'president', ',', 'regularly', 'giving', 'congress', 'a', 'sense', 'of', 'the', 'state', 'of', 'the', 'union', '.', 'in', 'the', 'hours', 'before', 'his', 'speech', 'tonight', ',', 'president', 'obama', 'sat', 'down', 'with', 'a', 'few', 'republican', '-', 'controlled', 'capitol', 'hill', ',', 'newly', 'mint', '##ed', 'senate', 'leader', 'mitch', 'mcconnell', 'called', 'for', 'bi', '##partisan', 'proposals', '.', '!', 'sen', '-', 'mitch', '-', 'mcc', '##onne', '##l', '#', 'what', 'i', 'hope', 'to', 'hear', 'from', 'the', 'president', 'tonight', 'is', 'an', 'emphasis', 'on', 'things', 'that', 'we', 'can', 'agree', 'on', ',', 'things', 'that', 'give', 'us', 'a', 'chance', 'to', 'actually', 'advance', 'the', 'agenda', 'of', 'the', 'american', 'people', '.', 'any', 'president', 'in', 'this', 'situation', 'has', 'a', 'choice', '.', 'he', 'can', 'sort', 'of', 'act', 'like', 'he', \"'\", 's', 'still', 'running', 'for', 'office', ',', 'or', 'he', 'can', 'focus', 'on', 'the', 'things', 'that', 'we', 'have', 'a', 'chance', 'to', 'reach', 'an', 'agreement', 'on', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'political', 'editor', 'lisa', 'des', '##jar', '##din', '##s', 'is', 'at', 'the', 'capitol', ',', 'where', 'law', '##makers', 'are', 'preparing', 'for', 'the', 'president', \"'\", 's', 'address', 'later', 'tonight', '.', 'lisa', ',', 'you', \"'\", 're', 'up', 'on', 'the', 'hill', '.', 'you', 'have', 'been', 'talking', 'to', 'people', '.', 'we', 'just', 'heard', 'mitch', 'mcconnell', 'say', 'he', 'hoped', 'the', 'president', 'would', 'step', 'up', ',', 'and', 'we', 'also', 'saw', 'the', 'president', 'issue', 'two', 'veto', 'threats', 'today', '.', 'what', 'is', 'the', 'mood', 'up', 'there', '?', '!', 'lisa', '-', 'des', '##jar', '##din', '##s', '#', 'i', 'have', 'to', 'say', ',', 'gwen', ',', 'there', \"'\", 's', 'a', 'pal', '##pa', '##ble', 'lack', 'of', 'excitement', 'for', 'such', 'an', 'important', 'occasion', '.', 'i', 'myself', 'love', 'these', '.', 'this', 'is', 'but', 'the', 'truth', 'is', ',', 'here', 'at', 'the', 'capitol', ',', 'there', \"'\", 's', 'a', 'sense', 'that', 'there', \"'\", 's', 'not', 'only', 'grid', '##lock', ',', 'but', 'incredible', 'divide', '.', 'and', 'from', 'both', 'democrats', 'and', 'republicans', 'you', 'talk', 'to', 'today', ',', 'both', 'on', 'and', 'off', 'the', 'record', ',', 'they', 'all', 'say', 'no', 'matter', 'what', 'the', 'president', 'proposes', ',', 'it', 'may', 'be', 'interesting', ',', 'you', 'may', 'agree', 'or', 'disagree', '.', 'they', 'do', 'n', \"'\", 't', 'think', 'his', 'proposals', 'are', 'going', 'to', 'go', 'very', 'far', '.', 'and', 'that', \"'\", 's', 'led', 'to', 'kind', 'of', 'a', 'sense', 'of', 'very', 'low', 'expectations', 'and', 'low', 'excitement', 'tonight', ',', 'i', 'have', 'to', 'say', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'well', ',', 'let', \"'\", 's', 'talk', 'about', 'the', 'expectations', 'of', 'the', 'white', 'house', ',', 'where', 'they', \"'\", 're', 'being', 'determined', '##ly', 'upbeat', 'about', 'this', ',', 'talking', 'about', 'closing', 'the', 'gap', 'for', 'the', 'middle', 'class', '.', 'is', 'that', 'res', '##ona', '##ting', 'on', 'capitol', 'hill', '?', '!', 'lisa', '-', 'des', '##jar', '##din', '##s', '#', 'absolutely', '.', 'of', 'all', 'the', 'things', 'that', 'the', 'president', 'plans', 'to', 'talk', 'about', ',', 'housing', ',', 'education', ',', 'there', 'is', 'only', 'one', 'topic', 'that', 'anyone', 'really', 'is', 'talking', 'about', 'tonight', ',', 'and', 'that', 'is', 'the', 'tax', 'plan', 'the', 'president', 'is', 'going', 'to', 'present', ',', 'he', 'says', ',', 'to', 'help', 'the', ',', 'it', \"'\", 's', 'a', 'shift', 'in', 'wealth', '.', 'republicans', 'say', 'they', 'were', 'genuinely', 'surprised', '.', 'they', 'say', 'this', 'could', 'hurt', 'chances', 'for', 'tax', 'reform', '.', 'but', 'on', 'the', 'other', 'hand', ',', 'gwen', ',', 'democrats', 'feel', 'a', 'sense', 'of', 'the', 'president', 'that', 'they', 'say', 'they', 'wanted', 'to', 'see', 'this', 'entire', 'time', '.', 'talking', 'to', 'one', ',', 'eliot', 'eng', '##el', ',', 'he', 'said', 'he', 'thinks', 'this', 'president', 'has', 'been', 'saddle', '##d', 'with', 'wars', ',', 'with', 'a', 'recession', '.', 'and', 'from', 'his', 'point', 'of', 'view', ',', 'he', 'thinks', 'now', 'this', 'is', 'a', 'president', 'freeing', 'himself', 'of', 'those', 'burden', '##s', 'and', 'being', 'the', 'democrat', 'that', 'the', 'democrats', 'up', 'here', 'wanted', 'to', 'see', 'all', 'this', 'time', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'but', 'republicans', 'are', 'in', 'control', 'this', 'time', '.', '!', 'lisa', '-', 'des', '##jar', '##din', '##s', '#', 'that', \"'\", 's', 'right', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'you', 'have', 'covered', 'the', 'hill', 'for', 'past', 'states', 'of', 'the', 'union', '.', 'does', 'it', 'feel', 'like', 'republicans', 'are', 'in', 'control', 'up', 'there', 'tonight', '?', '!', 'lisa', '-', 'des', '##jar', '##din', '##s', '#', 'it', 'absolutely', 'does', '.', 'and', ',', 'in', 'fact', ',', 'one', 'sign', ',', 'there', 'is', 'really', 'no', 'effort', 'to', 'even', 'sit', 'with', 'democrats', '.', 'republicans', 'are', 'watching', 'this', 'speech', '.', 'they', \"'\", 're', 'preparing', 'their', 'responses', 'en', 'mass', '##e', '.', 'they', 'are', 'not', 'happy', 'with', 'putting', 'it', 'in', 'a', 'way', 'that', 'they', 'think', 'compromise', 'will', 'be', 'more', 'difficult', 'to', 'get', '.', 'now', ',', 'who', 'you', 'blame', 'for', 'that', 'is', 'up', 'to', 'you', '.', 'republicans', 'clearly', 'are', 'blaming', 'the', 'president', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'lisa', 'des', '##jar', '##din', '##s', 'on', 'the', 'hill', 'for', 'us', 'tonight', ',', 'thank', 'you', '.', '!', 'lisa', '-', 'des', '##jar', '##din', '##s', '#', 'you', 'got', 'it', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'now', 'we', 'go', 'to', 'the', 'white', 'house', 'for', 'a', 'preview', 'of', 'the', 'address', 'from', 'jennifer', 'palm', '##ieri', '.', 'she', \"'\", 's', 'assistant', 'to', 'the', 'president', 'and', 'director', 'of', 'communications', '.', 'i', 'spoke', 'with', 'her', 'a', 'short', 'time', 'ago', '.', 'jennifer', 'palm', '##ieri', ',', 'thank', 'you', 'for', 'talking', 'with', 'us', '.', 'there', 'are', 'going', 'to', 'be', 'tens', 'of', 'millions', 'of', 'americans', 'watching', 'the', 'president', 'tonight', '.', 'what', \"'\", 's', 'the', 'main', 'thing', 'he', 'wants', 'to', 'get', 'across', 'to', 'them', '?', '!', 'jennifer', '-', 'palm', '##ieri', ',', '#', 'that', 'he', 'wants', 'to', 'talk', 'to', 'them', 'about', 'the', 'progress', 'that', 'the', 'country', 'has', 'made', '.', 'and', 'we', 'believe', 'that', 'part', 'of', 'the', 'progress', 'is', 'due', 'to', 'the', 'ability', 'of', 'the', 'middle', 'class', 'to', 'fight', 'back', '.', 'but', 'he', 'wants', 'to', 'explain', 'to', 'the', 'american', 'public', 'what', 'he', 'thinks', 'we', 'need', 'to', 'do', 'to', 'finish', 'the', 'job', 'of', 'the', 'problem', 'of', 'declining', 'wages', '.', 'so', 'he', 'will', 'have', '-', '-', 'he', 'will', 'refer', 'to', 'this', 'as', 'middle', '-', 'class', 'economics', '.', 'that', 'is', 'in', 'contrast', 'to', 'trickle', '-', 'down', 'economics', '.', 'we', 'believe', 'that', 'the', 'economy', ',', 'it', 'is', 'not', 'just', 'about', 'lifting', 'up', 'the', 'middle', 'class', ',', 'but', 'the', 'economy', 'ca', 'n', \"'\", 't', 'sustain', 'itself', ',', 'the', 'manner', 'in', 'which', 'you', 'want', 'it', 'to', ',', 'without', 'a', 'healthy', 'middle', 'class', '.', 'you', 'need', '-', '-', 'you', 'know', ',', 'for', 'purchasing', 'power', ',', 'buying', 'homes', ',', 'et', 'ce', '##tera', ',', 'this', 'is', 'what', '-', '-', 'the', 'steps', 'we', 'need', 'to', 'do', 'not', 'just', 'to', 'help', 'the', 'middle', 'class', ',', 'but', 'to', 'help', 'build', 'a', 'stronger', 'economy', 'overall', '.', 'so', 'he', 'will', 'have', 'specific', 'ideas', 'of', 'what', 'he', 'thinks', 'the', 'steps', 'are', 'and', 'choices', 'that', 'we', 'need', 'to', 'make', 'to', 'do', 'that', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'so', ',', 'what', 'is', 'his', 'message', 'though', 'to', 'congressional', 'republicans', 'who', 'are', 'already', 'out', 'there', 'saying', 'that', 'this', 'tax', 'proposal', 'to', 'essentially', 'raise', 'some', 'taxes', 'on', 'the', 'very', 'wealthy', 'in', 'order', 'to', 'give', 'some', 'tax', 'credits', 'to', 'people', 'with', 'moderate', 'income', '?', 'they', \"'\", 're', 'saying', 'this', 'is', 'a', 'poke', 'in', 'the', 'eye', 'to', 'republicans', 'and', 'all', ',', 'in', 'the', 'state', 'of', 'the', 'union', ',', 'the', 'president', \"'\", 's', 'job', 'is', 'to', 'present', 'to', 'the', 'country', ',', 'these', 'are', 'the', 'steps', 'we', 'need', 'to', 'take', 'in', 'order', 'to', 'face', 'our', 'biggest', 'challenge', ',', 'which', 'is', 'helping', 'stop', 'this', 'middle', '-', 'class', 'slide', '.', 'so', 'he', \"'\", 's', 'not', 'going', 'to', 'trim', 'his', 'sails', 'about', 'what', 'that', 'answer', 'is', 'because', ',', 'you', 'know', ',', 'of', 'what', 'republicans', 'may', 'have', 'said', 'in', 'advance', 'of', 'the', 'speech', ',', 'you', 'know', ',', 'that', 'the', 'answer', 'is', 'the', 'answer', '.', 'but', 'what', 'we', 'would', 'ask', 'them', 'to', 'look', 'at', 'is', 'to', 'look', 'at', 'these', 'ideas', '.', 'some', 'of', 'them', '-', '-', 'one', 'of', 'our', 'revenue', '-', 'raising', 'ideas', 'is', 'something', 'that', 'the', 'chairman', 'of', 'the', 'house', 'ways', 'and', 'means', 'committee', 'has', 'supported', ',', 'so', '-', '-', 'but', ',', 'moreover', ',', 'that', 'everyone', 'has', 'said', 'they', 'want', 'to', 'figure', 'out', 'how', 'we', 'can', 'expand', 'the', 'middle', 'class', '.', 'these', 'are', 'ideas', 'that', 'are', 'going', 'to', 'help', 'bring', 'that', 'about', ',', 'not', '-', '-', 'because', 'it', \"'\", 's', 'in', 'the', 'interest', 'of', 'the', 'economy', 'overall', 'to', 'make', 'these', 'changes', '.', 'this', 'is', 'not', 'just', 'about', '-', '-', 'this', 'is', 'not', 'class', 'warfare', '.', 'this', 'is', 'need', 'to', 'make', 'sure', 'that', 'the', 'middle', 'class', 'is', 'succeeding', '.', 'the', 'economy', 'is', 'not', 'going', 'to', 'be', 'able', 'to', 'thrive', 'until', 'that', 'happens', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'but', 'do', 'you', 'think', 'there', \"'\", 's', 'a', 'compromise', 'reach', '##able', 'that', 'is', 'n', \"'\", 't', 'what', 'the', 'republicans', 'call', 'redistribution', 'of', 'wealth', '?', '!', 'jennifer', '-', 'palm', '##ieri', '#', 'right', '.', 'we', 'think', 'there', 'is', '.', 'we', 'believe', 'that', 'tax', 'reform', 'is', 'going', 'to', 'be', 'a', 'part', 'of', 'what', 'this', 'congress', 'is', 'working', 'on', '.', 'we', 'want', 'to', 'make', 'sure', '-', '-', 'and', 'we', 'have', 'that', 'we', 'want', 'to', 'work', 'with', 'them', 'on', 'it', '.', 'we', 'want', 'to', 'work', 'with', 'them', 'on', 'business', 'tax', 'reform', '.', 'it', 'needs', 'to', 'be', 'done', 'in', 'a', 'way', 'that', 'helps', 'the', 'middle', 'class', '.', 'this', 'is', 'the', 'opening', 'conversation', 'the', 'president', 'is', 'having', 'with', 'the', 'congress', 'on', 'this', '.', 'we', 'want', 'to', 'make', 'sure', 'that', 'the', 'middle', 'class', 'is', 'part', 'of', 'that', 'discussion', 'and', 'broad', '##en', 'it', 'out', ',', 'so', 'when', 'we', \"'\", 're', '-', '-', 'when', 'congress', 'is', 'debating', 'these', 'issues', 'that', 'they', \"'\", 're', 'looking', 'at', 'a', 'full', 'ga', '##mut', 'of', 'legislative', 'options', 'and', 'not', 'something', 'that', 'is', 'particularly', 'narrow', '.', 'so', ',', 'we', 'will', 'start', 'tonight', '.', 'we', 'hope', 'that', 'and', 'then', 'we', 'will', 'take', 'it', 'from', 'there', '.', 'but', 'these', 'are', 'the', 'steps', 'the', 'country', 'needs', 'to', 'take', 'in', 'order', 'to', 'deal', 'with', 'this', 'continued', 'problem', 'of', 'inequality', '.', 'so', 'he', 'will', '-', '-', 'he', \"'\", 's', 'going', 'to', 'lay', 'that', 'out', '.', 'and', 'he', \"'\", 's', 'willing', 'to', 'compromise', ',', 'but', 'these', 'are', 'what', '-', '-', 'the', 'answers', 'we', 'believe', 'we', 'need', 'to', 'do', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'well', ',', 'we', 'will', 'all', 'be', 'listening', 'here', '.', 'jennifer', 'palm', '##ieri', ',', 'thank', 'you', '.', '!', 'jennifer', '-', 'palm', '##ieri', '#', 'thanks', ',', 'judy', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'so', ',', 'now', 'we', 'go', 'back', 'to', 'capitol', 'hill', 'for', 'a', 'republican', 'take', 'on', 'the', 'state', 'of', 'the', 'union', '.', 'and', 'that', 'comes', 'from', 'texas', 'representative', 'mac', 'thorn', '##berry', '.', 'he', \"'\", 's', 'chair', 'of', 'the', 'house', 'armed', 'services', 'committee', '.', 'and', 'i', 'also', 'spoke', 'with', 'him', 'a', 'short', 'time', 'ago', '.', 'congressman', 'thorn', '##berry', ',', 'thank', 'you', 'very', 'much', 'for', 'joining', 'us', '.', 'so', ',', 'we', 'just', 'heard', 'from', 'jennifer', 'palm', '##ieri', ',', 'the', 'white', 'house', 'communications', 'director', ',', 'about', 'the', 'president', \"'\", 's', 'plan', ',', 'among', 'other', 'things', ',', 'to', 'expand', 'the', 'economic', 'recovery', 'so', 'that', 'it', 'reaches', 'more', 'americans', ',', 'including', 'working', '-', 'class', ',', 'middle', '-', 'class', 'americans', '.', 'why', 'is', 'is', 'certainly', 'a', 'worthy', 'goal', 'to', 'grow', 'the', 'economy', 'and', 'expand', 'the', 'recovery', ',', 'so', 'that', 'everybody', 'is', 'included', 'in', 'it', '.', 'but', ',', 'unfortunately', ',', 'the', 'president', \"'\", 's', 'proposal', 'is', 'not', 'serious', '.', 'it', 'would', 'have', 'been', 'the', 'same', 'as', 'if', 'george', 'w', '.', 'bush', 'had', 'come', 'to', 'congress', 'in', '2007', ',', 'when', 'the', 'democrats', 'had', 'just', 'taken', 'control', ',', 'and', 'said', ',', 'i', 'propose', 'a', '$', '320', 'billion', 'tax', 'cut', '.', 'he', 'knows', 'that', 'this', 'congress', 'will', 'not', 'consider', 'it', '.', 'so', 'he', \"'\", 's', 'trying', 'to', 'make', 'political', 'points', '.', 'and', ',', 'really', ',', 'we', 'need', 'a', 'president', 'who', 'will', 'rise', 'above', 'that', 'sort', 'of', 'politics', 'and', 'really', 'try', 'to', 'solve', 'the', 'problems', 'that', 'are', 'facing', 'this', 'country', ',', 'including', 'an', '##emi', '##c', 'economic', 'growth', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'what', 'would', 'republicans', ',', 'what', 'would', 'you', 'like', 'to', 'see', 'done', 'to', 'expand', 'this', 'recovery', 'to', 'the', 'middle', 'class', '?', '!', 'rep', '-', 'mac', '-', 'thorn', '##berry', '#', 'well', ',', 'a', 'key', 'thing', 'is', 'that', 'the', 'regulations', 'and', 'fear', 'of', 'government', 'are', 'holding', 'down', 'the', 'recovery', '.', 'businesses', 'just', 'do', 'n', \"'\", 't', 'expand', '.', 'part', 'of', 'that', 'is', 'obama', '##care', '.', 'but', 'part', 'of', 'it', 'is', 'this', 'blizzard', 'of', 'regulations', 'that', 'epa', 'and', 'everyone', 'else', 'businesses', 'to', 'grow', 'and', 'less', 'likely', 'that', 'jobs', 'will', 'be', 'created', '.', 'and', 'so', 'you', 'have', 'this', 'tremendous', 'under', '##em', '##pl', '##oy', '##ment', 'that', \"'\", 's', 'not', 'reflected', 'in', 'the', 'unemployment', 'rate', ',', 'but', 'it', \"'\", 's', 'lots', 'of', 'folks', 'who', 'would', 'like', 'a', 'job', 'or', 'would', 'like', 'a', 'better', 'job', ',', 'and', 'those', 'opportunities', 'are', 'just', 'not', 'out', 'there', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'do', 'you', 'think', 'there', 'is', 'common', 'ground', 'to', 'be', 'found', 'between', 'where', 'the', 'president', 'is', 'and', 'where', 'congressional', 'republicans', 'are', '?', '!', 'rep', '-', 'mac', '-', 'thorn', '##berry', '#', 'oh', ',', 'and', 'it', \"'\", 's', 'one', 'of', 'the', '-', '-', 'what', 'comes', 'to', 'my', 'mind', 'is', 'hope', 'springs', 'eternal', '.', 'but', ',', 'as', 'you', 'know', ',', 'the', 'president', 'started', 'this', 'year', 'by', 'issuing', 'a', 'series', 'of', 'veto', 'threats', '.', 'he', 'comes', 'and', 'basically', 'is', 'poking', 'his', 'finger', 'in', 'the', 'eye', 'of', 'the', 'new', 'congress', '.', 'and', 'so', 'i', \"'\", 'm', 'getting', 'less', '-', '-', 'i', \"'\", 'm', 'feeling', 'more', 'di', '##sil', '##lusion', '##ed', ',', 'to', 'tell', 'you', 'the', 'truth', ',', 'that', 'the', 'president', 'has', 'any', 'interest', 'in', 'working', 'with', 'congress', '.', 'as', 'i', 'say', ',', 'it', \"'\", 's', 'mainly', 'about', 'political', 'point', '-', 'scoring', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'finally', ',', 'congressman', ',', 'as', 'the', 'new', 'chairman', 'of', 'the', 'house', 'armed', 'services', 'committee', ',', 'what', 'are', 'you', 'it', 'comes', 'to', 'defense', 'and', 'foreign', 'policy', 'in', 'this', 'country', '?', '!', 'rep', '-', 'mac', '-', 'thorn', '##berry', '#', 'well', ',', 'i', 'would', 'love', 'to', 'hear', 'a', 'strategy', 'from', 'the', 'president', 'that', 'was', 'adequate', 'to', 'meet', 'the', 'threats', 'that', 'we', 'see', 'around', 'the', 'world', ',', 'just', 'what', \"'\", 's', 'happened', 'today', 'in', 'yemen', ',', 'for', 'example', '.', 'and', ',', 'again', ',', 'i', 'would', 'really', 'like', 'to', 'see', 'a', 'president', 'who', 'would', 'sincerely', 'say', ',', 'i', 'want', 'to', 'work', 'with', 'congress', 'to', 'address', 'these', 'problems', ',', 'whether', 'it', 'is', 'cyber', '-', 'security', 'or', 'whether', 'it', \"'\", 's', 'terrorism', 'or', 'the', 'ukraine', 'or', 'what', 'china', 'is', 'doing', '.', 'rather', 'than', 'just', 'saying', 'it', 'and', 'then', 'not', 'following', 'up', ',', 'we', 'need', 'someone', 'who', 'will', 'say', 'it', 'and', 'then', 'follow', 'up', ',', 'and', 'really', 'be', 'a', 'partnership', ',', 'because', 'that', \"'\", 's', 'the', 'only', 'way', 'we', 'are', 'going', 'to', 'provide', 'for', 'our', 'country', \"'\", 's', 'security', 'or', 'solve', 'our', 'other', 'problems', '.', 'these', 'two', 'branches', 'of', 'government', 'have', 'to', 'work', 'together', 'and', 'a', 'president', 'has', 'to', 'be', 'the', 'leader', 'in', 'that', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'congressman', 'mac', 'thorn', '##berry', ',', 'chairman', 'of', 'the', 'house', 'armed', 'services', 'committee', ',', 'we', 'thank', 'you', 'for', 'talking', 'with', 'us', '.', '!', 'rep', '-', 'mac', '-', 'thorn', '##berry', '#', 'you', \"'\", 're', 'most', 'welcome', 'po', '##mp', 'and', 'pageant', '##ry', ',', 'but', 'also', 'about', 'policy', '.', 'presidents', 'often', 'present', 'congress', 'with', 'a', 'list', 'of', 'their', 'priorities', ',', 'which', 'often', 'do', 'not', 'always', 'mesh', 'with', 'what', 'law', '##makers', 'have', 'in', 'mind', '.', 'so', 'how', 'much', 'of', 'it', 'actually', 'becomes', 'law', '?', 'political', 'director', 'domenico', 'montana', '##ro', 'has', 'been', 'crunch', '##ing', 'the', 'numbers', '.', 'and', 'he', 'joins', 'me', 'now', '.', 'domenico', ',', 'as', 'the', 'president', 'lays', 'out', 'his', 'plan', 'for', 'what', 'it', 'is', 'he', 'wants', 'to', 'do', ',', 'what', 'is', 'it', 'that', 'we', 'actually', 'see', 'happen', '?', '!', 'domenico', '-', 'montana', '##ro', '#', 'well', ',', 'we', 'went', 'back', 'and', 'looked', 'at', '2014', 'and', 'saw', 'in', 'his', 'state', 'of', 'the', 'union', 'address', 'just', 'a', 'year', 'ago', 'how', 'many', 'things', 'he', 'put', 'forward', '.', 'so', ',', 'there', 'were', 'about', '18', 'items', 'that', 'were', 'proposals', 'that', 'were', 'things', 'that', 'he', 'wanted', 'congress', 'to', 'act', 'on', '.', 'well', ',', 'just', 'two', 'of', 'those', 'things', 'actually', 'made', 'it', 'through', 'congress', ',', 'when', 'you', 'look', 'at', 'it', ',', 'just', 'on', 'job', 're', '##train', '##ing', 'and', 'on', 'increasing', 'research', 'funding', '.', 'on', 'increasing', 'research', 'funding', ',', 'too', ',', 'a', 'lot', 'of', 'people', 'say', 'that', ',', 'well', ',', 'it', 'was', 'n', \"'\", 't', 'really', 'that', 'much', 'of', 'an', 'increase', 'because', 'up', '1', '.', '7', 'percent', ',', 'which', 'kept', 'with', 'inflation', ',', 'so', 'not', 'much', 'of', 'an', 'increase', 'there', '.', 'when', 'you', 'look', 'at', 'some', 'of', 'the', 'items', 'that', 'were', 'big', '-', 'ticket', 'items', 'that', 'the', 'president', 'had', 'called', 'for', 'that', 'did', 'n', \"'\", 't', 'get', 'addressed', ',', 'you', 'look', 'at', 'things', 'like', 'universal', 'pre', '-', 'k', ',', 'raising', 'the', 'minimum', 'wage', ',', 'cutting', 'the', 'oil', 'and', 'gas', 'subsidies', '.', 'those', 'and', '13', 'other', 'items', 'did', 'n', \"'\", 't', 'get', 'through', '.', 'now', ',', 'the', 'white', 'house', 'will', 'say', 'that', 'there', 'are', 'plenty', 'of', 'other', 'ways', 'to', 'go', 'about', 'getting', 'action', 'done', ',', 'like', 'executive', 'action', ',', 'encouraging', 'states', ',', 'and', 'that', \"'\", 's', 'some', 'of', 'what', 'you', 'saw', 'them', 'roll', 'out', 'last', 'year', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'which', 'of', 'course', 'is', 'what', \"'\", 's', 'making', 'everybody', 'so', 'unhappy', 'on', 'capitol', 'hill', '.', 'but', ',', 'to', 'be', 'fair', ',', 'let', \"'\", 's', 'compare', 'this', 'to', 'how', 'other', 'presidents', 'have', 'done', '.', 'do', 'state', 'of', 'the', 'union', 'addresses', 'usually', '-', '-', 'any', '-', '-', 'no', 'matter', 'who', 'the', 'president', 'is', ',', 'yield', 'actual', 'policy', ',', 'actual', 'law', '?', '!', 'domenico', '-', 'montana', '##ro', '#', 'well', ',', 'this', 'is', 'pretty', 'much', 'par', 'for', 'the', 'course', 'when', 'you', 'look', 'at', 'divided', 'government', '.', 'the', 'only', 'times', 'where', 'presidents', 'have', 'big', 'numbers', '.', 'and', 'you', 'look', 'back', 'to', 'lb', '##j', 'in', '1965', ',', 'his', 'state', 'of', 'the', 'union', 'address', ',', 'with', 'a', 'lot', 'of', 'the', 'great', 'society', 'measures', ',', 'sure', ',', 'a', 'lot', 'of', 'those', 'got', 'through', ',', 'but', 'after', 'that', ',', 'not', 'much', 'else', 'from', 'lb', '##j', 'even', '.', 'look', 'at', 'president', 'obama', 'in', 'his', 'first', 'two', 'years', '.', 'certainly', ',', 'when', 'he', 'had', 'big', 'numbers', ',', 'he', 'was', 'able', 'to', 'get', 'through', 'health', 'care', ',', 'get', 'through', 'the', 'stimulus', ',', 'but', 'beyond', 'that', ',', 'it', \"'\", 's', 'much', 'of', 'what', 'we', 'have', 'seen', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'and', ',', 'sometimes', ',', 'we', 'were', 'talking', 'about', 'small', '-', 'bore', 'items', ',', 'not', 'necessarily', 'big', 'ones', '.', '!', 'domenico', '-', 'montana', '##ro', '#', 'absolutely', '.', 'i', 'mean', ',', 'you', \"'\", 're', 'talking', 'about', '-', '-', 'everybody', 'talks', 'about', 'president', 'clinton', 'and', 'school', 'uniforms', 'and', 'things', 'like', 'that', '.', 'it', 'is', 'a', 'lot', 'of', 'these', 'smaller', 'items', 'that', 'wind', 'up', 'getting', 'pushed', 'through', '.', 'job', 're', '##train', '##ing', 'is', 'an', 'important', 'thing', 'that', 'the', 'president', 'and', 'republicans', 'were', 'able', 'to', 'work', 'together', 'on', ',', 'but', 'some', 'of', 'that', 'low', '-', 'hanging', 'fruit', 'is', 'really', 'what', 'for', 'the', 'most', 'part', 'in', 'divided', 'government', ',', 'you', \"'\", 're', 'able', 'to', 'get', 'tonight', 'that', 'the', 'president', 'is', 'able', 'to', 'talk', 'about', 'on', 'things', 'like', 'trade', 'and', 'cyber', '-', 'security', '.', 'we', 'will', 'wait', 'and', 'see', 'what', 'the', 'results', 'are', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'well', ',', 'i', 'know', 'for', 'a', 'fact', 'you', 'will', 'be', 'scrub', '##bing', 'all', 'those', 'numbers', 'tonight', '.', 'domenico', 'montana', '##ro', ',', 'thank', 'you', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'now', 'for', 'some', 'pre', '-', 'speech', 'analysis', 'from', 'shields', 'and', 'brooks', 'joining', 'us', ',', 'syndicated', 'columnist', 'mark', 'shields', 'and', 'new', 'york', 'times', 'columnist', 'david', 'brooks', '.', 'so', ',', 'david', ',', 'with', 'that', 'gloom', '##y', 'outlook', 'we', 'just', 'heard', 'from', 'domenico', 'and', 'listening', 'to', 'congressman', 'thorn', '##berry', 'saying', 'this', 'is', 'a', 'poke', 'in', 'the', 'eye', ',', 'what', 'can', 'we', 'expect', 'to', 'get', 'accomplished', '?', '!', 'david', '-', 'brooks', '#', 'who', 'says', 'passing', 'nothing', 'is', 'gloom', '##y', '?', '(', 'laughter', ')', '!', 'david', '-', 'brooks', '#', 'it', 'makes', 'me', 'happy', '.', 'it', 'is', '-', '-', 'as', 'you', 'hear', 'the', 'administration', 'talk', 'about', 'the', 'speech', ',', 'what', \"'\", 's', 'interesting', 'to', 'me', 'is', 'how', 'it', \"'\", 's', 'not', 'that', 'legislative', ',', 'a', 'little', 'less', 'laundry', 'list', 'this', 'year', 'and', 'a', 'little', 'more', 'rhetoric', '.', 'and', 'they', 'wanted', 'to', 'do', 'two', 'things', '.', 'they', 'want', 'to', 'show', 'the', 'country', 'that', 'they', 'get', 'the', 'middle', 'class', 'and', 'they', 'want', 'to', 'take', 'some', 'credit', 'you', 'can', 'make', 'a', 'choice', '.', 'you', 'can', 'pick', 'the', 'six', 'pieces', 'of', 'legislation', 'that', 'are', 'most', 'likely', 'to', 'pass', ',', 'or', 'you', 'can', 'pick', 'the', 'six', 'where', 'your', 'party', 'has', 'a', '70', '-', '30', 'polling', 'advantage', ',', 'and', 'you', 'can', 'put', 'the', 'other', 'party', 'in', 'a', 'difficult', 'position', '.', 'they', 'have', 'chosen', 'the', 'latter', '.', 'they', \"'\", 're', 'going', 'with', 'the', '70', '-', '30', '.', 'so', ',', 'they', 'have', 'a', 'lot', 'of', 'things', 'like', ',', 'should', 'we', 'tax', 'the', 'rich', 'to', 'pay', 'for', 'middle', '-', 'class', 'concerns', 'where', 'the', 'republicans', 'are', 'in', 'a', 'tough', 'pick', '##le', 'politically', '.', 'and', 'so', 'they', \"'\", 're', 'just', 'going', 'to', 'hit', 'them', 'with', 'that', ',', 'and', 'try', 'to', 'frame', 'debates', ',', 'score', 'political', 'points', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'mark', ',', 'senator', 'mcconnell', ',', 'senate', 'majority', 'leader', ',', 'senator', '-', '-', 'senate', 'leader', 'mcconnell', 'said', 'today', 'that', 'the', 'president', 'would', '-', '-', 'this', 'would', 'be', 'his', '-', '-', 'represent', 'his', 'final', 'act', ',', 'this', 'speech', 'tonight', '.', 'is', 'it', '?', '!', 'mark', '-', 'shields', '#', 'hardly', 'his', 'final', 'act', ',', 'gwen', '.', 'but', 'it', 'is', 'probably', 'his', 'last', 'chance', 'to', 'really', 'set', 'the', 'agenda', ',', 'barr', '##ing', 'a', 'crisis', 'in', '2016', ',', 'because', 'a', 'year', 'from', 'now', ',', 'we', 'will', 'be', 'right', 'in', 'the', 'and', 'new', 'hampshire', 'primaries', '.', 'and', 'to', 'a', 'great', 'degree', ',', 'the', 'terms', 'of', 'that', 'debate', 'will', 'have', 'been', 'set', '.', 'the', 'president', 'has', 'a', 'chance', '-', '-', 'and', 'is', 'seizing', 'it', 'tonight', '-', '-', 'to', 'set', 'the', 'terms', 'of', 'the', 'debate', ',', 'not', 'simply', 'for', '2015', 'and', 'with', 'the', 'congress', ',', 'but', 'for', '-', '-', 'establishing', 'the', 'lack', 'of', 'opportunity', ',', 'declining', 'opportunity', 'in', 'this', 'country', 'for', 'people', 'across', 'the', 'board', 'and', 'income', 'inequality', 'and', 'wealth', 'inequality', '.', 'and', 'we', 'saw', 'it', ',', 'the', 'progress', ',', 'when', 'mit', '##t', 'romney', ',', 'the', 'republican', 'standard', '-', 'bearer', 'in', '2012', ',', 'in', 'his', 'maiden', 'speech', 'launching', 'his', 'hopes', 'for', '2016', 'talked', 'about', 'income', 'inequality', '.', 'and', 'the', 'president', 'speaks', 'from', ',', 'a', 'strengthened', 'position', 'tonight', '.', 'a', 'week', 'is', 'a', 'lifetime', 'in', 'politics', '.', 'two', 'months', 'is', 'an', 'eternity', '.', 'two', 'months', 'ago', ',', 'he', 'was', 'the', 'democrat', 'that', 'every', 'democrat', 'ran', 'away', 'from', ',', 'did', 'n', \"'\", 't', 'want', 'to', 'be', 'seen', 'with', '.', 'and', 'now', 'his', 'numbers', 'are', 'up', ',', 'and', 'people', 'feel', 'better', 'about', '.', '.', '.', '(', 'cross', '##talk', ')', '!', 'gwen', '-', 'if', '##ill', '#', 'let', \"'\", 's', 'take', 'a', 'look', 'at', 'that', '.', 'actually', ',', 'we', 'have', 'the', 'numbers', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'yes', '.', 'i', 'was', 'january', 'numbers', ',', 'we', 'can', 'show', 'you', ',', '46', 'percent', 'average', 'approval', 'in', 'the', 'polls', ',', 'compared', 'to', '43', 'percent', 'in', 'december', '.', 'it', \"'\", 's', 'only', 'a', 'little', 'bit', 'of', 'a', 'bump', ',', 'david', ',', 'but', 'does', 'something', 'like', 'that', 'make', 'a', 'difference', 'in', 'what', 'he', 'can', 'get', 'done', '?', '!', 'david', '-', 'brooks', '#', 'it', 'does', 'in', 'sort', 'of', 'the', 'morale', 'and', 'the', 'spirit', ',', 'especially', 'of', 'the', 'white', 'house', '.', 'we', 'have', 'had', 'two', 'second', 'terms', ',', 'two', 'models', '.', 'we', 'have', 'had', 'the', 'reagan', '-', 'clinton', 'model', ',', 'where', 'they', 'go', 'up', 'and', 'they', 'end', 'very', 'high', '.', 'we', 'have', 'had', 'the', 'george', 'w', '.', 'bush', 'model', ',', 'where', 'they', 'go', 'down', ',', 'or', 'jimmy', 'carter', '.', 'they', 'end', 'really', 'low', '.', 'he', \"'\", 's', 'sort', 'of', 'in', 'the', 'middle', 'now', '.', 'he', 'was', 'looking', 'like', 'he', 'was', 'on', 'the', 'bush', 'trajectory', ',', 'but', 'now', 'he', \"'\", 's', 'sort', 'of', 'in', 'the', 'middle', 'there', '.', 'and', 'it', \"'\", 's', 'all', 'about', 'internal', 'morale', '.', 'and', 'they', \"'\", 're', 'feeling', 'kind', 'of', 'combat', '##ive', 'in', 'the', 'white', 'house', '.', 'they', \"'\", 're', 'not', 'expecting', 'to', 'get', 'a', 'lot', 'done', '.', 'but', 'they', \"'\", 're', 'feeling', 'combat', '##ive', '.', 'the', 'economy', 'is', 'clearly', 'picking', 'admit', 'we', \"'\", 're', 'in', 'a', 'strong', 'economy', '.', 'you', 'look', 'at', 'the', 'rest', 'of', 'the', 'world', ',', 'we', 'are', 'in', 'a', 'strong', 'economy', '.', 'the', 'structural', 'problems', 'are', 'the', 'same', '.', 'and', 'also', 'there', 'have', 'been', 'no', 'be', '##head', '##ings', '.', 'putin', 'has', 'n', \"'\", 't', 'done', 'much', '.', 'we', 'have', 'had', 'a', 'period', 'of', 'relative', 'calm', 'around', 'the', 'world', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'my', 'favorite', 'numbers', 'today', 'are', 'comparing', 'this', 'president', \"'\", 's', 'second', '-', 'to', '-', 'last', 'state', 'of', 'the', 'union', 'speech', 'to', 'other', 'presidents', '.', 'at', 'this', 'point', 'in', 'his', 'term', ',', 'president', 'obama', 'is', 'at', '46', 'percent', ',', 'as', 'we', 'just', 'said', '.', 'but', 'president', 'bush', 'was', 'at', '36', 'percent', 'approval', '.', 'and', 'bill', 'clinton', 'post', '-', 'lew', '##insky', 'was', 'at', '69', 'percent', '.', 'so', 'maybe', 'it', 'does', 'n', \"'\", 't', 'matter', '.', '!', 'mark', '-', 'shields', '#', 'well', ',', 'it', 'does', 'matter', '-', '-', 'post', '-', 'imp', '##ea', '##chment', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'post', '-', 'imp', '##ea', '##chment', '.', '(', 'cross', '##talk', ')', '!', 'mark', '-', 'shields', '#', 'and', 'remember', 'this', '.', 'remember', 'this', ',', 'gwen', '.', 'bill', 'clinton', 'laid', 'down', 'the', 'pre', '##dicate', 'that', 'al', 'gore', 'carried', 'the', 'popular', 'vote', 'in', '2000', ',', 'in', 'other', 'words', ',', 'because', 'people', 'were', 'optimistic', 'about', 'the', 'country', 'and', 'satisfied', 'with', 'his', 'performance', '.', 'the', 'same', 'thing', ',', 'that', 'growing', 'optimism', 'sets', 'the', 'terms', 'of', 'the', '2016', 'election', '.', 'judy', 'december', ',', 'i', 'would', 'just', 'point', 'out', 'he', 'was', 'at', '40', 'percent', 'for', 'most', 'of', '2014', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'right', '.', '!', 'mark', '-', 'shields', '#', 'he', 'was', 'really', 'down', '.', 'and', 'it', 'is', 'a', 'surge', '.', 'and', 'we', 'have', 'fewer', 'people', 'dissatisfied', 'with', 'the', 'economy', 'now', ',', 'more', 'satisfied', 'actually', ',', 'than', 'any', 'time', 'in', 'the', 'last', 'nine', 'years', '.', 'so', 'there', 'is', 'a', 'sense', 'of', 'res', '##urgent', 'optimism', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'well', ',', 'we', 'are', 'so', 'glad', 'the', 'two', 'of', 'you', 'are', 'going', 'to', 'be', 'joining', 'us', 'for', 'our', 'special', 'state', 'of', 'the', 'union', 'coverage', 'tonight', '.', 'and', 'we', 'want', 'to', 'ask', 'everybody', 'watching', 'to', 'join', 'us', 'for', 'special', 'coverage', 'of', 'the', 'president', \"'\", 's', 'address', 'at', '9', ':', '00', 'eastern', 'right', 'here', 'on', 'your', 'local', 'pbs', 'station', 'and', 'throughout', 'the', 'evening', 'online', '.', 'and', 'that', \"'\", 's', 'at', 'pbs', '.', 'org', '.', '/', 'news', '##ho', '##ur', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'the', 'president', 'will', 'be', 'talking', 'about', 'foreign', 'policy', 'tonight', 'as', 'well', ',', 'even', 'as', 'events', 'continue', 'to', 'un', '##fold', 'abroad', '.', 'the', 'islamic', 'state', 'group', 'released', 'another', 'hostage', 'video', 'today', ',', 'threatening', 'to', 'kill', 'two', 'japanese', 'nationals', 'if', 'hundreds', 'of', 'millions', 'of', 'dollars', 'in', 'ransom', 'is', 'n', \"'\", 't', 'paid', 'later', 'this', 'week', '.', 'jeffrey', 'brown', 'reports', '.', '!', 'jeffrey', '-', 'brown', '#', 'the', 'threat', 'came', 'and', 'israeli', 'leaders', 'in', 'the', 'middle', 'east', '.', 'a', 'new', 'video', 'message', 'warned', 'that', 'islamic', 'state', 'militants', 'will', 'be', '##head', 'two', 'japanese', 'hostages', 'in', 'syria', 'unless', 'tokyo', 'pays', 'a', 'ransom', '.', '!', 'man', '#', 'to', 'the', 'japanese', 'public', ',', 'you', 'now', 'have', '72', 'hours', 'to', 'pressure', 'your', 'government', 'in', 'making', 'a', 'wise', 'decision', 'by', 'paying', 'the', '$', '200', 'million', 'to', 'save', 'the', 'lives', 'of', 'your', 'citizens', '.', 'otherwise', ',', 'this', 'knife', 'will', 'become', 'your', 'nightmare', '.', '!', 'jeffrey', '-', 'brown', '#', 'the', 'captives', 'were', 'identified', 'as', 'kenji', 'got', '##o', ',', 'a', 'freelance', 'journalist', ',', 'and', 'ha', '##runa', 'yu', '##kawa', ',', 'who', 'founded', 'a', 'private', 'security', 'firm', '.', 'the', 'ransom', 'matched', 'japan', \"'\", 's', 'pledge', 'of', '$', '200', 'million', 'in', 'non', '-', 'military', 'aid', 'to', 'help', 'iraq', 'battle', 'islamic', 'state', 'forces', 'and', 'to', 'aid', 'syrian', 'refugees', '.', 'in', 'jerusalem', ',', 'prime', 'minister', 'abe', 'would', 'n', \"'\", 't', 'say', 'if', 'his', 'government', 'will', 'pay', ',', 'but', 'he', 'did', 'say', 'that', 'saving', 'the', 'hostages', 'is', '-', '-', 'quote', '-', '-', '\"', 'the', 'top', 'priority', '.', '\"', '!', 'shin', '##zo', '-', 'abe', ',', '-', 'japan', '##e', '#', 'it', 'is', 'an', 'unacceptable', 'act', 'to', 'threaten', 'us', 'in', 'exchange', 'for', 'human', 'lives', ',', 'and', 'i', 'feel', 'angry', 'about', 'it', '.', 'i', 'strongly', 'urge', 'them', 'to', 'immediately', 'release', 'the', 'hostages', 'without', 'harm', '##ing', 'them', '.', 'state', 'has', 'made', 'a', 'ransom', 'demand', 'public', '.', 'u', '.', 's', '.', 'officials', 'say', 'the', 'family', 'of', 'james', 'foley', 'rejected', 'a', 'private', 'demand', 'for', '$', '130', 'million', '.', 'foley', ',', 'along', 'with', 'fellow', 'americans', 'peter', 'ka', '##ssi', '##g', 'and', 'steven', 'so', '##tl', '##off', 'were', 'later', 'beheaded', '.', 'and', 'joining', 'me', 'now', 'is', 'author', 'and', '21', '-', 'year', 'cia', 'veteran', 'bob', 'bae', '##r', '.', 'his', 'latest', 'book', 'is', 'called', '\"', 'the', 'perfect', 'kill', ':', '21', 'laws', 'for', 'assassins', '.', '\"', 'and', 'let', \"'\", 's', 'start', 'with', 'the', 'demand', 'for', 'money', '.', 'if', 'this', 'is', ',', 'in', 'fact', ',', 'the', 'first', 'time', 'it', \"'\", 's', 'been', 'made', 'so', 'publicly', ',', 'what', 'do', 'you', 'make', 'of', 'that', '?', '!', 'robert', '-', 'bae', '##r', ',', '-', 'form', '##e', '#', 'well', ',', 'the', 'islamic', 'state', 'is', 'suffering', 'economically', '.', 'they', 'need', 'the', 'money', '.', 'its', 'so', '-', 'called', 'oil', 'sales', 'have', 'run', 'out', ',', 'especially', 'with', 'the', 'price', 'of', 'oil', 'down', '.', 'they', \"'\", 're', 'em', '##bat', '##tled', '.', 'and', ',', 'you', 'know', ',', 'it', \"'\", 's', 'sort', 'of', 'well', '-', 'known', 'in', 'these', 'circles', 'that', 'the', 'french', 'first', 'started', 'paying', 'to', 'get', 'hostages', 'out', '.', 'the', 'french', 'set', 'a', 'price', '.', 'and', ',', 'in', 'fact', ',', 'the', 'hostages', 'were', 'released', '.', 'subsequently', ',', 'various', 'ngos', ',', 'nonprofit', 'organizations', ',', 'also', 'paid', 'ransom', '##s', '.', 'islamic', 'state', 'needs', 'the', 'money', '.', '!', 'jeffrey', '-', 'brown', '#', 'in', 'this', 'case', ',', 'though', ',', 'clearly', 'no', 'coincidence', 'between', 'the', 'demand', 'for', 'the', 'same', 'amount', 'as', 'the', 'japanese', 'are', 'giving', 'for', 'non', '-', 'military', 'aid', '?', '!', 'robert', '-', 'bae', '##r', '#', 'oh', ',', 'exactly', '.', 'it', \"'\", 's', 'wild', 'that', 'the', 'japanese', 'have', 'offered', 'this', 'aid', 'to', 'iraq', ',', 'most', 'of', 'what', 'they', 'have', 'offered', 'to', 'iraq', ',', 'simply', 'because', 'iraq', 'is', 'in', 'the', 'middle', 'of', 'a', 'civil', 'war', 'between', 'sunni', 'and', 'shia', '.', 'and', 'the', 'japanese', 'are', 'so', 'lo', '##ath', '##e', 'to', 'get', 'involved', 'in', 'middle', 'east', 'conflicts', ',', 'yet', 'they', 'are', 'taking', 'the', 'side', 'of', 'what', 'is', 'essentially', 'the', 'shia', 'government', 'in', 'baghdad', '.', 'so', ',', 'abe', ',', 'made', 'a', 'clear', 'mistake', 'on', 'this', ',', 'getting', 'involved', ',', 'at', 'least', 'in', 'regards', 'to', 'the', 'hostages', '.', '!', 'jeffrey', '-', 'brown', '#', 'how', 'unusual', 'is', 'it', 'though', 'to', 'see', 'i', '.', 's', '.', 'targeting', 'a', 'country', 'like', 'japan', '?', 'have', 'we', 'seen', 'anything', 'like', 'that', '?', '!', 'robert', '-', 'bae', '##r', '#', 'i', 'have', 'never', 'seen', 'the', 'islamic', 'state', 'target', 'non', '##comb', '##ative', 'states', '.', 'there', 'may', 'be', 'some', 'ngos', 'that', 'they', 'picked', 'up', 'that', 'they', 'traded', 'for', 'money', ',', 'but', ',', 'generally', ',', 'they', \"'\", 're', 'after', 'americans', ',', 'british', 'citizens', ',', 'french', ',', 'anybody', '.', 'so', 'going', 'after', 'the', 'japanese', '-', '-', 'well', ',', 'there', \"'\", 's', 'another', 'side', 'to', 'this', ',', 'of', 'course', ',', 'and', 'they', 'have', 'probably', 'run', 'out', 'of', 'hostages', 'as', 'well', 'to', 'trade', ',', 'so', 'they', 'have', 'moved', 'down', 'the', 'list', 'to', 'the', 'japanese', '.', '!', 'jeffrey', '-', 'brown', '#', 'what', 'do', 'we', 'know', 'about', 'the', 'experience', 'of', 'dealing', 'with', 'or', 'negotiating', 'with', 'i', '.', 's', '.', 'based', 'on', 'these', 'other', 'experience', '-', '-', 'other', 'examples', '?', '!', 'robert', '-', 'bae', '##r', '#', 'i', \"'\", 'm', 'in', 'touch', 'with', 'people', 'who', 'have', 'negotiated', 'with', 'the', 'islamic', 'state', ',', 'and', 'they', 'have', 'said', ',', 'frankly', '-', '-', 'these', 'people', 'have', 'dealt', 'with', 'hamas', ',', 'al', '-', 'q', '##aid', '##a', ',', 'right', 'down', '-', '-', 'they', 'have', 'been', 'in', 'somalia', 'for', 'years', 'and', 'they', 'have', 'never', 'seen', 'a', 'group', 'that', 'is', 'more', 'paranoid', '.', 'they', 'use', 'the', 'word', 'psychotic', ',', 'unpredictable', '.', 'but', 'they', 'did', 'follow', 'through', 'and', 'released', 'hostages', 'when', 'they', 'were', 'paid', 'money', '.', 'and', 'they', 'were', 'also', '-', '-', 'what', 'is', 'interesting', 'is', 'that', 'the', 'islamic', 'state', 'is', 'using', 'en', '##cr', '##yp', '##ted', 'e', '-', 'mail', 'to', 'negotiate', ',', 'so', 'they', \"'\", 're', 'very', 'conscious', 'of', 'not', 'leaving', 'a', 'trail', 'behind', '.', '!', 'jeffrey', '-', 'brown', '#', 'we', 'heard', 'that', 'voice', 'of', 'the', 'i', '.', 's', 'kidnap', '##per', ',', 'a', 'familiar', 'voice', ',', 'with', 'a', '.', 'i', 'realize', 'the', 'fbi', 'director', 'has', 'said', 'that', 'they', 'know', 'who', 'it', 'is', '.', 'on', 'the', 'other', 'hand', ',', 'i', 'have', 'heard', 'law', 'enforcement', 'people', 'in', 'washington', 'tell', 'me', 'they', 'still', 'have', 'n', \"'\", 't', 'identified', 'him', ',', 'they', 'are', 'not', 'quite', 'sure', '.', 'so', 'i', \"'\", 'm', 'not', 'quite', 'sure', 'what', 'his', 'true', 'name', 'is', ',', 'but', 'that', \"'\", 's', 'certainly', 'the', 'same', 'accent', '.', 'and', 'all', 'these', 'hostage', '-', 'take', '##rs', ',', 'by', 'the', 'way', ',', 'are', 'foreigners', '.', 'they', \"'\", 're', 'mostly', 'western', 'europeans', '.', 'and', ',', 'of', 'course', ',', 'they', 'would', 'be', 'able', 'to', 'speak', 'english', 'to', 'the', 'japanese', 'hostages', '.', '!', 'jeffrey', '-', 'brown', '#', 'and', 'what', 'do', 'your', 'sources', 'tell', 'you', 'about', 'how', 'organized', 'these', 'kidnapping', '##s', 'are', 'in', 'these', 'kind', 'of', 'ransom', 'offers', ',', 'individuals', ',', 'factions', 'or', 'well', '-', 'organized', 'by', 'a', 'group', '?', '!', 'robert', '-', 'bae', '##r', '#', 'they', 'think', 'that', 'they', 'just', 'belong', 'to', 'their', 'wing', 'of', 'the', 'islamic', 'state', '.', 'there', \"'\", 's', 'no', 'evidence', 'that', 'these', 'people', 'are', 'free', '##lan', '##cing', 'it', '.', 'you', 'know', ',', 'it', \"'\", 's', 'a', 'policy', 'decision', 'made', 'by', 'the', 'supreme', 'leader', 'of', 'the', 'islamic', 'state', ',', 'abu', 'ba', '##kr', 'al', '-', 'baghdad', '##i', '.', 'so', 'they', 'know', 'what', 'they', \"'\", 're', 'doing', '.', 'and', 'for', 'a', 'while', ',', 'i', 'thought', 'but', 'that', \"'\", 's', 'clearly', 'not', 'true', '.', 'and', 'it', \"'\", 's', 'very', 'possible', 'that', 'if', 'these', 'demands', 'are', 'not', 'met', 'within', 'the', 'next', '72', 'hours', ',', 'that', 'they', 'will', 'execute', 'them', '.', '!', 'jeffrey', '-', 'brown', '#', 'and', 'just', 'briefly', ',', 'finally', ',', 'you', 'said', 'that', 'you', 'think', 'they', 'need', 'more', 'money', '.', 'that', 'would', 'suggest', 'more', 'kidnapping', '##s', ',', 'more', 'ransom', '.', '!', 'robert', '-', 'bae', '##r', '#', 'there', \"'\", 's', 'probably', 'a', 'lot', 'more', 'kidnapping', '##s', 'going', 'on', ',', 'more', 'than', 'we', 'know', 'of', ',', 'for', 'instance', ',', 'iraqi', '##s', 'or', 'even', 'syrian', '##s', ',', 'and', 'these', 'are', 'private', 'negotiations', '.', 'they', \"'\", 're', 'getting', 'money', 'for', 'it', '.', 'all', 'the', 'oil', 'income', 'coming', 'out', 'of', 'iraq', 'is', 'going', 'to', 'the', 'government', 'of', 'baghdad', 'or', 'the', 'ku', '##rds', ',', 'and', 'they', \"'\", 're', 'up', 'to', 'four', 'million', 'barrels', '.', 'so', 'the', 'sunni', ',', 'whether', 'the', 'islamic', 'state', 'or', 'tribes', 'in', 'the', 'al', '-', 'an', '##bar', 'province', ',', 'are', 'really', 'feeling', 'an', 'economic', 'pinch', ',', 'in', 'addition', 'of', 'course', 'to', 'the', 'bombings', '.', '!', 'jeffrey', '-', 'brown', '#', 'all', 'right', ',', 'robert', 'bae', '##r', ',', 'thank', 'you', 'so', 'much', '.', '!', 'robert', '-', 'bae', '##r', '#', 'thank', 'you', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'it', 'was', 'a', 'day', 'of', 'decisions', 'and', 'argument', 'at', 'the', 'u', '.', 's', '.', 'supreme', 'court', '.', 'justices', 'ruled', 'in', 'favor', 'of', 'an', 'arkansas', ',', 'but', 'had', 'been', 'barred', 'from', 'doing', 'so', 'because', 'of', 'prison', 'rules', '.', 'and', 'the', 'justices', 'heard', 'a', 'case', 'that', 'could', 'un', '##rave', '##l', 'state', 'laws', 'around', 'the', 'country', 'regarding', 'judicial', 'candidates', 'sol', '##ici', '##ting', 'campaign', 'donations', '.', 'we', 'are', 'joined', 'now', 'by', '\"', 'news', '##ho', '##ur', '\"', 'contributor', 'marcia', 'co', '##yle', 'of', '\"', 'the', 'national', 'law', 'journal', '.', '\"', 'so', ',', 'marcia', ',', 'let', \"'\", 's', 'talk', 'first', 'about', 'this', 'unanimous', 'decision', 'having', 'to', 'do', 'with', 'the', 'case', '-', '-', 'we', 'just', 'described', 'it', '-', '-', 'a', 'prisoner', 'in', 'arkansas', 'who', 'wanted', 'to', 'grow', 'a', 'beard', '.', '!', 'marcia', '-', 'co', '##yle', ',', '-', '\"', 'the', '#', 'right', '.', 'right', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'he', \"'\", 's', 'a', 'muslim', '.', 'tell', 'us', 'about', 'this', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'yes', '.', 'gregory', 'holt', 'claimed', 'that', 'the', 'no', '-', 'beard', 'policy', 'by', 'arkansas', 'violated', 'his', 'right', 'under', 'a', 'federal', 'law', 'known', 'as', 'the', 'religious', 'land', 'use', 'and', 'institutional', '##ized', 'person', 'act', 'of', '2000', '.', 'and', 'that', 'act', 'prohibits', 'the', 'government', 'from', 'substantially', 'burden', '##ing', 'an', 'institutional', '##ized', 'person', \"'\", 's', 'religious', 'expression', ',', 'unless', 'the', 'government', 'chooses', 'the', 'least', 'restrictive', 'means', 'to', 'achieve', 'a', 'compelling', 'government', 'interest', '.', 'arkansas', 'defended', 'its', 'policy', 'in', 'the', 'supreme', 'court', ',', 'saying', 'it', 'had', 'two', 'compelling', 'interests', '.', 'it', 'wanted', 'to', '-', '-', 'within', 'beard', '##s', '.', 'and', 'it', 'also', 'wanted', 'to', 'ensure', 'quick', 'identification', 'of', 'prisoners', 'who', 'might', ',', 'by', 'growing', 'beard', '##s', ',', 'alter', 'their', 'appearance', 'and', 'get', 'into', 'forbidden', 'areas', 'of', 'a', 'prison', '.', 'justice', 'ali', '##to', 'wrote', 'for', 'a', 'unanimous', 'supreme', 'court', 'today', ',', 'and', 'he', 'said', 'that', 'the', 'court', 'really', 'does', 'n', \"'\", 't', 'question', 'those', 'compelling', 'interests', '.', 'but', 'the', 'problem', 'was', 'that', 'arkansas', 'did', 'n', \"'\", 't', 'choose', 'the', 'least', 'restrictive', 'means', 'here', 'to', 'achieve', 'them', '.', 'for', 'example', ',', 'the', 'contra', '##band', 'issue', ',', 'he', 'said', 'it', \"'\", 's', 'very', 'hard', 'to', 'believe', 'that', 'a', 'prisoner', 'could', 'hide', 'contra', '##band', 'like', 'a', 'razor', 'blade', 'in', 'a', 'half', '-', 'inch', 'beard', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'it', \"'\", 's', 'only', 'a', 'half', '-', 'inch', 'beard', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'exactly', '.', 'and', 'he', 'said', 'they', 'allow', 'prisoners', 'to', 'grow', 'their', 'hair', 'long', ',', 'and', 'they', 'search', 'that', 'hair', 'for', 'contra', '##band', '.', 'why', 'not', 'do', 'it', 'with', 'beard', '##s', '?', 'and', 'then', 'on', 'the', 'quick', 'identification', 'of', 'prisoners', ',', 'he', 'said', ',', 'why', 'not', 'a', 'before', '-', 'and', '-', 'after', 'photograph', 'that', 'you', 'can', 'use', '?', 'so', 'it', 'was', 'a', 'very', 'straightforward', 'opinion', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', ',', 'again', ',', 'unanimous', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'yes', '.', 'and', 'i', 'would', 'also', 'like', 'to', 'point', 'out', ',', 'beard', '##s', 'on', 'prisoners', ',', 'some', 'even', 'longer', 'than', 'a', 'half', '-', 'inch', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'now', ',', 'let', 'me', 'ask', 'you', 'about', 'something', 'else', '.', 'the', 'court', 'also', 'heard', 'an', 'argument', 'today', '.', 'this', 'was', 'a', 'florida', 'case', 'involving', 'campaign', 'contributions', 'to', 'candidates', 'for', 'judicial', 'office', '.', 'tell', 'us', 'about', 'the', 'argument', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'ok', '.', 'this', 'was', 'a', 'challenge', 'under', 'the', 'first', 'amendment', 'by', 'a', 'woman', 'who', 'had', 'run', 'for', 'a', 'county', 'court', 'seat', ',', 'and', 'she', 'lost', '.', 'she', 'was', 'challenging', 'the', 'florida', 'bar', '-', '-', 'or', ',', 'actually', ',', 'it', \"'\", 's', 'the', 'judicial', 'ethics', 'rule', ',', 'and', 'it', 'was', 'defended', 'by', 'the', 'florida', 'bar', '-', '-', 'that', 'prohibits', 'judicial', 'candidates', 'from', 'directly', 'sol', '##ici', '##ting', 'contributions', 'in', 'their', 'campaigns', '.', 'but', 'the', 'rule', 'allows', 'the', 'judicial', 'candidate', 'to', 'create', 'a', 'committee', ',', 'a', 'candidate', 'committee', 'to', 'sol', '##ici', '##t', 'those', 'contributions', 'under', 'the', 'candidate', \"'\", 's', 'name', 'and', 'also', 'allows', 'the', 'candidate', 'to', 'write', 'thank', '-', 'you', 'notes', 'for', 'contributions', '.', 'so', ',', 'in', 'the', 'arguments', 'today', ',', 'the', 'court', 'appeared', 'divided', 'over', 'what', 'to', 'do', 'here', ',', 'a', 'very', 'similar', 'divide', 'to', 'all', 'of', 'its', 'campaign', 'finance', 'cases', 'recently', '.', 'on', 'the', 'one', 'hand', ',', 'you', 'had', 'chief', 'makes', 'a', 'fundamental', 'decision', 'to', 'elect', 'its', 'judges', ',', 'it', \"'\", 's', 'putting', 'them', 'into', 'the', 'political', 'fray', '.', 'and', 'election', 'speech', 'is', 'at', 'the', 'core', 'of', 'first', 'amendment', 'protection', '.', 'and', 'then', ',', 'on', 'the', 'other', 'hand', ',', 'you', 'had', 'some', 'of', 'the', 'court', \"'\", 's', 'more', 'liberal', 'justices', ',', 'like', 'justices', 'br', '##eyer', 'and', 'soto', '##may', '##or', ',', 'saying', ',', 'yes', ',', 'but', 'judicial', 'candidates', 'are', 'n', \"'\", 't', 'exactly', 'like', 'other', 'political', 'candidates', 'for', 'office', '.', 'other', 'political', 'candidates', 'are', 'expected', 'to', 'make', 'commitments', 'to', 'do', 'things', '.', 'judicial', 'candidates', 'are', 'supposed', 'to', 'be', 'neutral', ',', 'imp', '##art', '##ial', '.', 'and', 'so', 'you', 'saw', 'that', 'divide', 'playing', 'out', ',', 'as', 'the', 'florida', 'bar', 'defended', 'the', 'rules', ',', 'saying', ',', 'we', \"'\", 're', 'trying', 'to', 'eliminate', 'that', 'direct', 'link', 'between', 'the', 'judicial', 'candidate', 'and', 'the', 'contributor', '.', 'that', 'is', 'a', 'link', 'that', 'creates', 'either', 'the', 'appearance', 'or', 'actual', 'corruption', ',', 'and', 'it', 'dim', '##ini', '##sh', '##es', 'public', 'confidence', 'in', 'the', 'judiciary', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'just', 'quickly', ',', 'what', 'piece', 'of', 'the', 'constitution', 'is', 'at', 'play', 'here', '?', '!', 'marcia', '-', 'co', '##yle', '#', 'this', 'is', 'the', 'first', 'amendment', '.', 'and', 'like', 'i', 'said', ',', 'election', 'speech', ',', 'even', 'if', 'it', 'involves', 'money', ',', 'is', 'at', 'the', 'core', 'short', 'thing', 'i', 'want', 'to', 'mention', ',', 'marcia', '.', 'we', 'know', 'that', 'the', 'justices', 'declined', 'to', 'hear', 'a', 'number', 'of', 'quick', 'cases', 'today', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'yes', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'but', 'one', 'of', 'those', 'involved', 'a', 'story', 'that', 'we', 'have', 'done', 'here', 'on', 'the', '\"', 'news', '##ho', '##ur', '\"', 'involving', 'so', '-', 'called', 'burn', 'pits', 'in', 'war', 'zones', 'in', 'iraq', 'and', 'afghanistan', 'where', 'chemicals', 'are', 'used', 'and', 'where', 'the', 'troops', 'have', 'experienced', 'after', '##ef', '##fect', '##s', 'of', 'breathing', 'in', 'from', 'these', 'burn', 'pits', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'right', '.', 'yes', '.', 'right', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'now', ',', 'this', 'mean', 'that', 'class', '-', 'action', 'suits', ',', 'as', 'i', 'understand', 'it', ',', 'can', 'go', 'forward', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'a', 'large', 'number', 'of', 'suits', 'were', 'filed', 'by', 'u', '.', 's', '.', 'military', 'service', 'members', 'who', 'claimed', 'injuries', 'from', 'the', 'exposure', 'to', 'the', 'chemicals', 'and', 'the', 'toxic', 'fu', '##mes', 'coming', 'out', 'of', 'these', 'open', '-', 'air', 'burn', 'pits', '.', 'the', 'supreme', 'court', 'declined', 'review', '.', 'it', 'was', 'an', 'appeal', 'brought', 'by', 'the', 'contractor', 'here', 'who', 'operated', 'the', 'burn', 'pits', ',', 'kellogg', 'brown', '&', 'amp', ';', 'root', '.', 'and', 'by', 'declining', 'that', 'appeal', ',', 'which', 'the', 'obama', 'administration', 'also', 'said', 'to', 'the', 'court', ',', 'decline', 'the', 'appeal', ',', 'it', 'left', 'in', 'place', 'a', 'federal', 'appellate', 'court', 'decision', 'saying', ',', 'court', 'dismissed', 'them', 'too', 'quickly', '.', 'there', 'was', 'n', \"'\", 't', 'enough', 'evidence', 'in', 'the', 'record', 'for', 'the', 'district', 'court', 'to', 'dismiss', 'them', '.', 'so', 'the', 'process', 'we', 'call', 'discovery', 'will', 'go', 'forward', 'on', 'these', 'claims', '.', 'the', 'people', 'bringing', 'them', 'live', 'to', 'fight', 'another', 'day', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'very', 'interesting', '.', 'marcia', 'co', '##yle', ',', 'we', 'thank', 'you', '.', '!', 'marcia', '-', 'co', '##yle', '#', 'my', 'pleasure', ',', 'judy', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'now', ':', 'a', 'colorado', 'court', 'tackles', 'the', 'job', 'of', 'picking', 'a', 'jury', 'for', 'one', 'of', 'the', 'dead', '##liest', 'mass', 'shooting', '##s', 'in', 'u', '.', 's', '.', 'history', '.', 'the', 'process', 'began', 'today', ',', 'two', '-', 'and', '-', 'a', '-', 'half', 'years', 'after', 'the', 'killings', 'in', 'a', 'crowded', 'movie', 'theater', 'outside', 'denver', '.', 'it', 'was', 'july', '20', ',', '2012', ',', 'a', 'midnight', 'screening', 'of', 'a', 'batman', 'movie', ',', 'when', 'a', 'gun', '##man', 'opened', 'fire', 'at', 'this', 'theater', 'in', 'aurora', '.', 'twelve', 'people', 'died', ',', 'and', 'another', '70', 'were', 'wounded', '.', '!', 'dona', '##van', '-', 'tate', ',', '-', 'wit', '##n', '#', 'it', 'was', 'just', 'chaos', '.', 'you', 'saw', 'injured', 'people', '.', 'you', '-', '-', 'there', 'was', 'this', 'one', 'guy', 'who', 'was', 'on', 'all', 'fours', 'crawling', '.', 'there', 'was', 'this', 'girl', 'spitting', 'up', 'blood', '.', 'there', 'were', 'bullet', 'holes', 'in', 'some', 'people', \"'\", 's', 'backs', ',', 'some', 'people', \"'\", 's', 'arms', '.', '!', 'gwen', '-', 'if', '##ill', '#', ',', 'wearing', 'a', 'gas', 'mask', ',', 'helmet', 'and', 'body', 'armor', '.', 'today', ',', 'the', 'scene', 'shifted', 'to', 'the', 'ara', '##pa', '##hoe', 'county', 'courthouse', '.', 'the', '27', '-', 'year', '-', 'old', 'defendant', 'sat', 'quietly', 'with', 'his', 'defense', 'team', ',', 'as', 'the', 'court', 'began', 'screening', '7', ',', '000', 'potential', 'ju', '##ror', '##s', ',', 'a', 'pains', '##taking', 'process', 'that', 'could', 'last', 'until', 'late', 'may', 'or', 'early', 'june', '.', 'holmes', 'has', 'pleaded', 'not', 'guilty', 'by', 'reason', 'of', 'insanity', 'to', '166', 'counts', 'of', 'murder', ',', 'attempted', 'murder', 'and', 'explosives', 'charges', '.', 'his', 'defense', 'attorneys', 'argue', 'he', 'was', 'in', 'the', 'th', '##ro', '##es', 'of', 'a', 'psychotic', 'episode', 'that', 'night', '.', 'if', 'holmes', 'is', 'convicted', ',', 'he', 'could', 'face', 'the', 'death', 'penalty', '.', 'but', 'if', 'ju', '##ror', '##s', 'accept', 'his', 'defense', ',', 'he', 'will', 'be', 'committed', 'to', 'a', 'state', 'mental', 'hospital', 'indefinitely', '.', 'for', 'more', 'on', 'the', 'beginning', 'of', 'what', 'is', 'sure', 'to', 'be', 'a', 'lengthy', 'trial', ',', 'we', 'are', 'joined', 'by', 'mary', 'mac', '##car', '##thy', 'of', 'feature', 'story', 'news', '.', 'she', 'was', 'in', 'the', 'courtroom', 'today', '.', 'mary', 'mac', '##car', '##thy', ',', 'how', 'are', 'they', 'going', 'about', 'narrowing', 'down', 'or', 'even', 'finding', 'an', 'imp', '##art', '##ial', 'jury', 'in', 'such', 'a', 'notorious', 'case', '?', '!', 'mary', '-', 'mac', '##car', '##thy', ',', '-', 'fe', '#', 'well', ',', 'they', \"'\", 're', 'doing', 'it', 'by', 'actually', 'making', 'history', 'here', '.', 'jury', 'summons', '##es', '.', 'we', 'found', 'out', 'today', 'that', 'that', 'initial', 'group', 'of', '9', ',', '000', 'was', 'w', '##hit', '##tled', 'down', 'already', 'to', '7', ',', '000', ',', 'because', '2', ',', '000', 'of', 'the', 'people', 'who', 'received', 'summons', '##es', 'were', '-', '-', 'either', 'had', 'a', 'connection', 'to', 'the', 'case', 'or', 'the', 'summons', 'was', 'in', 'fact', 'und', '##eli', '##vera', '##ble', '.', 'now', ',', 'the', 'judge', 'said', 'today', 'that', 'he', 'hopes', 'that', ',', 'of', 'those', '7', ',', '000', ',', 'they', 'will', 'actually', 'have', 'to', 'call', 'in', 'or', 'see', 'in', 'the', 'courtroom', 'somewhere', 'between', '3', ',', '000', 'and', '3', ',', '800', ',', 'and', ',', 'from', 'that', ',', 'w', '##hit', '##tle', 'the', 'group', 'down', 'once', 'again', 'to', 'between', '100', 'and', '120', '.', 'from', 'that', ',', 'they', 'will', 'find', '24', 'ju', '##ror', '##s', 'who', 'they', 'hope', 'can', 'serve', 'on', 'this', ',', '12', 'actual', 'ju', '##ror', '##s', ',', '12', 'alternate', '##s', ',', 'so', ',', 'again', ',', 'people', 'who', 'can', 'give', 'up', 'many', 'months', 'of', 'their', 'life', '.', 'they', \"'\", 're', 'saying', 'that', 'the', 'actual', 'trial', 'proper', 'will', 'likely', 'start', 'in', 'may', 'or', 'june', 'and', 'easily', 'go', 'until', 'october', '.', 'a', 'prosecution', 'lawyer', 'talking', 'about', 'scheduling', 'issues', 'in', 'the', 'courtroom', 'today', 'raised', 'the', 'issue', 'of', 'some', 'dates', 'in', 'october', ',', 'and', 'nobody', 'batted', 'an', 'eye', '##lid', '.', 'so', 'we', \"'\", 're', 'looking', 'at', 'a', 'trial', 'that', 'could', 'easily', 'take', 'jury', 'alone', 'seems', 'like', 'a', 'her', '##cule', '##an', 'task', '.', 'james', 'holmes', 'was', 'in', 'the', 'courtroom', 'today', '.', 'you', 'have', 'seen', 'him', 'before', 'in', 'this', 'setting', '.', 'we', 'have', 'just', 'seen', 'pictures', 'of', 'him', 'with', 'the', 'orange', 'hair', 'and', 'kind', 'of', 'the', 'wild', 'eyes', '.', 'what', 'was', 'he', 'like', 'today', '?', '!', 'mary', '-', 'mac', '##car', '##thy', '#', 'dramatically', 'different', 'today', '.', 'now', ',', 'we', 'do', 'n', \"'\", 't', 'have', 'video', 'coming', 'out', 'of', 'the', 'courtroom', ',', 'because', 'the', 'televised', 'hearings', 'will', 'again', 'begin', 'with', 'the', 'hearings', 'proper', ',', 'not', 'the', 'jury', 'selection', '.', 'so', 'coming', 'out', 'of', 'the', 'courtroom', 'today', ',', 'we', 'only', 'have', 'sketches', '.', 'but', 'when', 'i', 'walked', 'into', 'the', 'courtroom', ',', 'like', 'many', 'of', 'the', 'media', 'who', 'were', 'there', ',', 'at', 'first', ',', 'we', 'did', 'n', \"'\", 't', 'actually', 'realize', 'that', 'one', 'of', 'the', 'people', 'sitting', 'up', 'front', 'with', 'the', 'defense', 'team', 'was', 'the', 'suspect', 'himself', 'because', 'he', 'looked', 'so', 'different', '.', 'in', 'the', 'past', ',', 'he', \"'\", 's', 'always', 'been', 'wearing', 'the', 'prison', 'ga', '##rb', ',', 'the', 'prison', 'jumps', '##uit', ',', 'the', 'bright', 'orange', ',', 'very', 'definitive', '##ly', 'points', 'him', 'out', 'as', 'a', 'suspect', ',', 'whereas', 'today', 'he', 'was', 'in', 'street', 'clothes', ',', 'civilian', 'clothes', ',', 'a', 'nice', 'sports', 'jacket', ',', 'a', 'sort', 'of', 'burgundy', '-', 'colored', 'dark', 'glasses', '.', 'and', 'he', 'looked', 'very', 'spruce', '##d', 'up', '.', 'and', 'there', 'was', 'also', 'a', 'dramatic', 'difference', 'in', 'his', 'demeanor', '.', 'for', 'the', 'first', 'time', 'that', 'i', 'have', 'seen', 'him', ',', 'he', 'did', 'n', \"'\", 't', 'look', 'dazed', ',', 'he', 'did', 'n', \"'\", 't', 'look', 'sort', 'of', 'out', 'of', 'it', '.', 'he', 'looked', 'at', '##ten', '##tive', ',', 'engaged', '.', 'in', 'fact', ',', 'he', 'was', 'chatting', 'with', 'one', 'of', 'the', 'lead', 'defense', 'lawyers', ',', 'tamara', 'brady', ',', 'sitting', 'right', 'next', 'to', 'him', ',', 'and', 'even', 'sort', 'of', 'laughing', 'and', 'light', '##hearted', ',', 'not', 'in', 'a', 'di', '##sta', '##ste', '##ful', 'way', ',', 'but', 'simply', 'looking', 'relaxed', 'for', 'the', 'first', 'time', 'ever', 'in', 'the', 'courtroom', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'well', ',', 'now', ',', 'he', 'has', 'admitted', 'to', 'the', 'shooting', 'itself', ',', 'but', 'is', 'claiming', 'insanity', '.', 'how', 'does', 'this', 'then', 'compare', ',', 'this', 'case', ',', 'so', 'far', 'to', 'other', 'big', 'mass', 'shooting', 'trials', 'that', 'we', 'have', 'seen', '?', '!', 'mary', '-', 'mac', '##car', '##thy', '#', 'well', ',', 'this', 'is', 'very', 'rare', '.', 'as', 'we', 'know', ',', 'mass', 'shooting', '##s', 'are', 'quite', 'common', ',', 'according', 'to', 'the', 'fbi', ',', 'about', '16', 'per', 'year', '.', 'this', 'one', 'is', 'rare', ',', 'both', 'in', 'the', 'magnitude', 'of', 'the', 'victims', ',', 'that', 'it', 'was', 'more', 'injured', 'beyond', 'that', ',', 'but', '58', 'people', 'suffering', 'from', 'serious', 'injuries', ',', 'and', 'it', \"'\", 's', 'simply', 'rare', 'that', 'the', 'shooter', 'survives', '.', 'many', 'people', 'are', 'comparing', 'this', 'case', 'to', 'the', 'boston', 'marathon', 'bombing', 'case', ',', 'because', ',', 'again', ',', 'crimes', 'that', 'affected', 'a', 'huge', 'community', 'and', 'in', 'which', 'the', 'per', '##pet', '##rator', 'survived', ',', 'because', ',', 'as', 'we', 'know', ',', 'in', 'most', 'shooting', '##s', ',', 'the', 'ass', '##ail', '##ant', 'is', 'either', 'shot', 'by', 'police', 'or', 'turns', 'the', 'gun', 'on', 'himself', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'and', ',', 'certainly', ',', 'that', 'was', 'the', 'case', 'in', 'col', '##umb', '##ine', 'case', ',', 'which', 'casts', 'a', 'shadow', 'over', 'this', 'case', '.', '!', 'mary', '-', 'mac', '##car', '##thy', '#', 'that', \"'\", 's', 'right', '.', 'it', \"'\", 's', 'important', 'to', 'note', 'that', ',', 'here', 'in', 'colorado', ',', 'that', ',', 'even', 'if', 'the', 'shooting', '##s', ',', 'these', 'mass', 'shooting', '##s', 'are', 'happening', 'everywhere', ',', 'that', 'these', '-', '-', 'both', 'the', 'col', '##umb', '##ine', 'high', 'school', 'shooting', 'in', '1999', 'and', 'the', 'aurora', 'theater', 'shooting', 'in', '2012', 'are', 'things', 'that', 'stand', 'out', 'for', 'the', 'community', 'here', '.', 'there', 'has', 'been', 'some', 'solidarity', 'among', 'the', 'victims', ',', 'helping', 'them', 'out', ',', 'helping', 'each', 'other', 'out', ',', 'the', 'families', 'even', 'joining', 'causes', ',', 'with', 'many', 'of', 'them', 'working', 'towards', 'things', 'like', 'gun', 'control', ',', 'can', 'say', ',', 'as', 'a', 'longtime', 'resident', 'of', 'colorado', ',', 'that', 'both', 'of', 'these', 'shooting', '##s', 'have', 'really', 'cast', 'a', 'pal', '##lor', 'over', 'our', 'state', 'and', 'it', \"'\", 's', 'something', 'that', \"'\", 's', 'really', 'coming', 'to', 'mind', 'for', 'the', 'victims', 'as', 'they', 'have', 'to', 'deal', 'with', 'this', 'process', '.', 'now', 'perhaps', 'another', 'eight', 'months', 'or', 'longer', 'of', 'the', 'trial', 'being', 'in', 'the', 'headlines', 'every', 'day', 'is', 'going', 'to', 'raise', 'a', 'lot', 'of', 'painful', 'memories', 'for', 'them', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'mary', 'mac', '##car', '##thy', 'of', 'feature', 'story', 'news', 'in', 'centennial', ',', 'colorado', ',', 'for', 'us', 'tonight', ',', 'thank', 'you', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'finally', 'tonight', ':', 'one', 'of', 'the', 'perpetual', 'challenges', 'facing', 'educators', 'is', 'finding', 'way', 'to', 'make', 'school', 'relevant', 'in', 'the', 'lives', 'of', 'students', '.', 'a', 'school', 'in', 'oakland', ',', 'california', ',', 'has', 'found', 'success', 'by', 'designing', 'lessons', 'making', 'those', 'links', 'much', 'closer', 'between', 'the', 'classroom', 'and', 'the', 'working', 'world', '.', 'the', '\"', 'news', '##ho', '##ur', '\"', \"'\", 's', 'april', 'brown', 'reports', 'as', 'part', 'of', 'our', 'american', 'graduate', 'project', '.', 'it', \"'\", 's', 'a', 'public', 'media', 'initiative', 'funded', 'by', 'the', 'corporation', 'for', 'public', 'broadcasting', '.', '!', 'woman', '#', 'because', 'there', \"'\", 's', 'a', 'lot', 'of', 'fluid', 'involved', 'in', 'giving', 'birth', '.', '!', 'april', '-', 'brown', '#', 'this', 'is', 'probably', 'not', 'to', 'be', 'getting', ',', 'learning', 'side', 'by', 'side', 'with', 'emergency', 'medical', 'technicians', 'in', 'training', 'about', 'how', 'to', 'deliver', 'a', 'baby', '.', '!', 'woman', '#', 'hopefully', ',', 'we', 'deliver', 'one', 'shoulder', 'first', '.', 'and', 'baby', 'is', 'all', 'the', 'way', 'out', ',', 'ok', '?', '!', 'april', '-', 'brown', '#', 'administering', 'cp', '##r', ',', 'the', 'best', 'way', 'to', 'apply', 'a', 'tour', '##ni', '##quet', ',', 'and', 'how', 'to', 'check', 'a', 'patient', \"'\", 's', 'blood', 'pressure', '.', 'getting', 'a', 'grasp', 'on', 'the', 'basics', 'of', 'the', 'medical', 'profession', 'is', 'a', 'large', 'part', 'of', 'the', 'curriculum', 'at', 'life', 'academy', 'of', 'health', 'and', 'bio', '##sc', '##ience', ',', 'a', 'sixth', '-', 'through', '-', '12th', 'grade', 'public', 'school', 'in', 'east', 'oakland', ',', 'one', 'of', 'the', 'poor', '##est', 'sections', 'in', 'the', 'city', '.', '!', 'preston', '-', 'thomas', ',', '-', 'pr', '#', 'ninety', '-', 'one', 'percent', 'of', 'our', 'kids', 'are', 'lc', '##ff', ',', 'or', 'free', 'and', 'reduced', 'lunch', ',', 'all', 'students', 'of', 'color', '.', 'and', 'most', 'of', 'them', ',', 'most', 'of', 'their', 'families', 'have', 'been', 'educated', 'up', 'to', 'about', 'sixth', 'grade', '.', '!', 'april', '-', 'brown', '#', 'preston', 'thomas', 'is', 'the', 'principal', 'at', 'life', 'academy', ',', 'which', 'was', 'founded', 'in', '2001', 'with', 'the', 'goal', 'of', 'preparing', 'low', '-', 'income', 'and', 'minority', 'students', 'for', 'health', 'science', 'careers', '.', 'the', 'school', 'uses', 'an', 'educational', 'approach', 'often', 'found', 'in', 'more', 'affluent', 'districts', 'known', 'as', 'linked', 'learning', 'workplace', 'environment', '.', '!', 'preston', '-', 'thomas', '#', 'we', 'have', 'kids', 'that', 'go', 'to', 'every', 'major', 'hospital', 'in', 'oakland', ',', 'and', 'they', 'are', 'starting', 'to', 'take', 'some', 'of', 'the', 'real', '-', 'world', 'practical', 'skills', 'that', 'they', 'are', 'learning', 'on', 'the', 'internship', '##s', ',', 'but', 'also', 'the', 'things', 'that', 'we', \"'\", 're', 'talking', 'about', 'in', 'class', ',', 'and', 'they', 'are', 'starting', 'to', 'synth', '##es', '##ize', 'that', 'into', 'the', 'vision', 'that', 'they', 'see', 'for', 'themselves', '.', '!', 'april', '-', 'brown', '#', 'the', 'nearly', '500', 'students', 'here', 'pick', 'one', 'of', 'three', 'career', 'pathways', ':', 'medicine', ',', 'mental', 'health', 'or', 'biotechnology', '.', 'many', 'of', 'them', ',', 'including', 'jorge', 'ruiz', ',', 'complete', 'a', 'two', '-', 'and', '-', 'a', '-', 'half', '-', 'year', 'internship', 'with', 'children', \"'\", 's', 'hospital', 'in', 'oakland', ',', 'rotating', 'through', 'different', 'departments', 'and', 'taking', 'field', 'trips', 'to', 'places', 'like', 'fast', 'response', ',', 'where', 'they', 'learned', 'about', 'what', 'happens', 'during', 'childbirth', '.', 'ruiz', ',', 'a', 'senior', ',', 'is', 'already', 'using', 'his', 'training', 'outside', 'the', 'classroom', ',', 'helping', 'his', 'mom', 'take', 'care', 'of', 'his', 'two', 'sisters', '.', '!', 'jorge', '-', 'ruiz', ',', '-', 'life', '-', 'a', '#', 'my', 'two', 'older', 'sisters', 'are', 'both', 'disabled', '.', 'my', 'oldest', ',', 'her', 'name', 'is', 'stephanie', '(', 'ph', ')', '.', 'she', 'has', 'cerebral', 'pal', '##sy', '.', 'and', 'then', 'my', 'second', 'older', 'sister', ',', 'lil', '##iana', '(', 'ph', ')', ',', 'has', 'cerebral', 'so', 'then', 'that', 'kind', 'of', 'pi', '##que', '##d', 'a', 'small', 'interest', '.', '!', 'april', '-', 'brown', '#', 'in', 'addition', 'to', 'the', 'linked', 'learning', 'approach', ',', 'life', 'academy', 'also', 'created', 'a', 'student', '-', 'centered', 'learning', 'environment', ',', 'which', 'means', 'the', 'school', 'work', 'is', 'collaborative', ',', 'challenging', ',', 'relevant', ',', 'and', 'connected', 'to', 'real', '-', 'life', 'situations', '.', '!', 'sun', '##eal', '-', 'ko', '##ll', '##uri', ',', '-', 'te', '#', 'these', 'are', 'the', 'social', 'security', ',', 'driver', \"'\", 's', 'license', 'and', 'a', 'work', 'permit', '.', 'cool', '.', '!', 'april', '-', 'brown', '#', 'this', 'year', ',', 'sun', '##eal', 'ko', '##ll', '##uri', '12th', 'grade', 'government', 'class', 'is', 'discussing', 'police', 'interactions', 'in', 'minority', 'communities', 'and', 'president', 'obama', \"'\", 's', 'executive', 'action', 'on', 'immigration', '.', 'those', 'are', 'issues', 'that', 'affect', 'many', 'of', 'his', 'students', ',', 'including', 'ruiz', ',', 'whose', 'father', 'was', 'deported', 'to', 'mexico', '.', '!', 'jorge', '-', 'ruiz', '#', 'when', 'he', 'was', 'deported', ',', 'i', 'never', 'really', 'had', 'a', 'chance', 'to', 'say', 'goodbye', '.', 'it', 'happened', 'in', 'the', 'morning', 'when', 'he', 'was', 'going', 'off', 'to', 'work', '.', 'and', 'then', 'the', 'story', 'is', 'that', 'some', 'people', 'pulled', 'over', ',', 'and', 'then', 'they', 'took', 'him', '.', '!', 'sun', '##eal', '-', 'ko', '##ll', '##uri', '#', 'when', 'you', 'engage', 'kids', 'in', 'what', \"'\", 's', 'happening', 'now', ',', 'it', \"'\", 's', 'so', 'much', 'more', 'real', 'and', 'so', 'much', 'more', 'alive', 'and', 'it', 'makes', 'them', 'so', 'much', 'more', 'excited', 'about', 'politics', 'in', 'a', 'developed', 'in', 'my', 'third', 'year', 'of', 'teaching', 'won', \"'\", 't', '.', '!', 'april', '-', 'brown', '#', 'but', 'it', \"'\", 's', 'the', 'school', \"'\", 's', 'primary', 'focus', 'on', 'health', 'careers', 'that', 'struck', 'a', 'personal', 'chord', 'with', 'junior', 'andrea', 'si', '##gal', '##a', '.', '!', 'andrea', '-', 'si', '##gal', '##a', ',', '-', 'li', '##f', '#', 'my', 'mom', ',', 'she', 'is', 'an', 'alcoholic', ',', 'and', 'she', \"'\", 's', 'been', 'an', 'alcoholic', 'for', 'her', 'whole', 'life', '.', 'and', 'she', \"'\", 's', 'been', 'hospitalized', '.', 'and', 'i', 'would', 'love', 'to', 'help', 'people', 'who', 'struggle', 'with', 'that', ',', 'too', '.', '!', 'april', '-', 'brown', '#', 'even', 'though', 'studies', 'have', 'shown', 'linked', 'learning', 'and', 'student', '-', 'centered', 'learning', 'can', 'improve', 'achievement', ',', 'the', 'challenges', 'of', 'everyday', 'life', 'can', 'sometimes', 'be', 'too', 'difficult', 'to', 'deal', 'with', '.', 'that', \"'\", 's', 'why', 'the', 'teachers', 'here', 'also', 'serve', 'as', 'advisers', 'who', 'stay', 'with', 'students', 'through', 'their', 'whole', 'career', 'at', 'life', 'academy', ',', 'helping', 'them', 'overcome', 'both', 'educational', 'and', 'personal', 'obstacles', '.', 'annie', 'hatch', 'teaches', 'humanities', 'and', 'english', 'and', 'is', 'andrea', 'si', '##gal', '##a', \"'\", 's', 'adviser', '.', '!', 'andrea', '-', 'si', '##gal', '##a', '#', 'every', 'time', 'she', 'sees', 'me', ',', 'like', ',', 'if', 'i', 'look', 'sad', 'or', 'anything', ',', 'she', 'asks', 'me', 'what', \"'\", 's', 'wrong', '.', 'and', 'i', 'can', '-', '-', 'i', 'know', 'i', 'can', 'count', 'on', 'her', 'and', 'tell', 'her', 'anything', 'and', 'she', 'wo', 'n', \"'\", 't', 'like', 'go', 'tell', 'anybody', 'else', 'where', 'she', \"'\", 's', 'felt', 'comfortable', 'expressing', 'her', 'struggles', ',', 'and', 'also', 'she', 'feels', 'really', 'supported', 'by', 'her', 'classmates', 'and', 'her', 'teachers', '.', '!', 'april', '-', 'brown', '#', 'students', 'like', 'si', '##gal', '##a', 'were', 'admitted', 'to', 'life', 'academy', 'through', 'an', 'open', 'enrollment', 'lottery', ',', 'and', 'like', 'other', 'oakland', 'schools', ',', 'its', 'financial', 'support', 'comes', 'largely', 'from', 'district', 'funds', '.', 'however', ',', 'results', 'here', 'have', 'been', 'remarkable', ',', 'in', 'a', 'city', 'where', 'only', 'half', 'of', 'african', '-', 'american', 'and', 'latino', 'students', 'graduate', 'from', 'high', 'school', 'and', 'even', 'fewer', 'enroll', 'in', 'college', '.', '!', 'diane', '-', 'fried', '##la', '##end', '##er', '#', 'students', 'are', 'being', 'prepared', 'for', 'college', 'and', 'career', 'together', '.', 'so', ',', 'in', 'the', 'past', ',', 'we', 'have', 'had', 'two', 'tracks', '.', 'you', 'can', 'be', 'college', 'prep', 'or', 'you', 'can', 'be', 'career', '.', 'and', 'that', 'was', 'auto', 'shop', 'or', 'home', 'ec', 'or', 'things', 'like', 'that', '.', 'but', 'now', 'we', \"'\", 're', 'talking', 'about', 'really', 'preparing', 'kids', 'for', 'professional', 'careers', 'where', 'the', 'two', 'are', 'integrated', '.', '!', 'april', '-', 'brown', '#', 'diane', 'fried', '##la', '##end', '##er', 'is', 'a', 'senior', 'researcher', 'at', 'stanford', 'university', '.', 'she', 'recently', 'oversaw', 'a', 'study', 'on', 'life', 'academy', '.', 'the', 'report', 'showed', 'that', 'among', 'oakland', \"'\", 's', 'public', 'high', 'schools', ',', 'life', 'academy', 'has', 'the', 'second', 'highest', 'rate', 'of', 'graduates', 'who', 'go', 'on', 'to', 'because', 'the', 'school', 'makes', 'the', 'benefits', 'of', 'learning', 'clear', 'to', 'students', ',', 'even', 'though', 'all', 'the', 'options', 'in', 'a', 'regular', 'school', ',', 'like', 'sports', 'and', 'many', 'elect', '##ives', ',', 'are', 'n', \"'\", 't', 'offered', 'here', '.', 'that', 'was', 'originally', 'a', 'concern', 'for', 'si', '##gal', '##a', '.', '!', 'andrea', '-', 'si', '##gal', '##a', '#', 'i', 'do', 'miss', 'that', '.', 'like', ',', 'i', 'used', 'to', 'be', 'in', 'theater', '.', 'i', 'used', 'to', 'love', 'acting', '.', 'but', 'now', 'that', 'i', 'know', 'what', 'i', 'want', 'to', 'do', ',', 'i', 'know', 'that', 'i', 'want', 'to', 'be', 'a', 'doctor', 'or', 'somewhere', 'in', 'the', 'medical', 'field', ',', 'i', 'do', 'n', \"'\", 't', 'miss', 'it', 'as', 'much', '.', '!', 'april', '-', 'brown', '#', 'the', 'stanford', 'report', 'also', 'notes', 'two', 'other', 'elements', 'that', 'have', 'helped', 'students', 'succeed', 'here', ':', 'engaging', 'parents', 'and', 'building', 'partnerships', 'within', 'the', 'community', '.', 'life', 'academy', 'holds', 'weekly', 'meetings', 'for', 'parents', 'to', 'get', 'updates', 'on', 'how', 'their', 'kids', 'are', 'doing', 'and', 'it', 'has', 'several', 'health', 'and', 'bio', '##tech', 'businesses', 'in', 'oakland', 'now', 'participating', ',', 'providing', 'both', 'internship', '##s', 'and', 'career', 'counseling', 'to', 'students', '.', 'shan', '##ta', 'ram', '##deh', '##oll', 'manages', 'the', 'student', 'internship', 'program', 'at', 'children', \"'\", 's', 'hospital', ',', 'and', 'she', 'believes', 'her', 'program', 'is', 'also', 'working', 'to', 'grow', 'a', 'more', 'diverse', 'work', 'force', 'of', 'local', 'care', '##gi', '##vers', '.', '!', 'shan', '##ta', '-', 'ram', '##deh', '##oll', ',', 'a', 'missed', 'opportunity', '.', 'just', 'imagine', 'for', 'doctors', 'who', 'go', 'on', 'to', 'medical', 'school', ',', 'what', 'these', 'kids', 'are', 'doing', 'today', ',', 'it', 'takes', 'two', 'years', 'in', 'medical', 'school', 'before', 'they', 'would', 'start', '-', '-', 'to', 'start', 'shadow', '##ing', 'medical', 'providers', 'or', 'doing', 'certain', 'things', 'that', 'these', 'students', 'are', 'doing', 'today', '.', 'so', 'it', 'opens', 'up', 'a', 'new', 'world', 'for', 'them', '.', '!', 'april', '-', 'brown', '#', 'but', 'stanford', \"'\", 's', 'diane', 'fried', '##la', '##end', '##er', 'warns', 'against', 'trying', 'to', 'create', 'exact', 'replica', '##s', ',', 'because', 'it', \"'\", 's', 'taken', 'years', 'to', 'get', 'buy', '-', 'in', 'from', 'everyone', 'involved', '.', '!', 'diane', '-', 'fried', '##la', '##end', '##er', '#', 'those', 'core', 'ideas', 'can', 'definitely', 'and', 'should', 'definitely', 'be', 'replicate', '##d', 'in', 'other', 'places', ',', 'but', 'to', 'take', 'it', 'as', 'a', 'kind', 'of', 'cookie', '-', 'cutter', 'model', 'and', 'put', 'it', 'down', 'in', 'another', 'site', 'wo', 'n', \"'\", 't', '-', '-', 'wo', 'n', \"'\", 't', 'work', '.', 'there', 'wo', 'n', \"'\", 't', 'be', 'ownership', '.', 'it', 'wo', 'n', \"'\", 't', 'be', 'connected', 'to', 'that', 'community', '.', '!', 'april', '-', 'brown', '#', 'but', 'in', 'east', 'oakland', ',', 'principal', 'thomas', 'has', 'found', 'these', 'opportunities', 'are', 'mo', '##tiv', '##ating', 'kids', 'to', 'continue', 'their', 'education', ',', 'so', 'they', 'can', 'get', 'the', 'jobs', 'they', 'really', 'want', '.', '!', 'preston', '-', 'thomas', '#', 'we', 'really', 'want', 'students', 'and', 'families', 'to', 'have', 'the', 'opportunity', 'to', 'attend', 'college', ',', 'and', 'not', 'just', ',', 'you', 'have', 'at', 'it', 'and', 'make', 'it', 'through', 'those', 'four', 'years', 'of', 'college', ',', 'which', 'are', 'really', 'rigorous', 'and', 'really', 'challenging', 'for', 'students', 'that', 'do', 'n', \"'\", 't', 'have', '-', '-', 'they', 'do', 'n', \"'\", 't', 'have', 'any', 'framework', '##s', 'for', 'understanding', 'what', 'college', 'is', 'going', 'to', 'be', 'like', 'because', 'they', 'are', 'the', 'first', '.', '!', 'april', '-', 'brown', '#', 'thus', 'far', ',', 'california', 'has', 'nine', 'districts', 'across', 'the', 'state', 'using', 'linked', 'learning', '.', 'and', 'the', 'city', 'of', 'oakland', 'recently', 'approved', 'a', 'measure', 'that', 'will', 'help', 'create', 'more', 'linked', 'learning', 'opportunities', 'in', 'its', 'schools', 'over', 'the', 'next', '10', 'years', '.', 'for', 'the', '\"', 'pbs', 'news', '##ho', '##ur', ',', '\"', 'i', \"'\", 'm', 'april', 'brown', 'in', 'oakland', ',', 'california', '.', '!', 'gwen', '-', 'if', '##ill', '#', 'again', ',', 'the', 'major', 'developments', 'of', 'the', 'day', '.', 'president', 'obama', 'prepares', 'to', 'lay', 'out', 'his', 'view', 'of', 'the', 'state', 'of', 'the', 'union', '.', 'in', 'excerpts', 'released', 'in', 'advance', ',', 'he', 'asserts', 'america', 'is', 'turning', 'the', 'page', 'after', 'years', 'of', 'war', 'and', 'recession', '.', 'and', 'that', \"'\", 's', 'the', '\"', 'news', '##ho', '##ur', '\"', 'for', 'now', '.', 'i', \"'\", 'm', 'gwen', 'if', '##ill', '.', '!', 'judy', '-', 'wood', '##ruff', '#', 'and', 'i', \"'\", 'm', 'judy', 'wood', '##ruff', '.', 'join', 'us', 'online', 'and', 'tonight', 'at', '9', ':', '00', 'p', '.', 'm', '.', 'eastern', 'for', 'special', '\"', 'news', '##ho', '##ur', '\"', 'coverage', 'of', 'the', 'state', 'of', 'the', 'then', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8bac1ede-2b0e-4c65-8ac5-8aa0580e8a33",
        "id": "tzCK_NakSciw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "evaluate_lstm = model_lstm.evaluate(input_ids, labels, batch_size=batch_size)\n",
        "evaluate_lstm"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r26/26 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5085266828536987, 0.9230769276618958]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Ojv5xJDSciy"
      },
      "source": [
        "###### BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYJ68ub9Sci0",
        "colab": {}
      },
      "source": [
        "train_labels = train_labels.astype(np.int64)\n",
        "validation_labels = validation_labels.astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6d17LqqoSci1",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Swlgw4fkSci4",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e0a348c-92b5-4994-8e59-329603b6e323",
        "id": "QjHYI11qSci7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-COa9LorSci8",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "npmoLbXOSci-",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ne0uT1-DScjB",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "disDeA-mScjD",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SilXXFhZScjF",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "677ac860-e2c2-4055-84a5-40c607f75dc8",
        "id": "5o93H763ScjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:00:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:00:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epcoh took: 0:00:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:00:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "14a68849-01e7-4a8b-cd9a-e879dc400656",
        "id": "SFO9gC6dScjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5166027386983235,\n",
              " 0.4744277199109395,\n",
              " 0.46440380613009136,\n",
              " 0.4312203774849574]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f33ca34e-0ad1-4a58-c7c4-5e7d07720e93",
        "id": "B1N6nWgxScjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(period4_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f9c193a8-eb49-4300-f530-9d926f17d6cb",
        "id": "vgGWslqpScjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(period4_test.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = period4_test.sentence.values\n",
        "labels = period4_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length=512\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels.astype(np.int64))\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 26\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "be82c493-7378-4333-9d8f-a03b92c186b9",
        "id": "rLb4E_DpScjO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 26 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "42bdba46-b053-476a-cfab-111717e26110",
        "id": "NSG1up_0ScjQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (period4_test.label.sum(), len(period4_test.label), (period4_test.label.sum() / len(period4_test.label) * 100.0)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 5 of 26 (19.23%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXi_Ygv1etat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68eb691b-fa55-4207-da55-66699bed180e"
      },
      "source": [
        "true_labels"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
              "        0, 0, 1, 0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a011b4f7-4edd-460d-ecef-6533daa2dc28",
        "id": "_pLpEbVeScjS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3a017cce-5285-4b64-c002-f9cae5112246",
        "id": "F9WfOcIfScjT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b27fb22-3700-4033-d00f-4eadca211718",
        "id": "Zk5XwP5vScjU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e103e78e-5f3f-44f8-fc4a-c909e23d2bf7",
        "id": "LmTVZ6-jScjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './period4/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./period4/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./period4/vocab.txt',\n",
              " './period4/special_tokens_map.json',\n",
              " './period4/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7G7JbAt9sn",
        "colab_type": "text"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XdVsM_cL_3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqiVuxFuLLvu",
        "colab_type": "code",
        "outputId": "d57de3e4-0e95-4ae3-99e9-353541795531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_trump --do_eval --eval_data_file=/content/test_text_trump --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/08/2020 07:38:57 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/08/2020 07:38:58 - INFO - filelock -   Lock 140686358784712 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/08/2020 07:38:58 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3_kyg7wp\n",
            "Downloading: 100% 224/224 [00:00<00:00, 201kB/s]\n",
            "03/08/2020 07:38:58 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/08/2020 07:38:58 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/08/2020 07:38:58 - INFO - filelock -   Lock 140686358784712 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/08/2020 07:38:58 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/08/2020 07:38:58 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/08/2020 07:38:59 - INFO - filelock -   Lock 140686731561000 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/08/2020 07:38:59 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4xdq222k\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.09MB/s]\n",
            "03/08/2020 07:39:00 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/08/2020 07:39:00 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/08/2020 07:39:00 - INFO - filelock -   Lock 140686731561000 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/08/2020 07:39:00 - INFO - filelock -   Lock 140686731561000 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/08/2020 07:39:00 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp72fltjnc\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.33MB/s]\n",
            "03/08/2020 07:39:01 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/08/2020 07:39:01 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/08/2020 07:39:01 - INFO - filelock -   Lock 140686731561000 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/08/2020 07:39:01 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/08/2020 07:39:01 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/08/2020 07:39:01 - INFO - filelock -   Lock 140686358751104 acquired on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/08/2020 07:39:01 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8b4s9lcx\n",
            "Downloading: 100% 548M/548M [00:19<00:00, 28.2MB/s]\n",
            "03/08/2020 07:39:21 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/08/2020 07:39:21 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/08/2020 07:39:21 - INFO - filelock -   Lock 140686358751104 released on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/08/2020 07:39:21 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/08/2020 07:39:34 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/test_text_trump', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_gpt_trump', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/train_text_trump', warmup_steps=0, weight_decay=0.0)\n",
            "03/08/2020 07:39:34 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/08/2020 07:39:37 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_train_text_trump\n",
            "03/08/2020 07:39:37 - INFO - __main__ -   ***** Running training *****\n",
            "03/08/2020 07:39:37 - INFO - __main__ -     Num examples = 669\n",
            "03/08/2020 07:39:37 - INFO - __main__ -     Num Epochs = 1\n",
            "03/08/2020 07:39:37 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "03/08/2020 07:39:37 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "03/08/2020 07:39:37 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/08/2020 07:39:37 - INFO - __main__ -     Total optimization steps = 669\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/669 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/669 [00:00<05:19,  2.09it/s]\u001b[A\n",
            "Iteration:   0% 2/669 [00:00<04:27,  2.50it/s]\u001b[A\n",
            "Iteration:   0% 3/669 [00:00<03:51,  2.88it/s]\u001b[A\n",
            "Iteration:   1% 4/669 [00:01<03:26,  3.22it/s]\u001b[A\n",
            "Iteration:   1% 5/669 [00:01<03:09,  3.51it/s]\u001b[A\n",
            "Iteration:   1% 6/669 [00:01<02:57,  3.74it/s]\u001b[A\n",
            "Iteration:   1% 7/669 [00:01<02:48,  3.93it/s]\u001b[A\n",
            "Iteration:   1% 8/669 [00:02<02:42,  4.06it/s]\u001b[A\n",
            "Iteration:   1% 9/669 [00:02<02:37,  4.18it/s]\u001b[A\n",
            "Iteration:   1% 10/669 [00:02<02:35,  4.25it/s]\u001b[A\n",
            "Iteration:   2% 11/669 [00:02<02:32,  4.31it/s]\u001b[A\n",
            "Iteration:   2% 12/669 [00:02<02:30,  4.35it/s]\u001b[A\n",
            "Iteration:   2% 13/669 [00:03<02:30,  4.37it/s]\u001b[A\n",
            "Iteration:   2% 14/669 [00:03<02:28,  4.40it/s]\u001b[A\n",
            "Iteration:   2% 15/669 [00:03<02:28,  4.41it/s]\u001b[A\n",
            "Iteration:   2% 16/669 [00:03<02:28,  4.41it/s]\u001b[A\n",
            "Iteration:   3% 17/669 [00:04<02:27,  4.41it/s]\u001b[A\n",
            "Iteration:   3% 18/669 [00:04<02:27,  4.43it/s]\u001b[A\n",
            "Iteration:   3% 19/669 [00:04<02:26,  4.43it/s]\u001b[A\n",
            "Iteration:   3% 20/669 [00:04<02:26,  4.44it/s]\u001b[A\n",
            "Iteration:   3% 21/669 [00:04<02:26,  4.43it/s]\u001b[A\n",
            "Iteration:   3% 22/669 [00:05<02:25,  4.45it/s]\u001b[A\n",
            "Iteration:   3% 23/669 [00:05<02:25,  4.45it/s]\u001b[A\n",
            "Iteration:   4% 24/669 [00:05<02:24,  4.45it/s]\u001b[A\n",
            "Iteration:   4% 25/669 [00:05<02:26,  4.38it/s]\u001b[A\n",
            "Iteration:   4% 26/669 [00:06<02:24,  4.46it/s]\u001b[A\n",
            "Iteration:   4% 27/669 [00:06<02:24,  4.45it/s]\u001b[A\n",
            "Iteration:   4% 28/669 [00:06<02:24,  4.44it/s]\u001b[A\n",
            "Iteration:   4% 29/669 [00:06<02:24,  4.42it/s]\u001b[A\n",
            "Iteration:   4% 30/669 [00:07<02:23,  4.45it/s]\u001b[A\n",
            "Iteration:   5% 31/669 [00:07<02:23,  4.45it/s]\u001b[A\n",
            "Iteration:   5% 32/669 [00:07<02:23,  4.45it/s]\u001b[A\n",
            "Iteration:   5% 33/669 [00:07<02:23,  4.45it/s]\u001b[A\n",
            "Iteration:   5% 34/669 [00:07<02:23,  4.43it/s]\u001b[A\n",
            "Iteration:   5% 35/669 [00:08<02:22,  4.44it/s]\u001b[A\n",
            "Iteration:   5% 36/669 [00:08<02:22,  4.44it/s]\u001b[A\n",
            "Iteration:   6% 37/669 [00:08<02:22,  4.44it/s]\u001b[A\n",
            "Iteration:   6% 38/669 [00:08<02:22,  4.44it/s]\u001b[A\n",
            "Iteration:   6% 39/669 [00:09<02:21,  4.44it/s]\u001b[A\n",
            "Iteration:   6% 40/669 [00:09<02:21,  4.44it/s]\u001b[A\n",
            "Iteration:   6% 41/669 [00:09<02:21,  4.45it/s]\u001b[A\n",
            "Iteration:   6% 42/669 [00:09<02:22,  4.41it/s]\u001b[A\n",
            "Iteration:   6% 43/669 [00:09<02:20,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 44/669 [00:10<02:20,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 45/669 [00:10<02:20,  4.45it/s]\u001b[A\n",
            "Iteration:   7% 46/669 [00:10<02:20,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 47/669 [00:10<02:19,  4.46it/s]\u001b[A\n",
            "Iteration:   7% 48/669 [00:11<02:19,  4.45it/s]\u001b[A\n",
            "Iteration:   7% 49/669 [00:11<02:19,  4.45it/s]\u001b[A\n",
            "Iteration:   7% 50/669 [00:11<02:19,  4.44it/s]\u001b[A\n",
            "Iteration:   8% 51/669 [00:11<02:18,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 52/669 [00:11<02:18,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 53/669 [00:12<02:18,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 54/669 [00:12<02:18,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 55/669 [00:12<02:18,  4.43it/s]\u001b[A\n",
            "Iteration:   8% 56/669 [00:12<02:18,  4.44it/s]\u001b[A\n",
            "Iteration:   9% 57/669 [00:13<02:17,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 58/669 [00:13<02:17,  4.44it/s]\u001b[A\n",
            "Iteration:   9% 59/669 [00:13<02:18,  4.41it/s]\u001b[A\n",
            "Iteration:   9% 60/669 [00:13<02:16,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 61/669 [00:13<02:16,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 62/669 [00:14<02:16,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 63/669 [00:14<02:16,  4.44it/s]\u001b[A\n",
            "Iteration:  10% 64/669 [00:14<02:15,  4.46it/s]\u001b[A\n",
            "Iteration:  10% 65/669 [00:14<02:15,  4.46it/s]\u001b[A\n",
            "Iteration:  10% 66/669 [00:15<02:16,  4.43it/s]\u001b[A\n",
            "Iteration:  10% 67/669 [00:15<02:15,  4.43it/s]\u001b[A\n",
            "Iteration:  10% 68/669 [00:15<02:15,  4.45it/s]\u001b[A\n",
            "Iteration:  10% 69/669 [00:15<02:14,  4.45it/s]\u001b[A\n",
            "Iteration:  10% 70/669 [00:16<02:14,  4.45it/s]\u001b[A\n",
            "Iteration:  11% 71/669 [00:16<02:14,  4.45it/s]\u001b[A\n",
            "Iteration:  11% 72/669 [00:16<02:14,  4.43it/s]\u001b[A\n",
            "Iteration:  11% 73/669 [00:16<02:14,  4.44it/s]\u001b[A\n",
            "Iteration:  11% 74/669 [00:16<02:14,  4.43it/s]\u001b[A\n",
            "Iteration:  11% 75/669 [00:17<02:14,  4.41it/s]\u001b[A\n",
            "Iteration:  11% 76/669 [00:17<02:13,  4.44it/s]\u001b[A\n",
            "Iteration:  12% 77/669 [00:17<02:13,  4.44it/s]\u001b[A\n",
            "Iteration:  12% 78/669 [00:17<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 79/669 [00:18<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 80/669 [00:18<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 81/669 [00:18<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 82/669 [00:18<02:12,  4.44it/s]\u001b[A\n",
            "Iteration:  12% 83/669 [00:18<02:11,  4.45it/s]\u001b[A\n",
            "Iteration:  13% 84/669 [00:19<02:12,  4.43it/s]\u001b[A\n",
            "Iteration:  13% 85/669 [00:19<02:11,  4.46it/s]\u001b[A\n",
            "Iteration:  13% 86/669 [00:19<02:10,  4.46it/s]\u001b[A\n",
            "Iteration:  13% 87/669 [00:19<02:10,  4.45it/s]\u001b[A\n",
            "Iteration:  13% 88/669 [00:20<02:10,  4.45it/s]\u001b[A\n",
            "Iteration:  13% 89/669 [00:20<02:10,  4.43it/s]\u001b[A\n",
            "Iteration:  13% 90/669 [00:20<02:10,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 91/669 [00:20<02:10,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 92/669 [00:20<02:09,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 93/669 [00:21<02:09,  4.43it/s]\u001b[A\n",
            "Iteration:  14% 94/669 [00:21<02:09,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 95/669 [00:21<02:09,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 96/669 [00:21<02:08,  4.45it/s]\u001b[A\n",
            "Iteration:  14% 97/669 [00:22<02:09,  4.43it/s]\u001b[A\n",
            "Iteration:  15% 98/669 [00:22<02:08,  4.45it/s]\u001b[A\n",
            "Iteration:  15% 99/669 [00:22<02:07,  4.45it/s]\u001b[A\n",
            "Iteration:  15% 100/669 [00:22<02:07,  4.45it/s]\u001b[A\n",
            "Iteration:  15% 101/669 [00:22<02:07,  4.44it/s]\u001b[A\n",
            "Iteration:  15% 102/669 [00:23<02:07,  4.46it/s]\u001b[A\n",
            "Iteration:  15% 103/669 [00:23<02:07,  4.45it/s]\u001b[A\n",
            "Iteration:  16% 104/669 [00:23<02:07,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 105/669 [00:23<02:06,  4.45it/s]\u001b[A\n",
            "Iteration:  16% 106/669 [00:24<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 107/669 [00:24<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 108/669 [00:24<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 109/669 [00:24<02:06,  4.43it/s]\u001b[A\n",
            "Iteration:  16% 110/669 [00:25<02:06,  4.42it/s]\u001b[A\n",
            "Iteration:  17% 111/669 [00:25<02:05,  4.43it/s]\u001b[A\n",
            "Iteration:  17% 112/669 [00:25<02:05,  4.44it/s]\u001b[A\n",
            "Iteration:  17% 113/669 [00:25<02:05,  4.44it/s]\u001b[A\n",
            "Iteration:  17% 114/669 [00:25<02:06,  4.40it/s]\u001b[A\n",
            "Iteration:  17% 115/669 [00:26<02:04,  4.45it/s]\u001b[A\n",
            "Iteration:  17% 116/669 [00:26<02:04,  4.45it/s]\u001b[A\n",
            "Iteration:  17% 117/669 [00:26<02:04,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 118/669 [00:26<02:04,  4.41it/s]\u001b[A\n",
            "Iteration:  18% 119/669 [00:27<02:03,  4.46it/s]\u001b[A\n",
            "Iteration:  18% 120/669 [00:27<02:03,  4.45it/s]\u001b[A\n",
            "Iteration:  18% 121/669 [00:27<02:03,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 122/669 [00:27<02:03,  4.45it/s]\u001b[A\n",
            "Iteration:  18% 123/669 [00:27<02:03,  4.43it/s]\u001b[A\n",
            "Iteration:  19% 124/669 [00:28<02:02,  4.43it/s]\u001b[A\n",
            "Iteration:  19% 125/669 [00:28<02:03,  4.41it/s]\u001b[A\n",
            "Iteration:  19% 126/669 [00:28<02:02,  4.42it/s]\u001b[A\n",
            "Iteration:  19% 127/669 [00:28<02:02,  4.42it/s]\u001b[A\n",
            "Iteration:  19% 128/669 [00:29<02:02,  4.43it/s]\u001b[A\n",
            "Iteration:  19% 129/669 [00:29<02:01,  4.44it/s]\u001b[A\n",
            "Iteration:  19% 130/669 [00:29<02:01,  4.44it/s]\u001b[A\n",
            "Iteration:  20% 131/669 [00:29<02:01,  4.41it/s]\u001b[A\n",
            "Iteration:  20% 132/669 [00:29<02:01,  4.43it/s]\u001b[A\n",
            "Iteration:  20% 133/669 [00:30<02:00,  4.43it/s]\u001b[A\n",
            "Iteration:  20% 134/669 [00:30<02:01,  4.39it/s]\u001b[A\n",
            "Iteration:  20% 135/669 [00:30<02:00,  4.43it/s]\u001b[A\n",
            "Iteration:  20% 136/669 [00:30<01:59,  4.44it/s]\u001b[A\n",
            "Iteration:  20% 137/669 [00:31<01:59,  4.44it/s]\u001b[A\n",
            "Iteration:  21% 138/669 [00:31<01:59,  4.44it/s]\u001b[A\n",
            "Iteration:  21% 139/669 [00:31<01:59,  4.42it/s]\u001b[A\n",
            "Iteration:  21% 140/669 [00:31<01:58,  4.45it/s]\u001b[A\n",
            "Iteration:  21% 141/669 [00:31<01:58,  4.45it/s]\u001b[A\n",
            "Iteration:  21% 142/669 [00:32<01:58,  4.45it/s]\u001b[A\n",
            "Iteration:  21% 143/669 [00:32<01:59,  4.39it/s]\u001b[A\n",
            "Iteration:  22% 144/669 [00:32<01:57,  4.45it/s]\u001b[A\n",
            "Iteration:  22% 145/669 [00:32<01:57,  4.45it/s]\u001b[A\n",
            "Iteration:  22% 146/669 [00:33<01:57,  4.45it/s]\u001b[A\n",
            "Iteration:  22% 147/669 [00:33<01:57,  4.45it/s]\u001b[A\n",
            "Iteration:  22% 148/669 [00:33<01:57,  4.42it/s]\u001b[A\n",
            "Iteration:  22% 149/669 [00:33<01:57,  4.43it/s]\u001b[A\n",
            "Iteration:  22% 150/669 [00:34<01:56,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 151/669 [00:34<01:56,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 152/669 [00:34<01:57,  4.42it/s]\u001b[A\n",
            "Iteration:  23% 153/669 [00:34<01:56,  4.42it/s]\u001b[A\n",
            "Iteration:  23% 154/669 [00:34<01:56,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 155/669 [00:35<01:55,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 156/669 [00:35<01:56,  4.41it/s]\u001b[A\n",
            "Iteration:  23% 157/669 [00:35<01:55,  4.43it/s]\u001b[A\n",
            "Iteration:  24% 158/669 [00:35<01:55,  4.43it/s]\u001b[A\n",
            "Iteration:  24% 159/669 [00:36<01:54,  4.44it/s]\u001b[A\n",
            "Iteration:  24% 160/669 [00:36<01:55,  4.41it/s]\u001b[A\n",
            "Iteration:  24% 161/669 [00:36<01:54,  4.43it/s]\u001b[A\n",
            "Iteration:  24% 162/669 [00:36<01:54,  4.43it/s]\u001b[A\n",
            "Iteration:  24% 163/669 [00:36<01:54,  4.44it/s]\u001b[A\n",
            "Iteration:  25% 164/669 [00:37<01:54,  4.42it/s]\u001b[A\n",
            "Iteration:  25% 165/669 [00:37<01:53,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 166/669 [00:37<01:53,  4.43it/s]\u001b[A\n",
            "Iteration:  25% 167/669 [00:37<01:52,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 168/669 [00:38<01:53,  4.40it/s]\u001b[A\n",
            "Iteration:  25% 169/669 [00:38<01:52,  4.46it/s]\u001b[A\n",
            "Iteration:  25% 170/669 [00:38<01:52,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 171/669 [00:38<01:51,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 172/669 [00:38<01:51,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 173/669 [00:39<01:52,  4.42it/s]\u001b[A\n",
            "Iteration:  26% 174/669 [00:39<01:52,  4.42it/s]\u001b[A\n",
            "Iteration:  26% 175/669 [00:39<01:51,  4.43it/s]\u001b[A\n",
            "Iteration:  26% 176/669 [00:39<01:51,  4.44it/s]\u001b[A\n",
            "Iteration:  26% 177/669 [00:40<01:50,  4.44it/s]\u001b[A\n",
            "Iteration:  27% 178/669 [00:40<01:50,  4.44it/s]\u001b[A\n",
            "Iteration:  27% 179/669 [00:40<01:50,  4.44it/s]\u001b[A\n",
            "Iteration:  27% 180/669 [00:40<01:50,  4.43it/s]\u001b[A\n",
            "Iteration:  27% 181/669 [00:41<01:50,  4.42it/s]\u001b[A\n",
            "Iteration:  27% 182/669 [00:41<01:49,  4.45it/s]\u001b[A\n",
            "Iteration:  27% 183/669 [00:41<01:49,  4.45it/s]\u001b[A\n",
            "Iteration:  28% 184/669 [00:41<01:49,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 185/669 [00:41<01:49,  4.41it/s]\u001b[A\n",
            "Iteration:  28% 186/669 [00:42<01:48,  4.47it/s]\u001b[A\n",
            "Iteration:  28% 187/669 [00:42<01:47,  4.46it/s]\u001b[A\n",
            "Iteration:  28% 188/669 [00:42<01:48,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 189/669 [00:42<01:47,  4.46it/s]\u001b[A\n",
            "Iteration:  28% 190/669 [00:43<01:48,  4.43it/s]\u001b[A\n",
            "Iteration:  29% 191/669 [00:43<01:47,  4.44it/s]\u001b[A\n",
            "Iteration:  29% 192/669 [00:43<01:47,  4.44it/s]\u001b[A\n",
            "Iteration:  29% 193/669 [00:43<01:47,  4.43it/s]\u001b[A\n",
            "Iteration:  29% 194/669 [00:43<01:47,  4.42it/s]\u001b[A\n",
            "Iteration:  29% 195/669 [00:44<01:46,  4.44it/s]\u001b[A\n",
            "Iteration:  29% 196/669 [00:44<01:46,  4.44it/s]\u001b[A\n",
            "Iteration:  29% 197/669 [00:44<01:46,  4.43it/s]\u001b[A\n",
            "Iteration:  30% 198/669 [00:44<01:46,  4.42it/s]\u001b[A\n",
            "Iteration:  30% 199/669 [00:45<01:45,  4.44it/s]\u001b[A\n",
            "Iteration:  30% 200/669 [00:45<01:45,  4.44it/s]\u001b[A\n",
            "Iteration:  30% 201/669 [00:45<01:45,  4.44it/s]\u001b[A\n",
            "Iteration:  30% 202/669 [00:45<01:46,  4.37it/s]\u001b[A\n",
            "Iteration:  30% 203/669 [00:45<01:44,  4.45it/s]\u001b[A\n",
            "Iteration:  30% 204/669 [00:46<01:44,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 205/669 [00:46<01:44,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 206/669 [00:46<01:44,  4.42it/s]\u001b[A\n",
            "Iteration:  31% 207/669 [00:46<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 208/669 [00:47<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 209/669 [00:47<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 210/669 [00:47<01:43,  4.42it/s]\u001b[A\n",
            "Iteration:  32% 211/669 [00:47<01:43,  4.43it/s]\u001b[A\n",
            "Iteration:  32% 212/669 [00:48<01:42,  4.45it/s]\u001b[A\n",
            "Iteration:  32% 213/669 [00:48<01:42,  4.45it/s]\u001b[A\n",
            "Iteration:  32% 214/669 [00:48<01:42,  4.45it/s]\u001b[A\n",
            "Iteration:  32% 215/669 [00:48<01:42,  4.44it/s]\u001b[A\n",
            "Iteration:  32% 216/669 [00:48<01:42,  4.44it/s]\u001b[A\n",
            "Iteration:  32% 217/669 [00:49<01:41,  4.44it/s]\u001b[A\n",
            "Iteration:  33% 218/669 [00:49<01:41,  4.44it/s]\u001b[A\n",
            "Iteration:  33% 219/669 [00:49<01:41,  4.44it/s]\u001b[A\n",
            "Iteration:  33% 220/669 [00:49<01:40,  4.45it/s]\u001b[A\n",
            "Iteration:  33% 221/669 [00:50<01:41,  4.42it/s]\u001b[A\n",
            "Iteration:  33% 222/669 [00:50<01:41,  4.42it/s]\u001b[A\n",
            "Iteration:  33% 223/669 [00:50<01:41,  4.41it/s]\u001b[A\n",
            "Iteration:  33% 224/669 [00:50<01:40,  4.43it/s]\u001b[A\n",
            "Iteration:  34% 225/669 [00:50<01:40,  4.43it/s]\u001b[A\n",
            "Iteration:  34% 226/669 [00:51<01:39,  4.44it/s]\u001b[A\n",
            "Iteration:  34% 227/669 [00:51<01:39,  4.42it/s]\u001b[A\n",
            "Iteration:  34% 228/669 [00:51<01:39,  4.42it/s]\u001b[A\n",
            "Iteration:  34% 229/669 [00:51<01:38,  4.46it/s]\u001b[A\n",
            "Iteration:  34% 230/669 [00:52<01:38,  4.45it/s]\u001b[A\n",
            "Iteration:  35% 231/669 [00:52<01:38,  4.43it/s]\u001b[A\n",
            "Iteration:  35% 232/669 [00:52<01:37,  4.46it/s]\u001b[A\n",
            "Iteration:  35% 233/669 [00:52<01:37,  4.45it/s]\u001b[A\n",
            "Iteration:  35% 234/669 [00:52<01:37,  4.45it/s]\u001b[A\n",
            "Iteration:  35% 235/669 [00:53<01:37,  4.45it/s]\u001b[A\n",
            "Iteration:  35% 236/669 [00:53<01:37,  4.44it/s]\u001b[A\n",
            "Iteration:  35% 237/669 [00:53<01:36,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 238/669 [00:53<01:36,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 239/669 [00:54<01:36,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 240/669 [00:54<01:36,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 241/669 [00:54<01:36,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 242/669 [00:54<01:35,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 243/669 [00:54<01:35,  4.45it/s]\u001b[A\n",
            "Iteration:  36% 244/669 [00:55<01:35,  4.43it/s]\u001b[A\n",
            "Iteration:  37% 245/669 [00:55<01:35,  4.45it/s]\u001b[A\n",
            "Iteration:  37% 246/669 [00:55<01:35,  4.45it/s]\u001b[A\n",
            "Iteration:  37% 247/669 [00:55<01:34,  4.45it/s]\u001b[A\n",
            "Iteration:  37% 248/669 [00:56<01:34,  4.45it/s]\u001b[A\n",
            "Iteration:  37% 249/669 [00:56<01:35,  4.41it/s]\u001b[A\n",
            "Iteration:  37% 250/669 [00:56<01:34,  4.43it/s]\u001b[A\n",
            "Iteration:  38% 251/669 [00:56<01:34,  4.44it/s]\u001b[A\n",
            "Iteration:  38% 252/669 [00:57<01:34,  4.43it/s]\u001b[A\n",
            "Iteration:  38% 253/669 [00:57<01:34,  4.42it/s]\u001b[A\n",
            "Iteration:  38% 254/669 [00:57<01:33,  4.43it/s]\u001b[A\n",
            "Iteration:  38% 255/669 [00:57<01:33,  4.43it/s]\u001b[A\n",
            "Iteration:  38% 256/669 [00:57<01:32,  4.44it/s]\u001b[A\n",
            "Iteration:  38% 257/669 [00:58<01:33,  4.43it/s]\u001b[A\n",
            "Iteration:  39% 258/669 [00:58<01:32,  4.43it/s]\u001b[A\n",
            "Iteration:  39% 259/669 [00:58<01:32,  4.43it/s]\u001b[A\n",
            "Iteration:  39% 260/669 [00:58<01:32,  4.44it/s]\u001b[A\n",
            "Iteration:  39% 261/669 [00:59<01:32,  4.40it/s]\u001b[A\n",
            "Iteration:  39% 262/669 [00:59<01:31,  4.43it/s]\u001b[A\n",
            "Iteration:  39% 263/669 [00:59<01:31,  4.44it/s]\u001b[A\n",
            "Iteration:  39% 264/669 [00:59<01:31,  4.44it/s]\u001b[A\n",
            "Iteration:  40% 265/669 [00:59<01:31,  4.43it/s]\u001b[A\n",
            "Iteration:  40% 266/669 [01:00<01:30,  4.45it/s]\u001b[A\n",
            "Iteration:  40% 267/669 [01:00<01:30,  4.45it/s]\u001b[A\n",
            "Iteration:  40% 268/669 [01:00<01:31,  4.40it/s]\u001b[A\n",
            "Iteration:  40% 269/669 [01:00<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  40% 270/669 [01:01<01:30,  4.43it/s]\u001b[A\n",
            "Iteration:  41% 271/669 [01:01<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 272/669 [01:01<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 273/669 [01:01<01:28,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 274/669 [01:01<01:28,  4.44it/s]\u001b[A\n",
            "Iteration:  41% 275/669 [01:02<01:28,  4.44it/s]\u001b[A\n",
            "Iteration:  41% 276/669 [01:02<01:28,  4.44it/s]\u001b[A\n",
            "Iteration:  41% 277/669 [01:02<01:28,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 278/669 [01:02<01:28,  4.43it/s]\u001b[A\n",
            "Iteration:  42% 279/669 [01:03<01:27,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 280/669 [01:03<01:27,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 281/669 [01:03<01:27,  4.42it/s]\u001b[A\n",
            "Iteration:  42% 282/669 [01:03<01:27,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 283/669 [01:04<01:26,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 284/669 [01:04<01:26,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 285/669 [01:04<01:26,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 286/669 [01:04<01:26,  4.42it/s]\u001b[A\n",
            "Iteration:  43% 287/669 [01:04<01:25,  4.46it/s]\u001b[A\n",
            "Iteration:  43% 288/669 [01:05<01:25,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 289/669 [01:05<01:25,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 290/669 [01:05<01:25,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 291/669 [01:05<01:24,  4.45it/s]\u001b[A\n",
            "Iteration:  44% 292/669 [01:06<01:24,  4.45it/s]\u001b[A\n",
            "Iteration:  44% 293/669 [01:06<01:24,  4.45it/s]\u001b[A\n",
            "Iteration:  44% 294/669 [01:06<01:24,  4.45it/s]\u001b[A\n",
            "Iteration:  44% 295/669 [01:06<01:25,  4.38it/s]\u001b[A\n",
            "Iteration:  44% 296/669 [01:06<01:24,  4.40it/s]\u001b[A\n",
            "Iteration:  44% 297/669 [01:07<01:24,  4.42it/s]\u001b[A\n",
            "Iteration:  45% 298/669 [01:07<01:23,  4.42it/s]\u001b[A\n",
            "Iteration:  45% 299/669 [01:07<01:23,  4.43it/s]\u001b[A\n",
            "Iteration:  45% 300/669 [01:07<01:23,  4.43it/s]\u001b[A\n",
            "Iteration:  45% 301/669 [01:08<01:22,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 302/669 [01:08<01:22,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 303/669 [01:08<01:22,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 304/669 [01:08<01:22,  4.44it/s]\u001b[A\n",
            "Iteration:  46% 305/669 [01:08<01:21,  4.44it/s]\u001b[A\n",
            "Iteration:  46% 306/669 [01:09<01:21,  4.45it/s]\u001b[A\n",
            "Iteration:  46% 307/669 [01:09<01:21,  4.42it/s]\u001b[A\n",
            "Iteration:  46% 308/669 [01:09<01:20,  4.46it/s]\u001b[A\n",
            "Iteration:  46% 309/669 [01:09<01:20,  4.46it/s]\u001b[A\n",
            "Iteration:  46% 310/669 [01:10<01:20,  4.46it/s]\u001b[A\n",
            "Iteration:  46% 311/669 [01:10<01:20,  4.44it/s]\u001b[A\n",
            "Iteration:  47% 312/669 [01:10<01:20,  4.46it/s]\u001b[A\n",
            "Iteration:  47% 313/669 [01:10<01:19,  4.46it/s]\u001b[A\n",
            "Iteration:  47% 314/669 [01:10<01:19,  4.45it/s]\u001b[A\n",
            "Iteration:  47% 315/669 [01:11<01:19,  4.45it/s]\u001b[A\n",
            "Iteration:  47% 316/669 [01:11<01:19,  4.44it/s]\u001b[A\n",
            "Iteration:  47% 317/669 [01:11<01:19,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 318/669 [01:11<01:18,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 319/669 [01:12<01:18,  4.45it/s]\u001b[A\n",
            "Iteration:  48% 320/669 [01:12<01:19,  4.41it/s]\u001b[A\n",
            "Iteration:  48% 321/669 [01:12<01:18,  4.43it/s]\u001b[A\n",
            "Iteration:  48% 322/669 [01:12<01:18,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 323/669 [01:13<01:17,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 324/669 [01:13<01:17,  4.43it/s]\u001b[A\n",
            "Iteration:  49% 325/669 [01:13<01:17,  4.45it/s]\u001b[A\n",
            "Iteration:  49% 326/669 [01:13<01:17,  4.45it/s]\u001b[A\n",
            "Iteration:  49% 327/669 [01:13<01:16,  4.46it/s]\u001b[A\n",
            "Iteration:  49% 328/669 [01:14<01:16,  4.44it/s]\u001b[A\n",
            "Iteration:  49% 329/669 [01:14<01:16,  4.43it/s]\u001b[A\n",
            "Iteration:  49% 330/669 [01:14<01:15,  4.46it/s]\u001b[A\n",
            "Iteration:  49% 331/669 [01:14<01:15,  4.47it/s]\u001b[A\n",
            "Iteration:  50% 332/669 [01:15<01:15,  4.46it/s]\u001b[A\n",
            "Iteration:  50% 333/669 [01:15<01:15,  4.44it/s]\u001b[A\n",
            "Iteration:  50% 334/669 [01:15<01:15,  4.45it/s]\u001b[A\n",
            "Iteration:  50% 335/669 [01:15<01:15,  4.45it/s]\u001b[A\n",
            "Iteration:  50% 336/669 [01:15<01:14,  4.45it/s]\u001b[A\n",
            "Iteration:  50% 337/669 [01:16<01:14,  4.44it/s]\u001b[A\n",
            "Iteration:  51% 338/669 [01:16<01:14,  4.45it/s]\u001b[A\n",
            "Iteration:  51% 339/669 [01:16<01:14,  4.44it/s]\u001b[A\n",
            "Iteration:  51% 340/669 [01:16<01:13,  4.45it/s]\u001b[A\n",
            "Iteration:  51% 341/669 [01:17<01:13,  4.45it/s]\u001b[A\n",
            "Iteration:  51% 342/669 [01:17<01:13,  4.45it/s]\u001b[A\n",
            "Iteration:  51% 343/669 [01:17<01:13,  4.45it/s]\u001b[A\n",
            "Iteration:  51% 344/669 [01:17<01:13,  4.45it/s]\u001b[A\n",
            "Iteration:  52% 345/669 [01:17<01:12,  4.45it/s]\u001b[A\n",
            "Iteration:  52% 346/669 [01:18<01:12,  4.45it/s]\u001b[A\n",
            "Iteration:  52% 347/669 [01:18<01:12,  4.45it/s]\u001b[A\n",
            "Iteration:  52% 348/669 [01:18<01:12,  4.45it/s]\u001b[A\n",
            "Iteration:  52% 349/669 [01:18<01:11,  4.45it/s]\u001b[A\n",
            "Iteration:  52% 350/669 [01:19<01:12,  4.43it/s]\u001b[A\n",
            "Iteration:  52% 351/669 [01:19<01:11,  4.44it/s]\u001b[A\n",
            "Iteration:  53% 352/669 [01:19<01:11,  4.44it/s]\u001b[A\n",
            "Iteration:  53% 353/669 [01:19<01:11,  4.44it/s]\u001b[A\n",
            "Iteration:  53% 354/669 [01:19<01:11,  4.43it/s]\u001b[A\n",
            "Iteration:  53% 355/669 [01:20<01:10,  4.45it/s]\u001b[A\n",
            "Iteration:  53% 356/669 [01:20<01:10,  4.45it/s]\u001b[A\n",
            "Iteration:  53% 357/669 [01:20<01:10,  4.45it/s]\u001b[A\n",
            "Iteration:  54% 358/669 [01:20<01:10,  4.43it/s]\u001b[A\n",
            "Iteration:  54% 359/669 [01:21<01:09,  4.45it/s]\u001b[A\n",
            "Iteration:  54% 360/669 [01:21<01:09,  4.45it/s]\u001b[A\n",
            "Iteration:  54% 361/669 [01:21<01:09,  4.45it/s]\u001b[A\n",
            "Iteration:  54% 362/669 [01:21<01:08,  4.45it/s]\u001b[A\n",
            "Iteration:  54% 363/669 [01:22<01:09,  4.43it/s]\u001b[A\n",
            "Iteration:  54% 364/669 [01:22<01:08,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 365/669 [01:22<01:08,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 366/669 [01:22<01:08,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 367/669 [01:22<01:08,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 368/669 [01:23<01:07,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 369/669 [01:23<01:07,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 370/669 [01:23<01:07,  4.43it/s]\u001b[A\n",
            "Iteration:  55% 371/669 [01:23<01:07,  4.41it/s]\u001b[A\n",
            "Iteration:  56% 372/669 [01:24<01:06,  4.44it/s]\u001b[A\n",
            "Iteration:  56% 373/669 [01:24<01:06,  4.44it/s]\u001b[A\n",
            "Iteration:  56% 374/669 [01:24<01:06,  4.44it/s]\u001b[A\n",
            "Iteration:  56% 375/669 [01:24<01:06,  4.44it/s]\u001b[A\n",
            "Iteration:  56% 376/669 [01:24<01:05,  4.45it/s]\u001b[A\n",
            "Iteration:  56% 377/669 [01:25<01:05,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 378/669 [01:25<01:05,  4.44it/s]\u001b[A\n",
            "Iteration:  57% 379/669 [01:25<01:06,  4.39it/s]\u001b[A\n",
            "Iteration:  57% 380/669 [01:25<01:04,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 381/669 [01:26<01:04,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 382/669 [01:26<01:04,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 383/669 [01:26<01:04,  4.41it/s]\u001b[A\n",
            "Iteration:  57% 384/669 [01:26<01:04,  4.41it/s]\u001b[A\n",
            "Iteration:  58% 385/669 [01:26<01:04,  4.43it/s]\u001b[A\n",
            "Iteration:  58% 386/669 [01:27<01:03,  4.43it/s]\u001b[A\n",
            "Iteration:  58% 387/669 [01:27<01:03,  4.43it/s]\u001b[A\n",
            "Iteration:  58% 388/669 [01:27<01:04,  4.38it/s]\u001b[A\n",
            "Iteration:  58% 389/669 [01:27<01:02,  4.45it/s]\u001b[A\n",
            "Iteration:  58% 390/669 [01:28<01:02,  4.45it/s]\u001b[A\n",
            "Iteration:  58% 391/669 [01:28<01:02,  4.45it/s]\u001b[A\n",
            "Iteration:  59% 392/669 [01:28<01:02,  4.43it/s]\u001b[A\n",
            "Iteration:  59% 393/669 [01:28<01:02,  4.44it/s]\u001b[A\n",
            "Iteration:  59% 394/669 [01:28<01:01,  4.45it/s]\u001b[A\n",
            "Iteration:  59% 395/669 [01:29<01:01,  4.45it/s]\u001b[A\n",
            "Iteration:  59% 396/669 [01:29<01:01,  4.43it/s]\u001b[A\n",
            "Iteration:  59% 397/669 [01:29<01:01,  4.45it/s]\u001b[A\n",
            "Iteration:  59% 398/669 [01:29<01:00,  4.45it/s]\u001b[A\n",
            "Iteration:  60% 399/669 [01:30<01:00,  4.45it/s]\u001b[A\n",
            "Iteration:  60% 400/669 [01:30<01:00,  4.43it/s]\u001b[A\n",
            "Iteration:  60% 401/669 [01:30<01:00,  4.42it/s]\u001b[A\n",
            "Iteration:  60% 402/669 [01:30<01:00,  4.44it/s]\u001b[A\n",
            "Iteration:  60% 403/669 [01:31<00:59,  4.45it/s]\u001b[A\n",
            "Iteration:  60% 404/669 [01:31<00:59,  4.44it/s]\u001b[A\n",
            "Iteration:  61% 405/669 [01:31<00:59,  4.45it/s]\u001b[A\n",
            "Iteration:  61% 406/669 [01:31<00:59,  4.44it/s]\u001b[A\n",
            "Iteration:  61% 407/669 [01:31<00:58,  4.45it/s]\u001b[A\n",
            "Iteration:  61% 408/669 [01:32<00:58,  4.44it/s]\u001b[A\n",
            "Iteration:  61% 409/669 [01:32<00:58,  4.43it/s]\u001b[A\n",
            "Iteration:  61% 410/669 [01:32<00:58,  4.43it/s]\u001b[A\n",
            "Iteration:  61% 411/669 [01:32<00:58,  4.42it/s]\u001b[A\n",
            "Iteration:  62% 412/669 [01:33<00:57,  4.44it/s]\u001b[A\n",
            "Iteration:  62% 413/669 [01:33<00:57,  4.43it/s]\u001b[A\n",
            "Iteration:  62% 414/669 [01:33<00:57,  4.44it/s]\u001b[A\n",
            "Iteration:  62% 415/669 [01:33<00:57,  4.45it/s]\u001b[A\n",
            "Iteration:  62% 416/669 [01:33<00:56,  4.45it/s]\u001b[A\n",
            "Iteration:  62% 417/669 [01:34<00:56,  4.43it/s]\u001b[A\n",
            "Iteration:  62% 418/669 [01:34<00:56,  4.45it/s]\u001b[A\n",
            "Iteration:  63% 419/669 [01:34<00:56,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 420/669 [01:34<00:55,  4.45it/s]\u001b[A\n",
            "Iteration:  63% 421/669 [01:35<00:55,  4.45it/s]\u001b[A\n",
            "Iteration:  63% 422/669 [01:35<00:55,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 423/669 [01:35<00:55,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 424/669 [01:35<00:55,  4.44it/s]\u001b[A\n",
            "Iteration:  64% 425/669 [01:35<00:54,  4.44it/s]\u001b[A\n",
            "Iteration:  64% 426/669 [01:36<00:54,  4.43it/s]\u001b[A\n",
            "Iteration:  64% 427/669 [01:36<00:54,  4.43it/s]\u001b[A\n",
            "Iteration:  64% 428/669 [01:36<00:54,  4.45it/s]\u001b[A\n",
            "Iteration:  64% 429/669 [01:36<00:53,  4.44it/s]\u001b[A\n",
            "Iteration:  64% 430/669 [01:37<00:53,  4.43it/s]\u001b[A\n",
            "Iteration:  64% 431/669 [01:37<00:53,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 432/669 [01:37<00:53,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 433/669 [01:37<00:53,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 434/669 [01:38<00:53,  4.43it/s]\u001b[A\n",
            "Iteration:  65% 435/669 [01:38<00:52,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 436/669 [01:38<00:52,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 437/669 [01:38<00:52,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 438/669 [01:38<00:52,  4.41it/s]\u001b[A\n",
            "Iteration:  66% 439/669 [01:39<00:51,  4.44it/s]\u001b[A\n",
            "Iteration:  66% 440/669 [01:39<00:51,  4.45it/s]\u001b[A\n",
            "Iteration:  66% 441/669 [01:39<00:51,  4.45it/s]\u001b[A\n",
            "Iteration:  66% 442/669 [01:39<00:51,  4.45it/s]\u001b[A\n",
            "Iteration:  66% 443/669 [01:40<00:51,  4.42it/s]\u001b[A\n",
            "Iteration:  66% 444/669 [01:40<00:50,  4.43it/s]\u001b[A\n",
            "Iteration:  67% 445/669 [01:40<00:50,  4.44it/s]\u001b[A\n",
            "Iteration:  67% 446/669 [01:40<00:50,  4.44it/s]\u001b[A\n",
            "Iteration:  67% 447/669 [01:40<00:50,  4.38it/s]\u001b[A\n",
            "Iteration:  67% 448/669 [01:41<00:49,  4.43it/s]\u001b[A\n",
            "Iteration:  67% 449/669 [01:41<00:49,  4.41it/s]\u001b[A\n",
            "Iteration:  67% 450/669 [01:41<00:49,  4.44it/s]\u001b[A\n",
            "Iteration:  67% 451/669 [01:41<00:49,  4.43it/s]\u001b[A\n",
            "Iteration:  68% 452/669 [01:42<00:48,  4.44it/s]\u001b[A\n",
            "Iteration:  68% 453/669 [01:42<00:48,  4.44it/s]\u001b[A\n",
            "Iteration:  68% 454/669 [01:42<00:48,  4.44it/s]\u001b[A\n",
            "Iteration:  68% 455/669 [01:42<00:48,  4.42it/s]\u001b[A\n",
            "Iteration:  68% 456/669 [01:42<00:47,  4.45it/s]\u001b[A\n",
            "Iteration:  68% 457/669 [01:43<00:47,  4.45it/s]\u001b[A\n",
            "Iteration:  68% 458/669 [01:43<00:47,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 459/669 [01:43<00:47,  4.42it/s]\u001b[A\n",
            "Iteration:  69% 460/669 [01:43<00:46,  4.46it/s]\u001b[A\n",
            "Iteration:  69% 461/669 [01:44<00:46,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 462/669 [01:44<00:46,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 463/669 [01:44<00:46,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 464/669 [01:44<00:46,  4.43it/s]\u001b[A\n",
            "Iteration:  70% 465/669 [01:44<00:46,  4.43it/s]\u001b[A\n",
            "Iteration:  70% 466/669 [01:45<00:45,  4.44it/s]\u001b[A\n",
            "Iteration:  70% 467/669 [01:45<00:45,  4.44it/s]\u001b[A\n",
            "Iteration:  70% 468/669 [01:45<00:45,  4.42it/s]\u001b[A\n",
            "Iteration:  70% 469/669 [01:45<00:45,  4.43it/s]\u001b[A\n",
            "Iteration:  70% 470/669 [01:46<00:44,  4.44it/s]\u001b[A\n",
            "Iteration:  70% 471/669 [01:46<00:44,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 472/669 [01:46<00:44,  4.43it/s]\u001b[A\n",
            "Iteration:  71% 473/669 [01:46<00:44,  4.45it/s]\u001b[A\n",
            "Iteration:  71% 474/669 [01:47<00:44,  4.43it/s]\u001b[A\n",
            "Iteration:  71% 475/669 [01:47<00:43,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 476/669 [01:47<00:43,  4.43it/s]\u001b[A\n",
            "Iteration:  71% 477/669 [01:47<00:43,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 478/669 [01:47<00:42,  4.46it/s]\u001b[A\n",
            "Iteration:  72% 479/669 [01:48<00:42,  4.46it/s]\u001b[A\n",
            "Iteration:  72% 480/669 [01:48<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 481/669 [01:48<00:42,  4.43it/s]\u001b[A\n",
            "Iteration:  72% 482/669 [01:48<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 483/669 [01:49<00:41,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 484/669 [01:49<00:41,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 485/669 [01:49<00:41,  4.44it/s]\u001b[A\n",
            "Iteration:  73% 486/669 [01:49<00:41,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 487/669 [01:49<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 488/669 [01:50<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 489/669 [01:50<00:40,  4.42it/s]\u001b[A\n",
            "Iteration:  73% 490/669 [01:50<00:40,  4.46it/s]\u001b[A\n",
            "Iteration:  73% 491/669 [01:50<00:39,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 492/669 [01:51<00:39,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 493/669 [01:51<00:39,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 494/669 [01:51<00:39,  4.43it/s]\u001b[A\n",
            "Iteration:  74% 495/669 [01:51<00:39,  4.44it/s]\u001b[A\n",
            "Iteration:  74% 496/669 [01:51<00:38,  4.44it/s]\u001b[A\n",
            "Iteration:  74% 497/669 [01:52<00:38,  4.42it/s]\u001b[A\n",
            "Iteration:  74% 498/669 [01:52<00:38,  4.42it/s]\u001b[A\n",
            "Iteration:  75% 499/669 [01:52<00:38,  4.43it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "03/08/2020 07:41:30 - INFO - transformers.configuration_utils -   Configuration saved in output_gpt_trump/checkpoint-500/config.json\n",
            "03/08/2020 07:41:31 - INFO - transformers.modeling_utils -   Model weights saved in output_gpt_trump/checkpoint-500/pytorch_model.bin\n",
            "03/08/2020 07:41:32 - INFO - __main__ -   Saving model checkpoint to output_gpt_trump/checkpoint-500\n",
            "03/08/2020 07:41:36 - INFO - __main__ -   Saving optimizer and scheduler states to output_gpt_trump/checkpoint-500\n",
            "\n",
            "Iteration:  75% 500/669 [01:58<05:32,  1.97s/it]\u001b[A\n",
            "Iteration:  75% 501/669 [01:58<04:03,  1.45s/it]\u001b[A\n",
            "Iteration:  75% 502/669 [01:59<03:00,  1.08s/it]\u001b[A\n",
            "Iteration:  75% 503/669 [01:59<02:16,  1.21it/s]\u001b[A\n",
            "Iteration:  75% 504/669 [01:59<01:46,  1.55it/s]\u001b[A\n",
            "Iteration:  75% 505/669 [01:59<01:25,  1.93it/s]\u001b[A\n",
            "Iteration:  76% 506/669 [02:00<01:10,  2.32it/s]\u001b[A\n",
            "Iteration:  76% 507/669 [02:00<00:59,  2.70it/s]\u001b[A\n",
            "Iteration:  76% 508/669 [02:00<00:52,  3.06it/s]\u001b[A\n",
            "Iteration:  76% 509/669 [02:00<00:47,  3.37it/s]\u001b[A\n",
            "Iteration:  76% 510/669 [02:00<00:43,  3.64it/s]\u001b[A\n",
            "Iteration:  76% 511/669 [02:01<00:41,  3.84it/s]\u001b[A\n",
            "Iteration:  77% 512/669 [02:01<00:39,  3.98it/s]\u001b[A\n",
            "Iteration:  77% 513/669 [02:01<00:37,  4.12it/s]\u001b[A\n",
            "Iteration:  77% 514/669 [02:01<00:36,  4.21it/s]\u001b[A\n",
            "Iteration:  77% 515/669 [02:02<00:36,  4.26it/s]\u001b[A\n",
            "Iteration:  77% 516/669 [02:02<00:35,  4.30it/s]\u001b[A\n",
            "Iteration:  77% 517/669 [02:02<00:34,  4.35it/s]\u001b[A\n",
            "Iteration:  77% 518/669 [02:02<00:35,  4.26it/s]\u001b[A\n",
            "Iteration:  78% 519/669 [02:03<00:34,  4.32it/s]\u001b[A\n",
            "Iteration:  78% 520/669 [02:03<00:34,  4.36it/s]\u001b[A\n",
            "Iteration:  78% 521/669 [02:03<00:33,  4.36it/s]\u001b[A\n",
            "Iteration:  78% 522/669 [02:03<00:33,  4.35it/s]\u001b[A\n",
            "Iteration:  78% 523/669 [02:03<00:33,  4.41it/s]\u001b[A\n",
            "Iteration:  78% 524/669 [02:04<00:32,  4.42it/s]\u001b[A\n",
            "Iteration:  78% 525/669 [02:04<00:32,  4.43it/s]\u001b[A\n",
            "Iteration:  79% 526/669 [02:04<00:32,  4.40it/s]\u001b[A\n",
            "Iteration:  79% 527/669 [02:04<00:31,  4.45it/s]\u001b[A\n",
            "Iteration:  79% 528/669 [02:05<00:31,  4.45it/s]\u001b[A\n",
            "Iteration:  79% 529/669 [02:05<00:31,  4.45it/s]\u001b[A\n",
            "Iteration:  79% 530/669 [02:05<00:31,  4.41it/s]\u001b[A\n",
            "Iteration:  79% 531/669 [02:05<00:31,  4.45it/s]\u001b[A\n",
            "Iteration:  80% 532/669 [02:05<00:30,  4.45it/s]\u001b[A\n",
            "Iteration:  80% 533/669 [02:06<00:30,  4.45it/s]\u001b[A\n",
            "Iteration:  80% 534/669 [02:06<00:30,  4.40it/s]\u001b[A\n",
            "Iteration:  80% 535/669 [02:06<00:30,  4.46it/s]\u001b[A\n",
            "Iteration:  80% 536/669 [02:06<00:29,  4.46it/s]\u001b[A\n",
            "Iteration:  80% 537/669 [02:07<00:29,  4.45it/s]\u001b[A\n",
            "Iteration:  80% 538/669 [02:07<00:29,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 539/669 [02:07<00:29,  4.41it/s]\u001b[A\n",
            "Iteration:  81% 540/669 [02:07<00:29,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 541/669 [02:07<00:28,  4.44it/s]\u001b[A\n",
            "Iteration:  81% 542/669 [02:08<00:28,  4.44it/s]\u001b[A\n",
            "Iteration:  81% 543/669 [02:08<00:28,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 544/669 [02:08<00:28,  4.44it/s]\u001b[A\n",
            "Iteration:  81% 545/669 [02:08<00:27,  4.45it/s]\u001b[A\n",
            "Iteration:  82% 546/669 [02:09<00:27,  4.45it/s]\u001b[A\n",
            "Iteration:  82% 547/669 [02:09<00:27,  4.43it/s]\u001b[A\n",
            "Iteration:  82% 548/669 [02:09<00:27,  4.43it/s]\u001b[A\n",
            "Iteration:  82% 549/669 [02:09<00:26,  4.46it/s]\u001b[A\n",
            "Iteration:  82% 550/669 [02:09<00:26,  4.45it/s]\u001b[A\n",
            "Iteration:  82% 551/669 [02:10<00:26,  4.45it/s]\u001b[A\n",
            "Iteration:  83% 552/669 [02:10<00:26,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 553/669 [02:10<00:26,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 554/669 [02:10<00:25,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 555/669 [02:11<00:25,  4.44it/s]\u001b[A\n",
            "Iteration:  83% 556/669 [02:11<00:25,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 557/669 [02:11<00:25,  4.44it/s]\u001b[A\n",
            "Iteration:  83% 558/669 [02:11<00:25,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 559/669 [02:12<00:24,  4.44it/s]\u001b[A\n",
            "Iteration:  84% 560/669 [02:12<00:24,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 561/669 [02:12<00:24,  4.44it/s]\u001b[A\n",
            "Iteration:  84% 562/669 [02:12<00:24,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 563/669 [02:12<00:23,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 564/669 [02:13<00:23,  4.45it/s]\u001b[A\n",
            "Iteration:  84% 565/669 [02:13<00:23,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 566/669 [02:13<00:23,  4.44it/s]\u001b[A\n",
            "Iteration:  85% 567/669 [02:13<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 568/669 [02:14<00:22,  4.43it/s]\u001b[A\n",
            "Iteration:  85% 569/669 [02:14<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 570/669 [02:14<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 571/669 [02:14<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  86% 572/669 [02:14<00:21,  4.45it/s]\u001b[A\n",
            "Iteration:  86% 573/669 [02:15<00:21,  4.44it/s]\u001b[A\n",
            "Iteration:  86% 574/669 [02:15<00:21,  4.44it/s]\u001b[A\n",
            "Iteration:  86% 575/669 [02:15<00:21,  4.45it/s]\u001b[A\n",
            "Iteration:  86% 576/669 [02:15<00:20,  4.44it/s]\u001b[A\n",
            "Iteration:  86% 577/669 [02:16<00:20,  4.44it/s]\u001b[A\n",
            "Iteration:  86% 578/669 [02:16<00:20,  4.43it/s]\u001b[A\n",
            "Iteration:  87% 579/669 [02:16<00:20,  4.44it/s]\u001b[A\n",
            "Iteration:  87% 580/669 [02:16<00:20,  4.42it/s]\u001b[A\n",
            "Iteration:  87% 581/669 [02:16<00:20,  4.38it/s]\u001b[A\n",
            "Iteration:  87% 582/669 [02:17<00:19,  4.44it/s]\u001b[A\n",
            "Iteration:  87% 583/669 [02:17<00:19,  4.44it/s]\u001b[A\n",
            "Iteration:  87% 584/669 [02:17<00:19,  4.44it/s]\u001b[A\n",
            "Iteration:  87% 585/669 [02:17<00:18,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 586/669 [02:18<00:18,  4.43it/s]\u001b[A\n",
            "Iteration:  88% 587/669 [02:18<00:18,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 588/669 [02:18<00:18,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 589/669 [02:18<00:18,  4.41it/s]\u001b[A\n",
            "Iteration:  88% 590/669 [02:19<00:17,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 591/669 [02:19<00:17,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 592/669 [02:19<00:17,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 593/669 [02:19<00:17,  4.45it/s]\u001b[A\n",
            "Iteration:  89% 594/669 [02:19<00:16,  4.43it/s]\u001b[A\n",
            "Iteration:  89% 595/669 [02:20<00:16,  4.45it/s]\u001b[A\n",
            "Iteration:  89% 596/669 [02:20<00:16,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 597/669 [02:20<00:16,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 598/669 [02:20<00:16,  4.41it/s]\u001b[A\n",
            "Iteration:  90% 599/669 [02:21<00:15,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 600/669 [02:21<00:15,  4.44it/s]\u001b[A\n",
            "Iteration:  90% 601/669 [02:21<00:15,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 602/669 [02:21<00:15,  4.44it/s]\u001b[A\n",
            "Iteration:  90% 603/669 [02:21<00:14,  4.43it/s]\u001b[A\n",
            "Iteration:  90% 604/669 [02:22<00:14,  4.44it/s]\u001b[A\n",
            "Iteration:  90% 605/669 [02:22<00:14,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 606/669 [02:22<00:14,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 607/669 [02:22<00:13,  4.43it/s]\u001b[A\n",
            "Iteration:  91% 608/669 [02:23<00:13,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 609/669 [02:23<00:13,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 610/669 [02:23<00:13,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 611/669 [02:23<00:13,  4.43it/s]\u001b[A\n",
            "Iteration:  91% 612/669 [02:23<00:12,  4.45it/s]\u001b[A\n",
            "Iteration:  92% 613/669 [02:24<00:12,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 614/669 [02:24<00:12,  4.45it/s]\u001b[A\n",
            "Iteration:  92% 615/669 [02:24<00:12,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 616/669 [02:24<00:11,  4.45it/s]\u001b[A\n",
            "Iteration:  92% 617/669 [02:25<00:11,  4.45it/s]\u001b[A\n",
            "Iteration:  92% 618/669 [02:25<00:11,  4.45it/s]\u001b[A\n",
            "Iteration:  93% 619/669 [02:25<00:11,  4.45it/s]\u001b[A\n",
            "Iteration:  93% 620/669 [02:25<00:11,  4.42it/s]\u001b[A\n",
            "Iteration:  93% 621/669 [02:25<00:10,  4.43it/s]\u001b[A\n",
            "Iteration:  93% 622/669 [02:26<00:10,  4.43it/s]\u001b[A\n",
            "Iteration:  93% 623/669 [02:26<00:10,  4.44it/s]\u001b[A\n",
            "Iteration:  93% 624/669 [02:26<00:10,  4.42it/s]\u001b[A\n",
            "Iteration:  93% 625/669 [02:26<00:09,  4.42it/s]\u001b[A\n",
            "Iteration:  94% 626/669 [02:27<00:09,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 627/669 [02:27<00:09,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 628/669 [02:27<00:09,  4.41it/s]\u001b[A\n",
            "Iteration:  94% 629/669 [02:27<00:09,  4.41it/s]\u001b[A\n",
            "Iteration:  94% 630/669 [02:28<00:08,  4.42it/s]\u001b[A\n",
            "Iteration:  94% 631/669 [02:28<00:08,  4.43it/s]\u001b[A\n",
            "Iteration:  94% 632/669 [02:28<00:08,  4.41it/s]\u001b[A\n",
            "Iteration:  95% 633/669 [02:28<00:08,  4.42it/s]\u001b[A\n",
            "Iteration:  95% 634/669 [02:28<00:07,  4.43it/s]\u001b[A\n",
            "Iteration:  95% 635/669 [02:29<00:07,  4.43it/s]\u001b[A\n",
            "Iteration:  95% 636/669 [02:29<00:07,  4.40it/s]\u001b[A\n",
            "Iteration:  95% 637/669 [02:29<00:07,  4.43it/s]\u001b[A\n",
            "Iteration:  95% 638/669 [02:29<00:06,  4.44it/s]\u001b[A\n",
            "Iteration:  96% 639/669 [02:30<00:06,  4.44it/s]\u001b[A\n",
            "Iteration:  96% 640/669 [02:30<00:06,  4.42it/s]\u001b[A\n",
            "Iteration:  96% 641/669 [02:30<00:06,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 642/669 [02:30<00:06,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 643/669 [02:30<00:05,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 644/669 [02:31<00:05,  4.43it/s]\u001b[A\n",
            "Iteration:  96% 645/669 [02:31<00:05,  4.43it/s]\u001b[A\n",
            "Iteration:  97% 646/669 [02:31<00:05,  4.45it/s]\u001b[A\n",
            "Iteration:  97% 647/669 [02:31<00:04,  4.45it/s]\u001b[A\n",
            "Iteration:  97% 648/669 [02:32<00:04,  4.44it/s]\u001b[A\n",
            "Iteration:  97% 649/669 [02:32<00:04,  4.42it/s]\u001b[A\n",
            "Iteration:  97% 650/669 [02:32<00:04,  4.43it/s]\u001b[A\n",
            "Iteration:  97% 651/669 [02:32<00:04,  4.44it/s]\u001b[A\n",
            "Iteration:  97% 652/669 [02:32<00:03,  4.44it/s]\u001b[A\n",
            "Iteration:  98% 653/669 [02:33<00:03,  4.42it/s]\u001b[A\n",
            "Iteration:  98% 654/669 [02:33<00:03,  4.43it/s]\u001b[A\n",
            "Iteration:  98% 655/669 [02:33<00:03,  4.44it/s]\u001b[A\n",
            "Iteration:  98% 656/669 [02:33<00:02,  4.44it/s]\u001b[A\n",
            "Iteration:  98% 657/669 [02:34<00:02,  4.39it/s]\u001b[A\n",
            "Iteration:  98% 658/669 [02:34<00:02,  4.41it/s]\u001b[A\n",
            "Iteration:  99% 659/669 [02:34<00:02,  4.42it/s]\u001b[A\n",
            "Iteration:  99% 660/669 [02:34<00:02,  4.43it/s]\u001b[A\n",
            "Iteration:  99% 661/669 [02:35<00:01,  4.42it/s]\u001b[A\n",
            "Iteration:  99% 662/669 [02:35<00:01,  4.45it/s]\u001b[A\n",
            "Iteration:  99% 663/669 [02:35<00:01,  4.41it/s]\u001b[A\n",
            "Iteration:  99% 664/669 [02:35<00:01,  4.44it/s]\u001b[A\n",
            "Iteration:  99% 665/669 [02:35<00:00,  4.43it/s]\u001b[A\n",
            "Iteration: 100% 666/669 [02:36<00:00,  4.41it/s]\u001b[A\n",
            "Iteration: 100% 667/669 [02:36<00:00,  4.45it/s]\u001b[A\n",
            "Iteration: 100% 668/669 [02:36<00:00,  4.45it/s]\u001b[A\n",
            "Iteration: 100% 669/669 [02:36<00:00,  4.43it/s]\u001b[A\n",
            "Epoch: 100% 1/1 [02:36<00:00, 156.83s/it]\n",
            "03/08/2020 07:42:14 - INFO - __main__ -    global_step = 669, average loss = 3.592823270190457\n",
            "03/08/2020 07:42:14 - INFO - __main__ -   Saving model checkpoint to output_gpt_trump\n",
            "03/08/2020 07:42:14 - INFO - transformers.configuration_utils -   Configuration saved in output_gpt_trump/config.json\n",
            "03/08/2020 07:42:16 - INFO - transformers.modeling_utils -   Model weights saved in output_gpt_trump/pytorch_model.bin\n",
            "03/08/2020 07:42:16 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_trump/config.json\n",
            "03/08/2020 07:42:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/08/2020 07:42:16 - INFO - transformers.modeling_utils -   loading weights file output_gpt_trump/pytorch_model.bin\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   Model name 'output_gpt_trump' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output_gpt_trump' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   Didn't find file output_gpt_trump/added_tokens.json. We won't load it.\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/vocab.json\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/merges.txt\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/special_tokens_map.json\n",
            "03/08/2020 07:42:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/tokenizer_config.json\n",
            "03/08/2020 07:42:20 - INFO - __main__ -   Evaluate the following checkpoints: ['output_gpt_trump']\n",
            "03/08/2020 07:42:20 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_trump/config.json\n",
            "03/08/2020 07:42:20 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/08/2020 07:42:20 - INFO - transformers.modeling_utils -   loading weights file output_gpt_trump/pytorch_model.bin\n",
            "03/08/2020 07:42:23 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/08/2020 07:42:24 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_test_text_trump\n",
            "03/08/2020 07:42:24 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/08/2020 07:42:24 - INFO - __main__ -     Num examples = 167\n",
            "03/08/2020 07:42:24 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 167/167 [00:11<00:00, 14.99it/s]\n",
            "03/08/2020 07:42:35 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/08/2020 07:42:35 - INFO - __main__ -     perplexity = tensor(29.4191)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hympHJXeMIhB",
        "colab_type": "code",
        "outputId": "70750d94-91d4-4967-86c7-27af0dc9338b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_trump"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_trump' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_trump' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_trump/added_tokens.json. We won't load it.\n",
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/vocab.json\n",
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/merges.txt\n",
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/special_tokens_map.json\n",
            "03/08/2020 07:43:17 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/tokenizer_config.json\n",
            "03/08/2020 07:43:17 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_trump/config.json\n",
            "03/08/2020 07:43:17 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/08/2020 07:43:17 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_trump/pytorch_model.bin\n",
            "03/08/2020 07:43:24 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=20, model_name_or_path='/content/output_gpt_trump', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"run_generation.py\", line 263, in <module>\n",
            "    main()\n",
            "  File \"run_generation.py\", line 210, in main\n",
            "    prompt_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JPmKEzBcJ1",
        "colab_type": "text"
      },
      "source": [
        "### Save Files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtcKAEMJD3sX",
        "colab_type": "code",
        "outputId": "775081ec-a80e-48bf-8eb4-105b1718da29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'pytorch_model.bin'})\n",
        "uploaded.SetContentFile('output_roberta_US/pytorch_model.bin')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1w4rzAyObL6YRo0TOmic7Cwbx1cfSPUPB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akRYEEF9OXf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}