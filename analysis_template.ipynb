{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim#For word2vec, etc\n",
    "from gensim.models import ldaseqmodel\n",
    "\n",
    "import lucem_illud_2020\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filepath=\"/Users/rachelker/Documents/UChic MSCAPP/Curriculum/2019-20 Winter/Computational Content Analysis/Project/twitterpolicydiscourse/data/\"\n",
    "coca_data = pd.read_csv(filepath + \"refugee_coca_foranalysis.csv\")\n",
    "#tweet_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to concatenate coca and tweet data\n",
    "data = coca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and normalize words\n",
    "data['tokenized_words'] = data['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "data['normalized_words'] = data['tokenized_words'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))\n",
    "data['normalized_words_POS'] = [lucem_illud_2020.spacy_pos(t) for t in data['text']]\n",
    "\n",
    "\n",
    "# tokenize and normalize sentences\n",
    "data['tokenized_sents'] = data['text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "data['normalized_sents'] = data['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/refugee_coca_normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>subgen</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_info</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>normalized_words</th>\n",
       "      <th>normalized_words_POS</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018849</td>\n",
       "      <td>\" bums . \" that 's what radio havana called ...</td>\n",
       "      <td>2950</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>This is the land of opportunity.</td>\n",
       "      <td>Vol. 19 Issue 8, p98, 8p, 1 chart, 3c, 4bw\\r\\n</td>\n",
       "      <td>[bums, that, 's, what, radio, havana, called, ...</td>\n",
       "      <td>[bum, radio, havana, call, cubans, flee, u.s, ...</td>\n",
       "      <td>[(  , _SP), (\", ``), (bums, NNS), (., .), (\", ...</td>\n",
       "      <td>[[bums], [that, 's, what, radio, havana, calle...</td>\n",
       "      <td>[[bum], [radio, havana, call], [cubans, flee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018850</td>\n",
       "      <td>section : investing  expanding petrochemical ...</td>\n",
       "      <td>2514</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>Betting on regional booms.</td>\n",
       "      <td>Vol. 19 Issue 8, p110, 5p, 1 chart, 2c\\r\\n</td>\n",
       "      <td>[section, investing, expanding, petrochemical,...</td>\n",
       "      <td>[section, invest, expand, petrochemical, plant...</td>\n",
       "      <td>[( , _SP), (section, NN), (:, :), (investing, ...</td>\n",
       "      <td>[[section, investing, expanding, petrochemical...</td>\n",
       "      <td>[[section, invest, expand, petrochemical, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019006</td>\n",
       "      <td>section : clothes that work american history ...</td>\n",
       "      <td>1667</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>130.0</td>\n",
       "      <td>MotherEarth</td>\n",
       "      <td>The evolution of jeans.</td>\n",
       "      <td>p60, 4p, 5c, 2bw\\r\\n</td>\n",
       "      <td>[section, clothes, that, work, american, histo...</td>\n",
       "      <td>[section, clothe, work, american, history, sam...</td>\n",
       "      <td>[( , _SP), (section, NN), (:, :), (clothes, NN...</td>\n",
       "      <td>[[section], [clothes, that, work, american, hi...</td>\n",
       "      <td>[[section], [clothe, work, american, history, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019061</td>\n",
       "      <td>section : movements from socialist to republi...</td>\n",
       "      <td>1754</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>123.0</td>\n",
       "      <td>MotherJones</td>\n",
       "      <td>Serve the people.</td>\n",
       "      <td>Vol. 15 Issue 5, p18, 3p, 1 illustration\\r\\n</td>\n",
       "      <td>[section, movements, from, socialist, to, repu...</td>\n",
       "      <td>[section, movement, socialist, republicans, ge...</td>\n",
       "      <td>[( , _SP), (section, NN), (:, :), (movements, ...</td>\n",
       "      <td>[[section], [movements, from, socialist, to, r...</td>\n",
       "      <td>[[section], [movement, socialist, republicans,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019063</td>\n",
       "      <td>inside a dusty cement-block house with worn ...</td>\n",
       "      <td>6032</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>123.0</td>\n",
       "      <td>MotherJones</td>\n",
       "      <td>No road to Tahuanti.</td>\n",
       "      <td>Vol. 15 Issue 5, p36, 11p, 8bw\\r\\n</td>\n",
       "      <td>[inside, a, dusty, cement, block, house, with,...</td>\n",
       "      <td>[inside, dusty, cement, block, house, wear, li...</td>\n",
       "      <td>[(  , _SP), (inside, IN), (a, DT), (dusty, JJ)...</td>\n",
       "      <td>[[inside, a, dusty, cement, block, house, with...</td>\n",
       "      <td>[[inside, dusty, cement, block, house, wear, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text  word_count  \\\n",
       "0  2018849    \" bums . \" that 's what radio havana called ...        2950   \n",
       "1  2018850   section : investing  expanding petrochemical ...        2514   \n",
       "2  2019006   section : clothes that work american history ...        1667   \n",
       "3  2019061   section : movements from socialist to republi...        1754   \n",
       "4  2019063    inside a dusty cement-block house with worn ...        6032   \n",
       "\n",
       "   year genre  subgen       source                             title  \\\n",
       "0  1990   MAG   124.0        Money  This is the land of opportunity.   \n",
       "1  1990   MAG   124.0        Money        Betting on regional booms.   \n",
       "2  1990   MAG   130.0  MotherEarth           The evolution of jeans.   \n",
       "3  1990   MAG   123.0  MotherJones                 Serve the people.   \n",
       "4  1990   MAG   123.0  MotherJones              No road to Tahuanti.   \n",
       "\n",
       "                                 publication_info  \\\n",
       "0  Vol. 19 Issue 8, p98, 8p, 1 chart, 3c, 4bw\\r\\n   \n",
       "1      Vol. 19 Issue 8, p110, 5p, 1 chart, 2c\\r\\n   \n",
       "2                            p60, 4p, 5c, 2bw\\r\\n   \n",
       "3    Vol. 15 Issue 5, p18, 3p, 1 illustration\\r\\n   \n",
       "4              Vol. 15 Issue 5, p36, 11p, 8bw\\r\\n   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [bums, that, 's, what, radio, havana, called, ...   \n",
       "1  [section, investing, expanding, petrochemical,...   \n",
       "2  [section, clothes, that, work, american, histo...   \n",
       "3  [section, movements, from, socialist, to, repu...   \n",
       "4  [inside, a, dusty, cement, block, house, with,...   \n",
       "\n",
       "                                    normalized_words  \\\n",
       "0  [bum, radio, havana, call, cubans, flee, u.s, ...   \n",
       "1  [section, invest, expand, petrochemical, plant...   \n",
       "2  [section, clothe, work, american, history, sam...   \n",
       "3  [section, movement, socialist, republicans, ge...   \n",
       "4  [inside, dusty, cement, block, house, wear, li...   \n",
       "\n",
       "                                normalized_words_POS  \\\n",
       "0  [(  , _SP), (\", ``), (bums, NNS), (., .), (\", ...   \n",
       "1  [( , _SP), (section, NN), (:, :), (investing, ...   \n",
       "2  [( , _SP), (section, NN), (:, :), (clothes, NN...   \n",
       "3  [( , _SP), (section, NN), (:, :), (movements, ...   \n",
       "4  [(  , _SP), (inside, IN), (a, DT), (dusty, JJ)...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [[bums], [that, 's, what, radio, havana, calle...   \n",
       "1  [[section, investing, expanding, petrochemical...   \n",
       "2  [[section], [clothes, that, work, american, hi...   \n",
       "3  [[section], [movements, from, socialist, to, r...   \n",
       "4  [[inside, a, dusty, cement, block, house, with...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [[bum], [radio, havana, call], [cubans, flee, ...  \n",
       "1  [[section, invest, expand, petrochemical, plan...  \n",
       "2  [[section], [clothe, work, american, history, ...  \n",
       "3  [[section], [movement, socialist, republicans,...  \n",
       "4  [[inside, dusty, cement, block, house, wear, l...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word/Phrase Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get context\n",
    "post_text = nltk.Text(data['tokenized_words'].sum())\n",
    "index = nltk.text.ConcordanceIndex(post_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 3582 matches:\n",
      "university and himself a 1960 cuban refugee mariels were driven less by politic\n",
      "he wall photo black white below the refugee center where the suarezes and other\n",
      "toon at second glance i look like a refugee farmer but careful examination reve\n",
      "s guerrillas as we pass a family of refugee indians from the highlands dressed \n",
      "ople creating a massive and complex refugee problem half a million saudis egypt\n",
      "tuation is of particular concern to refugee organizations if baghdad succeeds i\n",
      " an unusual briefing on the kuwaiti refugee dilemma and the palestinian aspect \n",
      "no one knows how many soviet jewish refugee immigrants will finally come in par\n",
      "at they had better shut down contra refugee camps before michael dukakis became\n",
      "d cutting why not the fanjuls are a refugee family he says the economy of the c\n",
      "ttee on the panama canal the aid to refugee chinese intellectuals among the bet\n",
      "ms fell over themselves to hire any refugee from drexel 's corporate finance de\n",
      "soviet army barracks into emergency refugee camps german officials speak uneasi\n",
      "tino was often there as was another refugee from rome the architect and sculpto\n",
      "was biochemist ernst chain a german refugee florey believed that many of the an\n",
      "ly thatch of hair california shirts refugee in marin county instead he is a tex\n",
      " at a time when he lived there as a refugee from the problems in mainz back in \n",
      "eum under the title britain and the refugee crisis 1933 1947 make this point wi\n",
      "near rudesheim in 1902 arrived as a refugee in 1933 hallgarten later a well kno\n",
      "hs of 1940 existing currents of pro refugee sentiment suddenly evaporated this \n",
      "victor gollancz noel coward and the refugee hermann rauschning the lists were n\n",
      "ittle sympathy for rincon a spanish refugee and rebel a traitor to both his sov\n",
      "themselves out to be leaders in the refugee community first the younger son and\n",
      " looked on ireland from the crowded refugee towns on the continent and saw that\n",
      "was a catalyst in an experience all refugee communities encounter whether to pr\n"
     ]
    }
   ],
   "source": [
    "index.print_concordance('refugee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 7256 matches:\n",
      "w in the water jammed with 80 or so refugees some of them just released from cub\n",
      "torm a documentary about the mariel refugees fate as he gazes at the hangar the \n",
      "hter 's future making a will mariel refugees belies the stigma that the cuban bo\n",
      "ift 's end in september some mariel refugees had committed murders burglaries or\n",
      "where the suarezes and other mariel refugees section investing expanding petroch\n",
      "uments and aerospace parts many are refugees from high tax california and unfort\n",
      "tytown later they met three teenage refugees from honduras the last surviving ma\n",
      "y center casa oscar romero when the refugees brought out guitars the students le\n",
      "ese newer immigrants were political refugees they were for the most part poor pe\n",
      ",744 people mostly eastern european refugees then living in germany and austria \n",
      "gulations provided for thousands of refugees from hungary after the hungarian ri\n",
      "ericans including relatively recent refugees or emigres or escapees from eastern\n",
      "ctober after interviewing scores of refugees in the region amnesty international\n",
      "king in kuwait as domestic servants refugees who have survived iraqi torture and\n",
      "the leaders of the resistance other refugees say the resistance is led by civili\n",
      "unate are the roughly 150,000 asian refugees from bangladesh india pakistan sri \n",
      "ficial put it the u.s committee for refugees reported that two months after the \n",
      "more than 75,000 non native kuwaiti refugees most of them asians in ill staffed \n",
      "amps in jordan approximately 60,000 refugees crossed into syria 40,000 into turk\n",
      "ching relations with tehran the new refugees as well as the half million refugee\n",
      "efugees as well as the half million refugees from the iran iraq war including 90\n",
      "eted by the arrival of thousands of refugees from the civil war in neighboring m\n",
      " those who came in 1969 79 they 're refugees most would be going to brooklyn if \n",
      " georgian born ephraim gur today 's refugees may finally who came in the seventi\n",
      "rations of sanctuary for salvadoran refugees a political movement designed to fo\n"
     ]
    }
   ],
   "source": [
    "index.print_concordance('refugees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so_some mariel_fate mariel_belies mariel_had mariel_section are_from\n",
      "teenage_from the_brought political_they european_then of_from\n",
      "recent_or of_in servants_who other_say asian_from for_reported\n",
      "kuwaiti_most 60,000_crossed new_as\n"
     ]
    }
   ],
   "source": [
    "# common context\n",
    "post_text.common_contexts(['refugees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelker/.pyenv/versions/3.8.1/lib/python3.8/site-packages/nltk/draw/__init__.py:15: UserWarning: nltk.draw package not loaded (please install Tkinter library).\n",
      "  warnings.warn(\"nltk.draw package not loaded \" \"(please install Tkinter library).\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lexical Dispersion Plot\n",
    "sns.reset_orig() #Seaborn messes with this plot, disabling it\n",
    "post_text.dispersion_plot(['refugee', 'refugees'])\n",
    "sns.set() #Re-enabling seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Distribution\n",
    "fdist = nltk.FreqDist([w for w in data['normalized_words'].sum()])\n",
    "freq = pd.DataFrame.from_dict(fdist, orient='index', columns=['count'])\n",
    "freq = freq.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>49565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>36155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>30895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>28799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>27061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>26441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>25620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>22330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>21985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>21222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>20884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>20519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>18344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>17934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>16583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>15979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>15933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>15745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>15177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>15109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "say        49565\n",
       "people     36155\n",
       "think      30895\n",
       "go         28799\n",
       "year       27061\n",
       "know       26441\n",
       "mr         25620\n",
       "time       22330\n",
       "come       21985\n",
       "like       21222\n",
       "state      20884\n",
       "new        20519\n",
       "president  18344\n",
       "right      17934\n",
       "want       16583\n",
       "war        15979\n",
       "country    15933\n",
       "work       15745\n",
       "day        15177\n",
       "have       15109"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Most common POS\n",
    "def get_most_common_POS(data, years, genre=None):\n",
    "    for yr in years:\n",
    "        if genre:\n",
    "            yearly = data[(data['genre']==genre) & (data['year']==yr)]\n",
    "        else:\n",
    "            yearly = data[data['year']==yr]\n",
    "        freqdist_POStoWord = nltk.ConditionalFreqDist((p, w) for w, p in yearly['normalized_words_POS'].sum())\n",
    "        print(yr)\n",
    "        print(\"Most Common Nouns\")\n",
    "        print(freqdist_POStoWord['NN'].most_common(10))\n",
    "        print()\n",
    "        print(\"Most Common Adj\")\n",
    "        print(freqdist_POStoWord['JJ'].most_common(10))\n",
    "        print()\n",
    "        print(\"Most Common Verbs\")\n",
    "        print(freqdist_POStoWord['VB'].most_common(10))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "Most Common Nouns\n",
      "[('time', 980), ('government', 940), ('today', 874), ('world', 802), ('country', 781), ('war', 687), ('way', 665), ('year', 610), ('day', 460), ('money', 420)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 1195), ('many', 820), ('new', 762), ('american', 638), ('last', 600), ('good', 573), ('iraqi', 571), ('military', 518), ('first', 513), ('soviet', 481)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 3589), ('have', 1334), ('do', 853), ('get', 709), ('go', 560), ('think', 486), ('take', 482), ('make', 476), ('say', 455), ('see', 407)]\n",
      "\n",
      "1991\n",
      "Most Common Nouns\n",
      "[('war', 1349), ('time', 1282), ('government', 1106), ('today', 1071), ('country', 925), ('way', 893), ('world', 790), ('year', 703), ('day', 602), ('problem', 559)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 1437), ('many', 1073), ('iraqi', 849), ('american', 841), ('new', 832), ('last', 810), ('good', 767), ('first', 687), ('political', 653), ('own', 568)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 4783), ('have', 1995), ('do', 1254), ('get', 888), ('go', 874), ('say', 709), ('think', 687), ('take', 670), ('make', 648), ('see', 639)]\n",
      "\n",
      "1992\n",
      "Most Common Nouns\n",
      "[('time', 835), ('government', 678), ('country', 666), ('today', 636), ('year', 591), ('world', 532), ('way', 521), ('percent', 475), ('war', 431), ('family', 396)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 907), ('many', 762), ('new', 622), ('last', 596), ('first', 517), ('good', 503), ('american', 503), ('political', 474), ('little', 370), ('old', 368)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2864), ('have', 1215), ('do', 823), ('get', 672), ('go', 536), ('make', 460), ('take', 412), ('think', 395), ('see', 391), ('say', 389)]\n",
      "\n",
      "1993\n",
      "Most Common Nouns\n",
      "[('time', 919), ('war', 722), ('year', 686), ('today', 664), ('way', 651), ('world', 641), ('government', 620), ('country', 564), ('man', 479), ('president', 426)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 962), ('many', 789), ('new', 664), ('last', 633), ('american', 545), ('first', 512), ('good', 510), ('political', 444), ('old', 411), ('own', 328)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 3154), ('have', 1260), ('do', 925), ('get', 631), ('go', 510), ('make', 487), ('think', 445), ('see', 441), ('say', 425), ('take', 420)]\n",
      "\n",
      "1994\n",
      "Most Common Nouns\n",
      "[('time', 1230), ('government', 1037), ('voice', 954), ('country', 886), ('today', 852), ('way', 846), ('year', 801), ('president', 771), ('world', 755), ('policy', 712)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 1352), ('new', 1033), ('many', 970), ('last', 807), ('american', 798), ('good', 731), ('first', 680), ('political', 566), ('old', 515), ('little', 486)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 4511), ('have', 1914), ('do', 1283), ('get', 1026), ('go', 830), ('say', 641), ('think', 638), ('make', 624), ('see', 620), ('let', 616)]\n",
      "\n",
      "1995\n",
      "Most Common Nouns\n",
      "[('time', 583), ('year', 486), ('war', 480), ('world', 480), ('government', 449), ('way', 426), ('today', 412), ('peace', 390), ('country', 380), ('day', 329)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 730), ('many', 528), ('new', 506), ('american', 467), ('first', 447), ('last', 378), ('good', 329), ('own', 282), ('old', 275), ('bosnian', 275)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2180), ('have', 846), ('do', 563), ('get', 398), ('go', 365), ('say', 324), ('take', 310), ('make', 297), ('let', 278), ('see', 266)]\n",
      "\n",
      "1996\n",
      "Most Common Nouns\n",
      "[('time', 744), ('today', 604), ('year', 565), ('government', 551), ('way', 530), ('country', 445), ('war', 377), ('day', 352), ('world', 350), ('life', 339)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 723), ('new', 587), ('many', 530), ('last', 517), ('american', 501), ('first', 476), ('good', 441), ('political', 315), ('military', 304), ('own', 271)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2471), ('have', 1077), ('do', 760), ('get', 559), ('go', 434), ('make', 375), ('say', 347), ('take', 327), ('think', 318), ('see', 317)]\n",
      "\n",
      "1997\n",
      "Most Common Nouns\n",
      "[('time', 558), ('government', 398), ('war', 376), ('year', 341), ('way', 338), ('country', 321), ('king', 301), ('world', 283), ('today', 275), ('part', 269)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 558), ('new', 466), ('many', 411), ('last', 321), ('first', 318), ('american', 293), ('political', 265), ('good', 236), ('old', 229), ('same', 210)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1690), ('have', 623), ('do', 389), ('get', 343), ('go', 235), ('see', 228), ('make', 226), ('think', 224), ('take', 216), ('say', 170)]\n",
      "\n",
      "1998\n",
      "Most Common Nouns\n",
      "[('time', 525), ('way', 330), ('year', 321), ('king', 314), ('day', 309), ('world', 301), ('man', 290), ('government', 288), ('country', 259), ('today', 252)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 450), ('many', 421), ('last', 363), ('first', 322), ('new', 319), ('good', 295), ('old', 241), ('american', 221), ('little', 220), ('few', 184)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1444), ('have', 671), ('do', 439), ('get', 301), ('go', 292), ('make', 243), ('see', 229), ('take', 217), ('say', 215), ('know', 212)]\n",
      "\n",
      "1999\n",
      "Most Common Nouns\n",
      "[('time', 802), ('war', 793), ('way', 561), ('country', 461), ('world', 455), ('family', 444), ('air', 427), ('lot', 407), ('today', 406), ('day', 398)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 819), ('many', 615), ('last', 483), ('good', 472), ('first', 470), ('new', 426), ('american', 401), ('own', 357), ('little', 338), ('old', 326)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2728), ('have', 1106), ('do', 752), ('get', 588), ('go', 549), ('say', 453), ('make', 417), ('see', 393), ('think', 369), ('take', 362)]\n",
      "\n",
      "2000\n",
      "Most Common Nouns\n",
      "[('time', 506), ('government', 454), ('war', 389), ('world', 349), ('year', 337), ('peace', 317), ('end', 312), ('way', 304), ('day', 284), ('voice', 265)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 541), ('many', 445), ('new', 418), ('last', 369), ('israeli', 319), ('palestinian', 312), ('first', 308), ('political', 285), ('old', 239), ('american', 228)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1476), ('have', 558), ('do', 290), ('get', 275), ('go', 243), ('see', 234), ('make', 219), ('take', 210), ('begin', 204), ('say', 188)]\n",
      "\n",
      "2001\n",
      "Most Common Nouns\n",
      "[('time', 778), ('world', 573), ('war', 523), ('year', 479), ('country', 466), ('way', 460), ('life', 401), ('government', 385), ('day', 382), ('lot', 310)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 661), ('many', 552), ('last', 465), ('new', 461), ('first', 440), ('american', 368), ('good', 351), ('old', 306), ('military', 305), ('same', 281)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2087), ('have', 866), ('do', 585), ('get', 426), ('go', 372), ('see', 342), ('make', 314), ('take', 307), ('say', 269), ('know', 251)]\n",
      "\n",
      "2002\n",
      "Most Common Nouns\n",
      "[('time', 620), ('world', 578), ('war', 535), ('way', 435), ('country', 406), ('year', 405), ('day', 399), ('life', 385), ('director', 381), ('government', 377)]\n",
      "\n",
      "Most Common Adj\n",
      "[('israeli', 709), ('other', 663), ('many', 556), ('palestinian', 511), ('new', 510), ('last', 376), ('first', 365), ('unidentified', 343), ('american', 315), ('own', 281)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1855), ('have', 699), ('do', 442), ('go', 365), ('get', 345), ('make', 314), ('see', 298), ('take', 291), ('say', 226), ('know', 191)]\n",
      "\n",
      "2003\n",
      "Most Common Nouns\n",
      "[('war', 704), ('time', 554), ('world', 447), ('government', 403), ('day', 373), ('way', 365), ('year', 362), ('director', 348), ('sex', 298), ('family', 297)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 544), ('many', 474), ('new', 371), ('first', 367), ('american', 362), ('last', 313), ('iraqi', 291), ('good', 280), ('old', 262), ('unidentified', 237)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1747), ('have', 624), ('do', 387), ('get', 373), ('go', 300), ('make', 250), ('see', 225), ('take', 203), ('say', 202), ('think', 195)]\n",
      "\n",
      "2004\n",
      "Most Common Nouns\n",
      "[('time', 462), ('war', 387), ('director', 375), ('way', 307), ('country', 273), ('government', 273), ('world', 271), ('year', 268), ('life', 250), ('day', 246)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 428), ('new', 387), ('many', 386), ('first', 300), ('last', 281), ('american', 264), ('good', 209), ('old', 203), ('palestinian', 186), ('unidentified', 180)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1248), ('have', 507), ('do', 286), ('get', 230), ('see', 219), ('take', 201), ('go', 194), ('make', 189), ('know', 179), ('say', 165)]\n",
      "\n",
      "2005\n",
      "Most Common Nouns\n",
      "[('time', 647), ('world', 379), ('war', 376), ('city', 360), ('way', 347), ('government', 346), ('life', 337), ('day', 332), ('today', 326), ('family', 319)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 582), ('other', 542), ('new', 411), ('first', 372), ('last', 359), ('good', 333), ('unidentified', 307), ('american', 289), ('old', 284), ('little', 268)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1905), ('have', 848), ('get', 550), ('do', 495), ('go', 364), ('see', 352), ('take', 321), ('make', 278), ('know', 248), ('say', 220)]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "Most Common Nouns\n",
      "[('time', 756), ('world', 485), ('day', 474), ('government', 461), ('country', 460), ('way', 453), ('today', 417), ('war', 382), ('year', 376), ('city', 374)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 669), ('many', 629), ('new', 586), ('last', 414), ('israeli', 399), ('first', 386), ('old', 354), ('good', 325), ('american', 310), ('little', 284)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2140), ('have', 812), ('get', 546), ('do', 510), ('see', 395), ('go', 387), ('take', 368), ('make', 327), ('say', 262), ('come', 250)]\n",
      "\n",
      "2007\n",
      "Most Common Nouns\n",
      "[('time', 537), ('war', 467), ('government', 419), ('world', 401), ('year', 395), ('country', 336), ('way', 331), ('today', 327), ('life', 310), ('day', 298)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 493), ('many', 410), ('new', 392), ('last', 295), ('first', 287), ('american', 278), ('good', 248), ('political', 227), ('own', 216), ('old', 209)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1640), ('have', 715), ('do', 473), ('get', 349), ('go', 293), ('make', 269), ('see', 255), ('say', 232), ('take', 231), ('think', 203)]\n",
      "\n",
      "2008\n",
      "Most Common Nouns\n",
      "[('time', 304), ('government', 247), ('war', 206), ('year', 204), ('country', 201), ('life', 177), ('world', 171), ('family', 169), ('day', 168), ('way', 155)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 305), ('other', 284), ('new', 258), ('last', 172), ('first', 164), ('old', 154), ('good', 144), ('american', 139), ('little', 133), ('own', 131)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 824), ('have', 308), ('do', 200), ('get', 174), ('go', 153), ('make', 149), ('see', 147), ('take', 124), ('say', 108), ('help', 90)]\n",
      "\n",
      "2009\n",
      "Most Common Nouns\n",
      "[('time', 306), ('war', 283), ('government', 266), ('#', 219), ('way', 204), ('family', 197), ('life', 184), ('world', 177), ('year', 176), ('country', 172)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 340), ('many', 294), ('new', 243), ('israeli', 190), ('old', 173), ('last', 172), ('unidentified', 169), ('good', 165), ('american', 164), ('first', 163)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 939), ('have', 368), ('do', 214), ('get', 167), ('make', 163), ('see', 155), ('go', 154), ('know', 135), ('say', 129), ('take', 123)]\n",
      "\n",
      "2010\n",
      "Most Common Nouns\n",
      "[('#', 695), ('time', 392), ('country', 297), ('year', 284), ('government', 265), ('way', 220), ('war', 215), ('today', 202), ('world', 200), ('life', 198)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 330), ('many', 324), ('new', 273), ('last', 238), ('first', 210), ('good', 194), ('old', 162), ('american', 158), ('few', 156), ('little', 144)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 975), ('have', 318), ('do', 236), ('get', 211), ('go', 156), ('make', 152), ('see', 144), ('take', 144), ('know', 101), ('say', 97)]\n",
      "\n",
      "2011\n",
      "Most Common Nouns\n",
      "[('#', 841), ('time', 430), ('government', 379), ('way', 309), ('war', 308), ('today', 297), ('country', 295), ('day', 294), ('world', 263), ('state', 246)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 453), ('new', 355), ('many', 324), ('last', 262), ('first', 249), ('good', 231), ('old', 171), ('political', 168), ('military', 161), ('american', 160)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1200), ('have', 473), ('do', 331), ('get', 293), ('see', 256), ('make', 240), ('go', 222), ('take', 178), ('say', 177), ('think', 164)]\n",
      "\n",
      "2012\n",
      "Most Common Nouns\n",
      "[('#', 1909), ('time', 527), ('government', 523), ('today', 424), ('way', 394), ('country', 367), ('year', 362), ('war', 335), ('lot', 321), ('day', 319)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 495), ('many', 408), ('new', 384), ('last', 334), ('first', 307), ('good', 301), ('syrian', 249), ('little', 231), ('right', 217), ('american', 216)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1588), ('have', 703), ('do', 489), ('get', 386), ('go', 308), ('make', 284), ('see', 282), ('say', 275), ('take', 267), ('think', 209)]\n",
      "\n",
      "2013\n",
      "Most Common Nouns\n",
      "[('#', 3286), ('time', 739), ('government', 516), ('today', 490), ('country', 474), ('world', 459), ('way', 455), ('lot', 432), ('year', 429), ('day', 428)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 689), ('many', 523), ('new', 513), ('last', 389), ('good', 387), ('american', 369), ('first', 365), ('right', 342), ('little', 306), ('great', 259)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2309), ('have', 969), ('do', 655), ('get', 575), ('go', 457), ('see', 422), ('make', 393), ('take', 336), ('let', 326), ('say', 317)]\n",
      "\n",
      "2014\n",
      "Most Common Nouns\n",
      "[('#', 3822), ('time', 882), ('government', 727), ('country', 674), ('way', 622), ('year', 558), ('lot', 556), ('world', 530), ('today', 519), ('war', 518)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 784), ('many', 690), ('new', 637), ('last', 494), ('good', 485), ('first', 460), ('american', 430), ('right', 379), ('political', 342), ('little', 324)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2748), ('have', 1132), ('do', 829), ('get', 690), ('see', 507), ('go', 468), ('take', 456), ('make', 448), ('say', 408), ('think', 373)]\n",
      "\n",
      "2015\n",
      "Most Common Nouns\n",
      "[('#', 4535), ('time', 936), ('country', 705), ('way', 694), ('today', 674), ('world', 651), ('lot', 622), ('government', 617), ('state', 565), ('year', 557)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 1018), ('many', 810), ('new', 795), ('good', 556), ('last', 553), ('right', 497), ('first', 497), ('political', 471), ('american', 424), ('same', 379)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 3100), ('have', 1255), ('do', 942), ('get', 744), ('go', 596), ('say', 553), ('make', 534), ('take', 515), ('see', 509), ('know', 435)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "years = sorted(data['year'].unique())\n",
    "get_most_common_POS(data, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "Most Common Nouns\n",
      "[('time', 219), ('year', 205), ('world', 181), ('state', 149), ('city', 144), ('photo', 144), ('government', 141), ('country', 128), ('family', 119), ('money', 117)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 254), ('new', 221), ('many', 201), ('such', 159), ('american', 157), ('first', 126), ('old', 123), ('own', 123), ('last', 117), ('foreign', 114)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 678), ('have', 224), ('do', 116), ('get', 99), ('take', 89), ('make', 80), ('see', 74), ('go', 72), ('keep', 51), ('come', 44)]\n",
      "\n",
      "1991\n",
      "Most Common Nouns\n",
      "[('time', 231), ('war', 152), ('world', 150), ('year', 133), ('photo', 116), ('way', 103), ('day', 102), ('man', 95), ('government', 92), ('country', 87)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 209), ('new', 161), ('first', 153), ('many', 147), ('last', 145), ('old', 141), ('american', 122), ('own', 111), ('few', 103), ('little', 99)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 474), ('have', 201), ('do', 107), ('get', 89), ('take', 80), ('make', 79), ('go', 73), ('come', 62), ('see', 61), ('say', 38)]\n",
      "\n",
      "1992\n",
      "Most Common Nouns\n",
      "[('time', 183), ('world', 158), ('year', 135), ('country', 126), ('government', 123), ('way', 113), ('war', 105), ('life', 93), ('water', 93), ('day', 86)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 208), ('many', 194), ('american', 171), ('first', 124), ('political', 120), ('new', 116), ('last', 106), ('own', 99), ('old', 97), ('such', 91)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 560), ('have', 203), ('do', 93), ('get', 74), ('take', 63), ('make', 60), ('see', 51), ('go', 51), ('say', 42), ('find', 38)]\n",
      "\n",
      "1993\n",
      "Most Common Nouns\n",
      "[('world', 166), ('year', 141), ('time', 140), ('war', 111), ('government', 100), ('way', 88), ('life', 82), ('refugee', 81), ('company', 80), ('man', 72)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 144), ('many', 121), ('first', 118), ('new', 116), ('political', 105), ('last', 86), ('old', 83), ('american', 70), ('social', 70), ('few', 69)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 417), ('have', 144), ('do', 70), ('make', 63), ('get', 61), ('go', 58), ('see', 51), ('take', 45), ('come', 42), ('help', 34)]\n",
      "\n",
      "1994\n",
      "Most Common Nouns\n",
      "[('world', 214), ('year', 192), ('war', 183), ('state', 174), ('time', 165), ('life', 153), ('government', 145), ('country', 128), ('percent', 128), ('way', 122)]\n",
      "\n",
      "Most Common Adj\n",
      "[('new', 285), ('other', 263), ('many', 217), ('american', 184), ('such', 158), ('political', 137), ('first', 136), ('-', 136), ('last', 125), ('economic', 121)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 784), ('have', 232), ('do', 118), ('make', 81), ('get', 70), ('take', 66), ('go', 63), ('come', 50), ('see', 46), ('give', 45)]\n",
      "\n",
      "1995\n",
      "Most Common Nouns\n",
      "[('time', 188), ('world', 175), ('war', 170), ('family', 155), ('year', 146), ('church', 138), ('way', 122), ('child', 119), ('government', 118), ('life', 115)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 264), ('many', 201), ('american', 199), ('new', 170), ('first', 141), ('black', 140), ('such', 119), ('own', 110), ('bosnian', 110), ('last', 106)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 583), ('have', 214), ('do', 100), ('get', 86), ('take', 82), ('make', 71), ('go', 67), ('see', 66), ('help', 49), ('come', 48)]\n",
      "\n",
      "1996\n",
      "Most Common Nouns\n",
      "[('fishing', 233), ('hunting', 203), ('time', 131), ('contact', 127), ('life', 87), ('world', 82), ('way', 81), ('war', 81), ('military', 76), ('government', 75)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 132), ('other', 128), ('american', 100), ('foreign', 87), ('first', 80), ('political', 79), ('new', 78), ('such', 77), ('military', 77), ('own', 74)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 305), ('have', 110), ('do', 83), ('get', 59), ('make', 57), ('go', 50), ('see', 38), ('give', 35), ('take', 35), ('know', 33)]\n",
      "\n",
      "1997\n",
      "Most Common Nouns\n",
      "[('time', 145), ('war', 116), ('world', 105), ('military', 102), ('way', 98), ('government', 76), ('year', 76), ('life', 74), ('company', 74), ('family', 70)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 165), ('many', 128), ('new', 127), ('own', 97), ('military', 97), ('first', 84), ('such', 82), ('american', 82), ('same', 81), ('old', 74)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 471), ('have', 142), ('get', 81), ('do', 80), ('see', 69), ('make', 51), ('take', 44), ('go', 40), ('know', 39), ('keep', 28)]\n",
      "\n",
      "1998\n",
      "Most Common Nouns\n",
      "[('time', 123), ('world', 100), ('way', 89), ('life', 71), ('war', 70), ('year', 68), ('day', 62), ('man', 60), ('piano', 55), ('work', 54)]\n",
      "\n",
      "Most Common Adj\n",
      "[('first', 111), ('new', 106), ('other', 93), ('many', 92), ('old', 80), ('human', 74), ('little', 70), ('black', 69), ('own', 63), ('american', 62)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 295), ('have', 124), ('see', 67), ('do', 63), ('get', 63), ('make', 53), ('go', 52), ('take', 44), ('come', 36), ('know', 33)]\n",
      "\n",
      "1999\n",
      "Most Common Nouns\n",
      "[('time', 137), ('year', 114), ('world', 111), ('war', 107), ('government', 104), ('day', 96), ('country', 93), ('oil', 87), ('life', 82), ('way', 82)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 158), ('new', 119), ('many', 110), ('first', 96), ('old', 91), ('big', 87), ('last', 86), ('few', 77), ('good', 75), ('own', 74)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 387), ('have', 124), ('make', 77), ('do', 64), ('get', 58), ('take', 45), ('go', 37), ('know', 37), ('find', 31), ('give', 31)]\n",
      "\n",
      "2000\n",
      "Most Common Nouns\n",
      "[('war', 132), ('time', 104), ('world', 97), ('day', 76), ('government', 68), ('way', 65), ('year', 58), ('work', 51), ('city', 50), ('life', 42)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 116), ('many', 94), ('new', 70), ('first', 70), ('french', 68), ('black', 66), ('american', 66), ('own', 59), ('few', 58), ('political', 58)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 238), ('have', 70), ('see', 55), ('do', 53), ('get', 43), ('make', 37), ('take', 35), ('go', 33), ('know', 29), ('let', 21)]\n",
      "\n",
      "2001\n",
      "Most Common Nouns\n",
      "[('time', 209), ('year', 187), ('world', 180), ('war', 153), ('way', 130), ('life', 122), ('country', 109), ('family', 89), ('day', 85), ('m', 77)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 194), ('many', 144), ('new', 142), ('first', 122), ('old', 108), ('last', 105), ('american', 99), ('little', 98), ('good', 88), ('own', 87)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 501), ('have', 214), ('do', 152), ('get', 99), ('make', 78), ('see', 77), ('go', 75), ('say', 60), ('take', 56), ('think', 46)]\n",
      "\n",
      "2002\n",
      "Most Common Nouns\n",
      "[('world', 198), ('war', 158), ('time', 129), ('country', 107), ('life', 107), ('way', 97), ('day', 85), ('water', 84), ('government', 78), ('year', 69)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 166), ('many', 157), ('new', 143), ('american', 106), ('military', 93), ('first', 82), ('such', 74), ('old', 70), ('few', 68), ('economic', 64)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 418), ('have', 126), ('do', 71), ('see', 67), ('take', 64), ('go', 58), ('make', 53), ('get', 53), ('help', 37), ('know', 31)]\n",
      "\n",
      "2003\n",
      "Most Common Nouns\n",
      "[('world', 201), ('year', 173), ('%', 171), ('time', 152), ('photo', 136), ('family', 127), ('color', 127), ('government', 119), ('water', 109), ('life', 105)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 189), ('new', 153), ('many', 150), ('first', 147), ('old', 105), ('last', 94), ('such', 91), ('american', 81), ('high', 77), ('former', 74)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 495), ('have', 135), ('get', 73), ('do', 71), ('make', 66), ('go', 55), ('take', 53), ('find', 51), ('help', 51), ('see', 47)]\n",
      "\n",
      "2004\n",
      "Most Common Nouns\n",
      "[('war', 118), ('city', 94), ('time', 89), ('world', 77), ('way', 76), ('peace', 71), ('year', 67), ('life', 64), ('day', 63), ('power', 63)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 157), ('other', 130), ('new', 96), ('first', 80), ('american', 74), ('old', 66), ('few', 64), ('own', 58), ('local', 57), ('last', 56)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 268), ('have', 90), ('do', 45), ('make', 42), ('find', 41), ('see', 35), ('take', 34), ('get', 30), ('give', 23), ('know', 23)]\n",
      "\n",
      "2005\n",
      "Most Common Nouns\n",
      "[('war', 172), ('time', 109), ('world', 108), ('government', 77), ('city', 75), ('man', 60), ('population', 55), ('day', 54), ('part', 54), ('way', 54)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 131), ('other', 129), ('american', 102), ('nuclear', 101), ('first', 87), ('military', 75), ('great', 75), ('new', 73), ('old', 71), ('korean', 70)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 366), ('have', 147), ('do', 82), ('make', 66), ('take', 57), ('see', 50), ('get', 49), ('say', 41), ('know', 36), ('go', 34)]\n",
      "\n",
      "2006\n",
      "Most Common Nouns\n",
      "[('time', 163), ('world', 154), ('city', 154), ('government', 120), ('country', 119), ('day', 119), ('way', 109), ('fire', 104), ('house', 95), ('year', 95)]\n",
      "\n",
      "Most Common Adj\n",
      "[('new', 180), ('many', 166), ('other', 158), ('old', 110), ('american', 98), ('last', 92), ('own', 91), ('political', 88), ('same', 82), ('first', 80)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 537), ('have', 196), ('do', 98), ('make', 88), ('get', 83), ('take', 73), ('go', 71), ('come', 67), ('see', 63), ('know', 49)]\n",
      "\n",
      "2007\n",
      "Most Common Nouns\n",
      "[('world', 185), ('war', 163), ('year', 152), ('time', 146), ('city', 139), ('country', 128), ('government', 113), ('day', 112), ('water', 108), ('life', 99)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 162), ('new', 142), ('many', 127), ('first', 121), ('old', 118), ('own', 94), ('such', 88), ('last', 88), ('global', 81), ('few', 80)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 462), ('have', 152), ('do', 118), ('get', 81), ('make', 75), ('see', 74), ('take', 69), ('help', 62), ('say', 57), ('go', 53)]\n",
      "\n",
      "2008\n",
      "Most Common Nouns\n",
      "[('time', 123), ('war', 119), ('water', 90), ('world', 87), ('government', 86), ('life', 84), ('year', 81), ('country', 77), ('day', 76), ('family', 69)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 126), ('many', 113), ('new', 113), ('first', 82), ('own', 72), ('american', 64), ('last', 58), ('military', 57), ('iraqi', 57), ('little', 56)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 304), ('have', 110), ('do', 57), ('make', 52), ('get', 42), ('take', 42), ('see', 40), ('go', 38), ('come', 27), ('help', 26)]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "Most Common Nouns\n",
      "[('time', 120), ('war', 105), ('family', 99), ('life', 93), ('way', 90), ('mother', 79), ('world', 79), ('show', 78), ('idf', 71), ('church', 68)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 109), ('many', 102), ('new', 88), ('israeli', 80), ('good', 79), ('jewish', 71), ('first', 69), ('own', 63), ('old', 60), ('little', 59)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 295), ('have', 118), ('do', 65), ('make', 60), ('get', 53), ('say', 51), ('see', 50), ('know', 44), ('take', 37), ('go', 29)]\n",
      "\n",
      "2010\n",
      "Most Common Nouns\n",
      "[('time', 207), ('year', 153), ('wife', 143), ('war', 124), ('-', 112), ('world', 104), ('mail', 102), ('class', 101), ('life', 96), ('climate', 91)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 144), ('other', 139), ('new', 137), ('last', 125), ('first', 109), ('good', 82), ('few', 79), ('old', 75), ('great', 72), ('next', 64)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 400), ('have', 116), ('do', 63), ('make', 62), ('get', 62), ('see', 55), ('take', 53), ('go', 46), ('know', 37), ('work', 35)]\n",
      "\n",
      "2011\n",
      "Most Common Nouns\n",
      "[('war', 139), ('time', 114), ('life', 95), ('way', 90), ('world', 90), ('day', 86), ('family', 84), ('marion', 81), ('city', 75), ('peace', 74)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 134), ('many', 93), ('new', 93), ('first', 89), ('old', 70), ('few', 69), ('british', 65), ('american', 59), ('military', 58), ('such', 52)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 323), ('have', 112), ('do', 69), ('see', 57), ('make', 54), ('go', 45), ('take', 43), ('get', 30), ('know', 29), ('say', 28)]\n",
      "\n",
      "2012\n",
      "Most Common Nouns\n",
      "[('#', 288), ('war', 157), ('time', 112), ('world', 74), ('way', 68), ('government', 62), ('year', 58), ('day', 57), ('city', 55), ('country', 54)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 104), ('many', 103), ('first', 90), ('new', 71), ('old', 65), ('jewish', 53), ('political', 48), ('few', 47), ('human', 47), ('-', 44)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 209), ('have', 72), ('do', 48), ('take', 41), ('get', 36), ('make', 34), ('see', 32), ('help', 30), ('go', 28), ('find', 28)]\n",
      "\n",
      "2013\n",
      "Most Common Nouns\n",
      "[('#', 961), ('time', 172), ('war', 121), ('day', 114), ('year', 111), ('city', 110), ('world', 108), ('state', 108), ('country', 102), ('government', 88)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 184), ('many', 154), ('new', 112), ('first', 104), ('old', 81), ('political', 75), ('young', 72), ('few', 71), ('such', 69), ('high', 69)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 435), ('have', 139), ('do', 97), ('make', 70), ('go', 58), ('see', 52), ('take', 52), ('get', 48), ('#', 44), ('help', 38)]\n",
      "\n",
      "2014\n",
      "Most Common Nouns\n",
      "[('#', 1359), ('time', 216), ('government', 178), ('war', 170), ('year', 160), ('world', 159), ('state', 153), ('family', 152), ('country', 148), ('day', 148)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 257), ('other', 232), ('new', 176), ('old', 137), ('first', 134), ('such', 116), ('young', 113), ('own', 102), ('few', 101), ('american', 97)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 513), ('have', 197), ('do', 101), ('see', 96), ('get', 91), ('make', 82), ('take', 71), ('go', 71), ('#', 58), ('come', 57)]\n",
      "\n",
      "2015\n",
      "Most Common Nouns\n",
      "[('#', 1440), ('world', 240), ('time', 228), ('state', 202), ('year', 161), ('war', 160), ('government', 157), ('life', 157), ('city', 152), ('country', 152)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 264), ('new', 260), ('other', 251), ('-', 177), ('political', 152), ('such', 149), ('first', 146), ('few', 143), ('old', 130), ('jewish', 126)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 701), ('have', 229), ('do', 133), ('make', 107), ('get', 93), ('see', 92), ('take', 92), ('go', 86), ('#', 59), ('say', 57)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre = 'MAG'\n",
    "genre_years = sorted(data[data['genre']==genre]['year'].unique())\n",
    "get_most_common_POS(data, genre_years, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "Most Common Nouns\n",
      "[('today', 699), ('government', 630), ('time', 625), ('country', 539), ('world', 506), ('way', 466), ('war', 448), ('-', 333), ('lot', 306), ('fact', 295)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 714), ('many', 430), ('good', 417), ('new', 392), ('american', 361), ('last', 359), ('military', 349), ('iraqi', 334), ('soviet', 310), ('first', 276)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2407), ('have', 974), ('do', 636), ('get', 524), ('think', 424), ('go', 422), ('say', 374), ('make', 330), ('let', 324), ('take', 306)]\n",
      "\n",
      "1991\n",
      "Most Common Nouns\n",
      "[('war', 951), ('today', 893), ('time', 870), ('government', 755), ('country', 707), ('way', 646), ('world', 493), ('question', 461), ('president', 461), ('m', 457)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 940), ('many', 632), ('good', 625), ('iraqi', 579), ('american', 553), ('last', 514), ('new', 489), ('first', 401), ('political', 394), ('military', 374)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 3613), ('have', 1593), ('do', 1030), ('get', 715), ('go', 713), ('think', 628), ('say', 625), ('let', 552), ('take', 504), ('see', 502)]\n",
      "\n",
      "1992\n",
      "Most Common Nouns\n",
      "[('today', 519), ('time', 507), ('country', 377), ('government', 372), ('way', 336), ('year', 321), ('president', 315), ('lot', 312), ('percent', 281), ('m', 263)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 484), ('many', 387), ('good', 383), ('last', 343), ('new', 330), ('first', 306), ('little', 266), ('political', 245), ('american', 235), ('next', 173)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1847), ('have', 847), ('do', 652), ('get', 521), ('go', 432), ('think', 343), ('make', 327), ('say', 319), ('let', 307), ('see', 294)]\n",
      "\n",
      "1993\n",
      "Most Common Nouns\n",
      "[('time', 560), ('today', 556), ('way', 460), ('year', 372), ('voice', 365), ('country', 365), ('president', 341), ('m', 324), ('world', 323), ('man', 308)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 562), ('many', 421), ('good', 393), ('last', 358), ('new', 342), ('american', 315), ('first', 268), ('little', 219), ('political', 209), ('right', 200)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2116), ('have', 900), ('do', 722), ('get', 479), ('think', 381), ('go', 379), ('say', 354), ('see', 325), ('make', 322), ('take', 283)]\n",
      "\n",
      "1994\n",
      "Most Common Nouns\n",
      "[('voice', 923), ('time', 902), ('today', 727), ('government', 711), ('president', 677), ('way', 642), ('country', 639), ('health', 549), ('policy', 546), ('lot', 493)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 912), ('good', 608), ('new', 578), ('last', 549), ('many', 546), ('american', 536), ('first', 421), ('little', 357), ('right', 335), ('political', 335)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 3240), ('have', 1514), ('do', 1105), ('get', 874), ('go', 680), ('think', 587), ('say', 574), ('let', 573), ('see', 518), ('make', 488)]\n",
      "\n",
      "1995\n",
      "Most Common Nouns\n",
      "[('today', 279), ('voice', 278), ('time', 265), ('way', 222), ('country', 196), ('president', 191), ('year', 187), ('m', 172), ('lot', 168), ('peace', 164)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 284), ('good', 205), ('american', 198), ('many', 179), ('new', 175), ('first', 166), ('last', 141), ('right', 117), ('important', 107), ('great', 106)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1111), ('have', 496), ('do', 384), ('say', 253), ('get', 247), ('go', 239), ('let', 219), ('think', 186), ('make', 171), ('see', 167)]\n",
      "\n",
      "1996\n",
      "Most Common Nouns\n",
      "[('today', 459), ('time', 454), ('way', 367), ('year', 335), ('government', 317), ('voice', 279), ('country', 276), ('president', 264), ('lot', 246), ('-', 241)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 432), ('good', 332), ('new', 326), ('american', 301), ('last', 297), ('many', 272), ('first', 215), ('right', 180), ('next', 169), ('important', 165)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1648), ('have', 784), ('do', 592), ('get', 436), ('go', 327), ('say', 291), ('think', 278), ('let', 272), ('make', 263), ('see', 244)]\n",
      "\n",
      "1997\n",
      "Most Common Nouns\n",
      "[('time', 294), ('king', 279), ('way', 187), ('part', 177), ('country', 168), ('today', 166), ('lot', 157), ('president', 138), ('year', 136), ('care', 133)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 236), ('new', 186), ('good', 158), ('last', 148), ('american', 146), ('many', 125), ('first', 123), ('political', 115), ('great', 104), ('little', 94)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 855), ('have', 352), ('do', 246), ('get', 210), ('think', 180), ('go', 154), ('see', 137), ('take', 131), ('make', 131), ('say', 125)]\n",
      "\n",
      "1998\n",
      "Most Common Nouns\n",
      "[('king', 301), ('time', 255), ('footage', 208), ('way', 183), ('man', 182), ('lot', 179), ('m', 172), ('today', 158), ('dole', 155), ('day', 145)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 198), ('good', 195), ('many', 183), ('last', 170), ('great', 99), ('new', 98), ('little', 96), ('long', 92), ('first', 88), ('right', 88)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 791), ('have', 424), ('do', 317), ('go', 183), ('get', 183), ('think', 158), ('say', 155), ('know', 150), ('let', 145), ('make', 135)]\n",
      "\n",
      "1999\n",
      "Most Common Nouns\n",
      "[('time', 485), ('war', 461), ('way', 391), ('lot', 363), ('m', 336), ('question', 300), ('air', 295), ('today', 291), ('ground', 276), ('country', 271)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 457), ('many', 363), ('good', 352), ('last', 267), ('first', 240), ('american', 229), ('little', 198), ('military', 196), ('own', 193), ('important', 191)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1846), ('have', 806), ('do', 577), ('get', 451), ('go', 439), ('say', 394), ('think', 318), ('let', 306), ('see', 299), ('make', 286)]\n",
      "\n",
      "2000\n",
      "Most Common Nouns\n",
      "[('voice', 252), ('end', 241), ('time', 235), ('government', 216), ('today', 197), ('peace', 186), ('world', 162), ('year', 152), ('way', 150), ('country', 131)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 231), ('new', 203), ('israeli', 198), ('palestinian', 196), ('many', 191), ('last', 187), ('political', 149), ('first', 131), ('russian', 119), ('old', 108)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 782), ('have', 354), ('begin', 191), ('do', 170), ('get', 161), ('go', 149), ('see', 138), ('say', 131), ('make', 125), ('take', 124)]\n",
      "\n",
      "2001\n",
      "Most Common Nouns\n",
      "[('time', 308), ('war', 210), ('country', 201), ('way', 200), ('today', 179), ('lot', 178), ('world', 177), ('government', 160), ('king', 135), ('part', 130)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 210), ('unidentified', 197), ('many', 196), ('rich', 186), ('last', 162), ('good', 159), ('military', 153), ('american', 142), ('first', 128), ('new', 126)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 898), ('have', 402), ('do', 287), ('go', 214), ('get', 197), ('see', 175), ('take', 164), ('say', 162), ('know', 153), ('think', 140)]\n",
      "\n",
      "2002\n",
      "Most Common Nouns\n",
      "[('time', 278), ('world', 234), ('way', 222), ('war', 214), ('today', 208), ('lot', 195), ('male', 175), ('day', 171), ('end', 169), ('year', 155)]\n",
      "\n",
      "Most Common Adj\n",
      "[('unidentified', 342), ('israeli', 328), ('other', 262), ('palestinian', 257), ('many', 202), ('new', 164), ('last', 156), ('own', 131), ('good', 128), ('little', 116)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 918), ('have', 360), ('do', 260), ('get', 220), ('go', 213), ('see', 158), ('say', 156), ('make', 149), ('take', 146), ('begin', 132)]\n",
      "\n",
      "2003\n",
      "Most Common Nouns\n",
      "[('war', 381), ('time', 257), ('way', 187), ('male', 168), ('end', 166), ('government', 163), ('today', 160), ('lot', 153), ('world', 152), ('country', 145)]\n",
      "\n",
      "Most Common Adj\n",
      "[('unidentified', 236), ('other', 215), ('american', 194), ('many', 178), ('good', 157), ('iraqi', 148), ('last', 128), ('unintelligible', 124), ('new', 117), ('first', 113)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 872), ('have', 357), ('do', 253), ('get', 225), ('go', 174), ('say', 150), ('think', 148), ('see', 135), ('make', 131), ('begin', 120)]\n",
      "\n",
      "2004\n",
      "Most Common Nouns\n",
      "[('time', 211), ('war', 170), ('way', 153), ('country', 147), ('today', 147), ('lot', 146), ('government', 129), ('story', 127), ('end', 118), ('morning', 110)]\n",
      "\n",
      "Most Common Adj\n",
      "[('unidentified', 180), ('other', 164), ('new', 150), ('last', 136), ('good', 120), ('american', 103), ('first', 102), ('many', 101), ('unintelligible', 101), ('iraqi', 90)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 626), ('have', 279), ('do', 164), ('go', 135), ('get', 134), ('see', 130), ('know', 118), ('say', 112), ('take', 112), ('think', 103)]\n",
      "\n",
      "2005\n",
      "Most Common Nouns\n",
      "[('time', 398), ('today', 241), ('lot', 238), ('day', 221), ('water', 216), ('end', 213), ('way', 209), ('world', 207), ('life', 202), ('country', 197)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 324), ('unidentified', 307), ('other', 250), ('good', 230), ('first', 208), ('new', 204), ('last', 200), ('little', 185), ('old', 135), ('american', 123)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1198), ('have', 564), ('get', 424), ('do', 359), ('go', 280), ('see', 214), ('take', 210), ('know', 177), ('come', 165), ('begin', 163)]\n",
      "\n",
      "2006\n",
      "Most Common Nouns\n",
      "[('time', 433), ('today', 301), ('end', 262), ('lot', 259), ('way', 257), ('day', 253), ('world', 246), ('country', 238), ('videotape', 218), ('video', 214)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 304), ('many', 246), ('israeli', 238), ('unidentified', 228), ('new', 224), ('good', 210), ('last', 200), ('first', 191), ('little', 173), ('next', 139)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1127), ('have', 502), ('get', 383), ('do', 317), ('see', 287), ('go', 247), ('take', 227), ('begin', 214), ('say', 182), ('make', 173)]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n",
      "Most Common Nouns\n",
      "[('time', 274), ('today', 227), ('war', 226), ('government', 196), ('way', 179), ('lot', 159), ('king', 152), ('world', 144), ('something', 140), ('year', 139)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 240), ('many', 187), ('american', 150), ('good', 144), ('new', 137), ('last', 134), ('political', 123), ('iraqi', 97), ('same', 95), ('off', 92)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 840), ('have', 447), ('do', 290), ('get', 206), ('go', 179), ('think', 153), ('see', 148), ('say', 145), ('know', 139), ('make', 132)]\n",
      "\n",
      "2008\n",
      "Most Common Nouns\n",
      "[('today', 109), ('time', 93), ('hemmer', 93), ('footage', 89), ('government', 81), ('lot', 59), ('health', 57), ('money', 54), ('part', 53), ('way', 52)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 95), ('other', 70), ('new', 67), ('good', 65), ('last', 46), ('american', 41), ('big', 39), ('same', 38), ('little', 34), ('much', 31)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 322), ('have', 129), ('do', 95), ('get', 91), ('go', 74), ('say', 71), ('see', 63), ('let', 61), ('make', 60), ('think', 56)]\n",
      "\n",
      "2009\n",
      "Most Common Nouns\n",
      "[('time', 124), ('amanpour', 124), ('today', 119), ('male', 116), ('lot', 86), ('way', 76), ('country', 72), ('war', 72), ('health', 63), ('something', 61)]\n",
      "\n",
      "Most Common Adj\n",
      "[('unidentified', 168), ('other', 122), ('many', 93), ('young', 81), ('new', 79), ('american', 72), ('last', 66), ('old', 63), ('good', 59), ('next', 51)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 388), ('have', 171), ('do', 116), ('go', 94), ('see', 81), ('get', 77), ('know', 76), ('say', 66), ('think', 64), ('make', 59)]\n",
      "\n",
      "2010\n",
      "Most Common Nouns\n",
      "[('today', 153), ('time', 121), ('lot', 108), ('country', 107), ('government', 106), ('way', 95), ('money', 73), ('world', 69), ('year', 69), ('tax', 68)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 123), ('good', 88), ('new', 84), ('many', 83), ('last', 66), ('little', 62), ('big', 53), ('first', 50), ('few', 49), ('own', 48)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 359), ('do', 134), ('have', 133), ('get', 129), ('go', 88), ('make', 73), ('see', 63), ('take', 62), ('think', 58), ('say', 57)]\n",
      "\n",
      "2011\n",
      "Most Common Nouns\n",
      "[('time', 253), ('today', 247), ('government', 210), ('lot', 200), ('way', 167), ('tonight', 164), ('day', 154), ('country', 148), ('man', 132), ('m', 129)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 221), ('new', 188), ('good', 173), ('last', 161), ('many', 145), ('first', 125), ('right', 95), ('political', 85), ('little', 84), ('big', 81)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 645), ('have', 303), ('do', 225), ('get', 224), ('see', 174), ('make', 157), ('go', 147), ('think', 128), ('say', 124), ('let', 123)]\n",
      "\n",
      "2012\n",
      "Most Common Nouns\n",
      "[('#', 1274), ('today', 379), ('time', 340), ('lot', 298), ('government', 275), ('way', 269), ('president', 262), ('year', 241), ('country', 219), ('day', 216)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 276), ('new', 254), ('last', 237), ('good', 233), ('many', 211), ('right', 189), ('big', 182), ('first', 163), ('little', 154), ('next', 132)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1125), ('have', 564), ('do', 398), ('get', 307), ('say', 250), ('go', 248), ('make', 223), ('see', 221), ('think', 190), ('take', 183)]\n",
      "\n",
      "2013\n",
      "Most Common Nouns\n",
      "[('#', 2325), ('time', 489), ('today', 416), ('lot', 399), ('government', 347), ('way', 328), ('president', 313), ('world', 303), ('country', 295), ('something', 268)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 433), ('new', 315), ('right', 302), ('good', 301), ('many', 282), ('last', 264), ('american', 257), ('little', 209), ('big', 197), ('first', 194)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1638), ('have', 753), ('do', 518), ('get', 488), ('go', 361), ('see', 346), ('let', 307), ('make', 296), ('say', 278), ('take', 258)]\n",
      "\n",
      "2014\n",
      "Most Common Nouns\n",
      "[('#', 2462), ('time', 569), ('lot', 501), ('country', 438), ('government', 437), ('today', 434), ('way', 418), ('president', 384), ('world', 311), ('something', 303)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 428), ('good', 363), ('new', 355), ('right', 335), ('last', 317), ('many', 314), ('american', 273), ('gross', 261), ('big', 246), ('first', 242)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 1944), ('have', 829), ('do', 653), ('get', 553), ('see', 379), ('go', 340), ('say', 340), ('take', 327), ('make', 315), ('let', 312)]\n",
      "\n",
      "2015\n",
      "Most Common Nouns\n",
      "[('#', 3094), ('time', 579), ('today', 538), ('lot', 536), ('way', 497), ('m', 461), ('something', 413), ('president', 401), ('country', 396), ('video', 374)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 619), ('good', 434), ('right', 417), ('new', 387), ('many', 381), ('last', 348), ('great', 265), ('first', 258), ('big', 255), ('american', 251)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 2058), ('have', 912), ('do', 750), ('get', 588), ('say', 474), ('go', 458), ('see', 376), ('know', 362), ('think', 360), ('take', 359)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre = 'SPOK'\n",
    "genre_years = sorted(data[data['genre']==genre]['year'].unique())\n",
    "get_most_common_POS(data, genre_years, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "Most Common Nouns\n",
      "[('government', 169), ('time', 136), ('war', 128), ('today', 118), ('world', 115), ('year', 115), ('country', 114), ('city', 105), ('life', 93), ('money', 90)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 227), ('iraqi', 191), ('many', 189), ('new', 149), ('last', 124), ('old', 122), ('-', 121), ('american', 120), ('first', 111), ('soviet', 108)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 504), ('have', 136), ('do', 101), ('take', 87), ('get', 86), ('make', 66), ('go', 66), ('help', 58), ('know', 52), ('see', 44)]\n",
      "\n",
      "1991\n",
      "Most Common Nouns\n",
      "[('government', 259), ('war', 246), ('%', 195), ('time', 181), ('year', 170), ('border', 148), ('world', 147), ('way', 144), ('peace', 133), ('country', 131)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 294), ('other', 288), ('iraqi', 256), ('new', 182), ('american', 166), ('political', 164), ('kurdish', 159), ('last', 151), ('first', 133), ('military', 125)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 696), ('have', 201), ('do', 117), ('go', 88), ('take', 86), ('get', 84), ('make', 83), ('see', 76), ('help', 58), ('say', 46)]\n",
      "\n",
      "1992\n",
      "Most Common Nouns\n",
      "[('government', 183), ('country', 163), ('time', 145), ('war', 136), ('year', 135), ('world', 132), ('percent', 116), ('state', 102), ('city', 102), ('life', 94)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 215), ('many', 181), ('new', 176), ('last', 147), ('former', 117), ('old', 110), ('political', 109), ('black', 99), ('american', 97), ('such', 89)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 457), ('have', 165), ('do', 78), ('get', 77), ('make', 73), ('take', 73), ('go', 53), ('see', 46), ('know', 42), ('help', 38)]\n",
      "\n",
      "1993\n",
      "Most Common Nouns\n",
      "[('war', 310), ('time', 219), ('government', 215), ('year', 173), ('city', 171), ('peace', 159), ('world', 152), ('family', 139), ('country', 130), ('state', 125)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 256), ('many', 247), ('new', 206), ('last', 189), ('american', 160), ('russian', 153), ('old', 142), ('political', 130), ('first', 126), ('bosnian', 125)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 621), ('have', 216), ('do', 133), ('make', 102), ('take', 92), ('get', 91), ('go', 73), ('help', 67), ('see', 65), ('know', 61)]\n",
      "\n",
      "1994\n",
      "Most Common Nouns\n",
      "[('government', 181), ('time', 163), ('war', 160), ('year', 145), ('day', 124), ('country', 119), ('town', 112), ('state', 109), ('percent', 97), ('world', 96)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 207), ('other', 177), ('new', 170), ('last', 133), ('first', 123), ('military', 106), ('old', 104), ('political', 94), ('former', 94), ('bosnian', 81)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 487), ('have', 168), ('go', 87), ('get', 82), ('take', 67), ('do', 60), ('see', 56), ('make', 55), ('help', 47), ('come', 43)]\n",
      "\n",
      "1995\n",
      "Most Common Nouns\n",
      "[('war', 182), ('government', 174), ('world', 159), ('year', 153), ('time', 130), ('peace', 115), ('country', 110), ('city', 100), ('percent', 100), ('day', 91)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 182), ('new', 161), ('many', 148), ('first', 140), ('last', 131), ('bosnian', 124), ('serb', 116), ('old', 106), ('former', 95), ('international', 87)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 486), ('have', 136), ('do', 79), ('get', 65), ('take', 64), ('go', 59), ('make', 55), ('find', 42), ('say', 37), ('help', 34)]\n",
      "\n",
      "1996\n",
      "Most Common Nouns\n",
      "[('year', 174), ('government', 159), ('time', 159), ('%', 149), ('percent', 128), ('war', 126), ('country', 122), ('day', 118), ('today', 108), ('world', 105)]\n",
      "\n",
      "Most Common Adj\n",
      "[('new', 183), ('first', 181), ('other', 163), ('last', 148), ('many', 126), ('military', 116), ('american', 100), ('former', 97), ('old', 93), ('israeli', 92)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 518), ('have', 183), ('do', 85), ('take', 81), ('help', 65), ('get', 64), ('go', 57), ('make', 55), ('give', 37), ('keep', 37)]\n",
      "\n",
      "1997\n",
      "Most Common Nouns\n",
      "[('government', 190), ('war', 147), ('year', 129), ('time', 119), ('percent', 113), ('country', 92), ('world', 87), ('day', 75), ('money', 72), ('aid', 69)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 158), ('other', 157), ('new', 153), ('first', 111), ('last', 110), ('old', 90), ('political', 85), ('former', 74), ('american', 65), ('few', 60)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 364), ('have', 129), ('do', 63), ('get', 52), ('make', 44), ('take', 41), ('go', 41), ('keep', 31), ('come', 28), ('help', 28)]\n",
      "\n",
      "1998\n",
      "Most Common Nouns\n",
      "[('government', 168), ('year', 150), ('time', 147), ('percent', 117), ('day', 102), ('family', 96), ('week', 89), ('war', 84), ('life', 82), ('country', 77)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 159), ('many', 146), ('last', 131), ('first', 123), ('new', 115), ('israeli', 102), ('old', 90), ('american', 80), ('palestinian', 75), ('few', 61)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 358), ('have', 123), ('do', 59), ('go', 57), ('make', 55), ('get', 55), ('take', 46), ('give', 44), ('help', 40), ('see', 32)]\n",
      "\n",
      "1999\n",
      "Most Common Nouns\n",
      "[('war', 225), ('time', 180), ('family', 131), ('year', 116), ('day', 113), ('world', 108), ('school', 103), ('country', 97), ('way', 88), ('peace', 88)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 204), ('many', 142), ('first', 134), ('last', 130), ('new', 129), ('ethnic', 123), ('old', 114), ('american', 106), ('serbian', 92), ('own', 90)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 495), ('have', 176), ('do', 111), ('get', 79), ('go', 73), ('take', 69), ('see', 64), ('help', 54), ('make', 54), ('know', 47)]\n",
      "\n",
      "2000\n",
      "Most Common Nouns\n",
      "[('government', 170), ('time', 167), ('year', 127), ('war', 126), ('family', 126), ('peace', 112), ('state', 94), ('country', 91), ('world', 90), ('way', 89)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 194), ('many', 160), ('last', 151), ('new', 145), ('israeli', 119), ('palestinian', 115), ('first', 107), ('american', 93), ('old', 79), ('political', 78)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 456), ('have', 134), ('get', 71), ('do', 67), ('go', 61), ('make', 57), ('take', 51), ('come', 44), ('help', 42), ('see', 41)]\n",
      "\n",
      "2001\n",
      "Most Common Nouns\n",
      "[('time', 261), ('world', 216), ('year', 191), ('day', 169), ('government', 169), ('war', 160), ('life', 158), ('country', 156), ('city', 154), ('+', 151)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 257), ('many', 212), ('last', 198), ('new', 193), ('first', 190), ('american', 127), ('old', 120), ('-', 114), ('few', 105), ('good', 104)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 688), ('have', 250), ('do', 146), ('get', 130), ('make', 114), ('see', 90), ('take', 87), ('go', 83), ('help', 69), ('find', 59)]\n",
      "\n",
      "2002\n",
      "Most Common Nouns\n",
      "[('director', 346), ('sex', 288), ('violence', 277), ('nudity', 229), ('staff', 228), ('time', 213), ('profanity', 187), ('year', 181), ('+', 170), ('war', 163)]\n",
      "\n",
      "Most Common Adj\n",
      "[('israeli', 349), ('palestinian', 239), ('other', 235), ('new', 203), ('many', 197), ('first', 169), ('last', 164), ('old', 113), ('young', 108), ('american', 107)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 519), ('have', 213), ('make', 112), ('do', 111), ('go', 94), ('take', 81), ('see', 73), ('get', 72), ('help', 49), ('find', 44)]\n",
      "\n",
      "2003\n",
      "Most Common Nouns\n",
      "[('director', 311), ('sex', 271), ('violence', 252), ('nudity', 241), ('war', 234), ('staff', 228), ('profanity', 197), ('drinking', 152), ('time', 145), ('day', 139)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 146), ('other', 140), ('iraqi', 119), ('first', 107), ('new', 101), ('last', 91), ('young', 88), ('american', 87), ('old', 83), ('high', 80)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 380), ('have', 132), ('get', 75), ('go', 71), ('do', 63), ('make', 53), ('take', 48), ('help', 46), ('find', 43), ('see', 43)]\n",
      "\n",
      "2004\n",
      "Most Common Nouns\n",
      "[('director', 322), ('violence', 177), ('time', 162), ('sex', 158), ('nudity', 128), ('profanity', 127), ('family', 119), ('min', 118), ('life', 113), ('world', 111)]\n",
      "\n",
      "Most Common Adj\n",
      "[('new', 141), ('other', 134), ('many', 128), ('palestinian', 121), ('first', 118), ('last', 89), ('american', 87), ('israeli', 82), ('old', 77), ('political', 68)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 354), ('have', 138), ('do', 77), ('make', 75), ('get', 66), ('take', 55), ('see', 54), ('help', 47), ('find', 40), ('know', 38)]\n",
      "\n",
      "2005\n",
      "Most Common Nouns\n",
      "[('director', 204), ('grade', 190), ('time', 140), ('family', 131), ('school', 103), ('life', 102), ('city', 99), ('government', 99), ('violence', 86), ('year', 85)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 163), ('new', 134), ('many', 127), ('last', 104), ('old', 78), ('first', 77), ('american', 64), ('few', 61), ('young', 60), ('high', 60)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 341), ('have', 137), ('see', 88), ('get', 77), ('make', 58), ('do', 54), ('take', 54), ('help', 52), ('go', 50), ('find', 38)]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "Most Common Nouns\n",
      "[('time', 160), ('city', 160), ('year', 143), ('government', 138), ('family', 131), ('war', 110), ('percent', 106), ('country', 103), ('day', 102), ('market', 93)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 217), ('other', 207), ('new', 182), ('israeli', 152), ('last', 122), ('palestinian', 117), ('first', 115), ('old', 113), ('american', 79), ('local', 70)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 476), ('have', 114), ('do', 95), ('get', 80), ('go', 69), ('take', 68), ('make', 66), ('help', 54), ('see', 45), ('know', 40)]\n",
      "\n",
      "2007\n",
      "Most Common Nouns\n",
      "[('time', 117), ('government', 110), ('year', 104), ('family', 90), ('life', 89), ('school', 84), ('city', 83), ('war', 78), ('world', 72), ('country', 71)]\n",
      "\n",
      "Most Common Adj\n",
      "[('new', 113), ('many', 96), ('other', 91), ('first', 87), ('last', 73), ('high', 65), ('former', 55), ('political', 54), ('iraqi', 52), ('arab', 50)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 338), ('have', 116), ('do', 65), ('make', 62), ('get', 62), ('go', 61), ('take', 38), ('help', 36), ('see', 33), ('find', 32)]\n",
      "\n",
      "2008\n",
      "Most Common Nouns\n",
      "[('time', 88), ('government', 80), ('country', 74), ('year', 72), ('life', 70), ('family', 65), ('war', 60), ('school', 58), ('state', 47), ('neighborhood', 47)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 97), ('other', 88), ('new', 78), ('old', 73), ('last', 68), ('first', 53), ('political', 43), ('little', 43), ('few', 40), ('good', 39)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 198), ('have', 69), ('do', 48), ('see', 44), ('go', 41), ('get', 41), ('make', 37), ('help', 34), ('take', 32), ('come', 30)]\n",
      "\n",
      "2009\n",
      "Most Common Nouns\n",
      "[('#', 215), ('government', 173), ('war', 106), ('peace', 73), ('state', 67), ('city', 66), ('time', 62), ('year', 61), ('country', 61), ('family', 57)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 109), ('many', 99), ('israeli', 87), ('new', 76), ('last', 64), ('military', 63), ('such', 52), ('high', 51), ('old', 50), ('political', 48)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 256), ('have', 79), ('make', 44), ('take', 40), ('get', 37), ('help', 36), ('do', 33), ('go', 31), ('bring', 27), ('see', 24)]\n",
      "\n",
      "2010\n",
      "Most Common Nouns\n",
      "[('#', 648), ('country', 118), ('government', 84), ('cream', 66), ('time', 64), ('ice', 63), ('year', 62), ('family', 54), ('city', 48), ('court', 47)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 97), ('other', 68), ('old', 65), ('new', 52), ('american', 51), ('first', 51), ('last', 47), ('international', 45), ('ethnic', 41), ('political', 40)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 216), ('have', 69), ('do', 39), ('take', 29), ('see', 26), ('say', 23), ('go', 22), ('#', 21), ('get', 20), ('make', 17)]\n",
      "\n",
      "2011\n",
      "Most Common Nouns\n",
      "[('#', 832), ('government', 118), ('country', 75), ('war', 75), ('peace', 71), ('state', 70), ('city', 66), ('time', 63), ('world', 62), ('regime', 58)]\n",
      "\n",
      "Most Common Adj\n",
      "[('palestinian', 104), ('other', 98), ('many', 86), ('new', 74), ('israeli', 72), ('syrian', 59), ('military', 57), ('last', 53), ('political', 44), ('foreign', 44)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 232), ('have', 58), ('get', 39), ('#', 38), ('do', 37), ('go', 30), ('make', 29), ('take', 25), ('see', 25), ('say', 25)]\n",
      "\n",
      "2012\n",
      "Most Common Nouns\n",
      "[('#', 347), ('government', 186), ('country', 94), ('sex', 81), ('time', 75), ('border', 75), ('trafficking', 65), ('year', 63), ('group', 61), ('way', 57)]\n",
      "\n",
      "Most Common Adj\n",
      "[('syrian', 144), ('other', 115), ('many', 94), ('american', 68), ('new', 59), ('last', 57), ('-', 54), ('military', 54), ('first', 54), ('human', 52)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 254), ('have', 67), ('take', 43), ('get', 43), ('do', 43), ('go', 32), ('help', 31), ('see', 29), ('make', 27), ('leave', 20)]\n",
      "\n",
      "2013\n",
      "Most Common Nouns\n",
      "[('government', 81), ('time', 78), ('country', 77), ('year', 76), ('war', 74), ('percent', 67), ('family', 55), ('state', 54), ('day', 54), ('life', 51)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 87), ('new', 86), ('other', 72), ('first', 67), ('last', 57), ('such', 51), ('american', 49), ('international', 43), ('syrian', 41), ('political', 41)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 236), ('have', 77), ('do', 40), ('get', 39), ('go', 38), ('help', 28), ('make', 27), ('take', 26), ('see', 24), ('find', 22)]\n",
      "\n",
      "2014\n",
      "Most Common Nouns\n",
      "[('year', 130), ('government', 112), ('time', 97), ('family', 93), ('country', 88), ('city', 86), ('border', 82), ('school', 82), ('market', 79), ('war', 73)]\n",
      "\n",
      "Most Common Adj\n",
      "[('other', 124), ('many', 119), ('new', 106), ('israeli', 92), ('last', 86), ('first', 84), ('such', 64), ('old', 63), ('american', 60), ('military', 48)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 291), ('have', 106), ('do', 75), ('take', 58), ('go', 57), ('make', 51), ('get', 46), ('help', 40), ('see', 32), ('find', 26)]\n",
      "\n",
      "2015\n",
      "Most Common Nouns\n",
      "[('country', 157), ('government', 144), ('family', 130), ('time', 129), ('year', 123), ('state', 101), ('life', 91), ('world', 86), ('part', 82), ('group', 82)]\n",
      "\n",
      "Most Common Adj\n",
      "[('many', 165), ('new', 148), ('other', 148), ('political', 95), ('american', 94), ('first', 93), ('last', 81), ('old', 78), ('-', 78), ('syrian', 73)]\n",
      "\n",
      "Most Common Verbs\n",
      "[('be', 341), ('have', 114), ('make', 80), ('take', 64), ('get', 63), ('do', 59), ('go', 52), ('help', 45), ('see', 41), ('give', 36)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre = 'NEWS'\n",
    "genre_years = sorted(data[data['genre']==genre]['year'].unique())\n",
    "get_most_common_POS(data, genre_years, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud\n",
    "for yr in years:\n",
    "    yearly = data[data['year']==yr]\n",
    "    words = yearly['normalized_words'].sum()\n",
    "    wc = wordcloud.WordCloud(background_color=\"white\", max_words=500, width= 1000, height = 1000, mode ='RGBA', scale=.5).generate(' '.join(data['normalized_words'].sum()))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    wc.to_file('wordcloud/data_{}.png'.format(yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "[(('unite', 'state'), 7199.697459742365), (('mr', 'macneil'), 5182.843636580887), (('saddam', 'hussein'), 5124.447968424104), (('mr', 'lehrer'), 4829.11002234646), (('saudi', 'arabia'), 4583.479938021688)]\n",
      "[(('unite', 'state', 'baker'), 11075.771911269172), (('ambassador', 'unite', 'state'), 11068.928726036236), (('unite', 'state', 'soviet'), 10892.836049392063), (('unite', 'state', 'government'), 10874.341495944747), (('unite', 'state', 'impose'), 10844.911026837526)]\n",
      "1991\n",
      "[(('unite', 'state'), 8589.98210113935), (('saddam', 'hussein'), 7472.851549691972), (('mr', 'macneil'), 5550.671758042515), (('mr', 'lehrer'), 5016.541191503217), (('soviet', 'union'), 4138.521090510931)]\n",
      "[(('unite', 'state', 'baker'), 13155.048049326459), (('president', 'unite', 'state'), 13047.280788762055), (('unite', 'state', 'america'), 12968.576770184256), (('ambassador', 'unite', 'state'), 12961.201633555607), (('unite', 'state', 'senate'), 12948.304813605911)]\n",
      "1992\n",
      "[(('unite', 'state'), 4038.452880980299), (('mr', 'macneil'), 3322.8011164045015), (('ms', 'woodruff'), 2809.1848425509306), (('peter', 'jennings'), 2765.143220706394), (('new', 'york'), 2649.600727757356)]\n",
      "[(('vatican', 'unite', 'state'), 6151.45220083735), (('president', 'unite', 'state'), 6124.457537960345), (('unite', 'state', 'america'), 6119.193306626022), (('asylum', 'unite', 'state'), 6112.647492593427), (('japan', 'unite', 'state'), 6105.4389108400355)]\n",
      "1993\n",
      "[(('unite', 'state'), 4260.093427476223), (('new', 'york'), 3147.018901289265), (('mr', 'macneil'), 2558.292523080376), (('unite', 'nation'), 2330.222911515005), (('mr', 'lehrer'), 2122.2735614798858)]\n",
      "[(('unite', 'state', 'official'), 6558.798315676144), (('unite', 'state', 'america'), 6460.703873282746), (('president', 'unite', 'state'), 6449.820717796123), (('unite', 'state', 'government'), 6449.329342135495), (('enter', 'unite', 'state'), 6444.671805393078)]\n",
      "1994\n",
      "[(('mr', 'lehrer'), 7353.770147091802), (('unite', 'state'), 7068.8926789601455), (('sam', 'donaldson'), 6405.791962714449), (('david', 'brinkley'), 5618.639314814278), (('mac', 'neil'), 5486.586355483177)]\n",
      "[(('mr', 'mac', 'neil'), 13372.053187126916), (('mr', 'lehrer', 'think'), 11579.212295527828), (('mr', 'lehrer', 'yes'), 11366.817903099545), (('jim', 'mr', 'lehrer'), 11345.458503364622), (('unite', 'state', 'department'), 11331.988910227854)]\n",
      "1995\n",
      "[(('ted', 'koppel'), 3509.373695820513), (('mr', 'lehrer'), 2606.7241434135185), (('unite', 'nation'), 2571.95888851649), (('unite', 'state'), 2338.2916744708973), (('new', 'york'), 2126.413630304061)]\n",
      "[(('ted', 'koppel', 'let'), 5468.600320849553), (('ted', 'koppel', 'voice'), 5413.649690843341), (('break', 'ted', 'koppel'), 5351.534631001678), (('ted', 'koppel', '-let'), 5345.907969735438), (('yes', 'ted', 'koppel'), 5331.939013184967)]\n",
      "1996\n",
      "[(('dept', 'fs'), 4287.494211532334), (('unite', 'state'), 3527.9909566044043), (('cokie', 'roberts'), 2747.4186194821177), (('jim', 'lehrer'), 2734.544481400253), (('sam', 'donaldson'), 2384.033037697133)]\n",
      "[(('nf', 'dept', 'fs'), 8494.020074609798), (('nwr', 'dept', 'fs'), 7858.141451937936), (('dept', 'fs', 'box'), 7707.678440058357), (('np', 'dept', 'fs'), 7013.626831184016), (('dept', 'fs', 'rt'), 6874.383240915199)]\n",
      "1997\n",
      "[(('elizabeth', 'farnswor'), 2765.215680620801), (('margaret', 'warner'), 2539.7523447507074), (('unite', 'state'), 2342.5521856807286), (('hong', 'kong'), 1771.3161025239822), (('paul', 'solman'), 1393.695190461161)]\n",
      "[(('elizabeth', 'farnswor', 'okay'), 4246.079708797098), (('thank', 'elizabeth', 'farnswor'), 4210.918261000934), (('recap', 'elizabeth', 'farnswor'), 4210.381891433902), (('elizabeth', 'farnswor', 'doug'), 4206.027213492175), (('elizabeth', 'farnswor', 'thank'), 4201.9499544914015)]\n",
      "1998\n",
      "[(('unite', 'state'), 2261.969441235115), (('jim', 'lehrer'), 2148.2715254007458), (('barbara', 'walters'), 2019.7416085919378), (('new', 'york'), 1800.931314325714), (('phil', 'ponce'), 1394.0418422386745)]\n",
      "[(('barbara', 'walters', 'voice'), 3609.9436997399207), (('counterpart', 'unite', 'state'), 3427.719482073616), (('immigrate', 'unite', 'state'), 3427.163182701137), (('switzerland', 'unite', 'state'), 3427.163182701137), (('union', 'unite', 'state'), 3422.8753628044105)]\n",
      "1999\n",
      "[(('unite', 'state'), 4323.728589586303), (('peter', 'jennings'), 3330.3944418195015), (('new', 'york'), 3261.3391194016185), (('mr', 'russert'), 2677.3809638284592), (('gov', 'bush'), 2388.083379659096)]\n",
      "[(('president', 'unite', 'state'), 6675.882560647182), (('unite', 'state', 'senate'), 6563.784756965479), (('unite', 'state', 'nato'), 6558.345413376292), (('unite', 'state', 'jimmy'), 6540.590530521527), (('unite', 'state', 'britain'), 6535.67264441271)]\n",
      "2000\n",
      "[(('unite', 'state'), 2388.8259342471247), (('video', 'clip'), 1976.1938177764584), (('new', 'york'), 1673.7748226925137), (('+', '+'), 1490.7669491946308), (('prime', 'minister'), 1355.7053219401266)]\n",
      "[(('+', '+', '+'), 4492.02094009854), (('begin', 'video', 'clip'), 3881.8596249963357), (('end', 'video', 'clip'), 3846.8049746257257), (('unite', 'state', 'high'), 3638.580255735752), (('cnn', 'unite', 'state'), 3636.242156555766)]\n",
      "2001\n",
      "[(('unite', 'state'), 3846.35252297361), (('bin', 'lade'), 3531.995746158046), (('new', 'york'), 3055.4954700898365), (('+', '+'), 2668.331307550984), (('northern', 'alliance'), 2047.9032453651353)]\n",
      "[(('+', '+', '+'), 8042.168291969198), (('osama', 'bin', 'lade'), 7387.319648378574), (('unite', 'state', 'america'), 5835.687643232386), (('attack', 'unite', 'state'), 5825.874351629108), (('nation', 'unite', 'state'), 5811.628546756716)]\n",
      "2002\n",
      "[(('unite', 'state'), 3618.557711956934), (('west', 'bank'), 2860.855606233732), (('+', '+'), 2853.583992022017), (('sex', 'nudity'), 2653.840501818144), (('min', 'sterritt'), 2524.8876112405756)]\n",
      "[(('+', '+', '+'), 8610.924846836331), (('unite', 'state', 'israel'), 5523.374660975188), (('president', 'unite', 'state'), 5499.1586917587865), (('asset', 'unite', 'state'), 5464.908106726978), (('state', 'unite', 'state'), 5462.268317987773)]\n",
      "2003\n",
      "[(('unite', 'state'), 3361.287890716951), (('min', 'sterritt'), 2994.37091786802), (('sex', 'nudity'), 2654.605540826378), (('unidentified', 'male'), 2286.3391059586124), (('new', 'york'), 1819.6150394316583)]\n",
      "[(('sex', 'nudity', 'scene'), 5252.076632159973), (('ambassador', 'unite', 'state'), 5093.827765669051), (('unite', 'state', 'australia'), 5086.071979804117), (('president', 'unite', 'state'), 5084.9878635493915), (('unite', 'state', 'britain'), 5082.497488660978)]\n",
      "2004\n",
      "[(('min', 'sterritt'), 3626.402300758521), (('unite', 'state'), 1949.4686009573134), (('sex', 'nudity'), 1707.3914274875965), (('new', 'york'), 1490.0164718632625), (('unidentified', 'male'), 1419.1702507800965)]\n",
      "[(('min', 'sterritt', 'documentary'), 5874.361216302116), (('min', 'sterritt', 'young'), 5538.187281123598), (('min', 'sterritt', 'remake'), 5512.618886675288), (('min', 'sterritt', 'fictionalize'), 5508.654191490938), (('moore', 'min', 'sterritt'), 5489.2339471142295)]\n",
      "2005\n",
      "[(('new', 'orleans'), 3379.7675273727236), (('unite', 'state'), 2419.0772856760414), (('kotb', 'voiceover'), 2112.4168204677735), (('unidentified', 'male'), 2109.7619182744634), (('new', 'york'), 1768.3899145611554)]\n",
      "[(('mayor', 'new', 'orleans'), 5253.614595885415), (('city', 'new', 'orleans'), 5231.3422176011745), (('new', 'orleans', 'time'), 5202.122400804199), (('new', 'orleans', 'city'), 5178.392074488935), (('new', 'orleans', 'louisiana'), 5144.511938359897)]\n",
      "2006\n",
      "[(('unite', 'state'), 3319.831624321015), (('north', 'korea'), 2880.7940514516768), (('san', 'francisco'), 2385.2158807385804), (('video', 'clip'), 2180.5987062212503), (('commercial', 'break'), 1871.55459641072)]\n",
      "[(('unite', 'state', 'condoleezza'), 5246.297944334848), (('unite', 'state', 'department'), 5239.973485115663), (('ambassador', 'unite', 'state'), 5082.452527049304), (('president', 'unite', 'state'), 5027.537075180043), (('unite', 'state', 'europe'), 5019.862857850091)]\n",
      "2007\n",
      "[(('unite', 'state'), 2169.108352610314), (('jim', 'lehrer'), 1319.8713217582642), (('new', 'york'), 1294.4178480478529), (('judy', 'woodruff'), 1237.459337580791), (('george', 'stephanopou'), 1101.8796652791425)]\n",
      "[(('unite', 'state', 'department'), 3455.2832790122657), (('president', 'unite', 'state'), 3400.613540854666), (('unite', 'state', 'israel'), 3391.628292998233), (('unite', 'state', 'senate'), 3321.6102786964157), (('immigrate', 'unite', 'state'), 3301.4854859715306)]\n",
      "2008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('new', 'york'), 1258.2633966990845), (('unite', 'state'), 1247.6715415649314), (('barack', 'obama'), 710.7214549763021), (('hillary', 'clinton'), 679.0065412618319), (('video', 'clip'), 610.2734742756992)]\n",
      "[(('new', 'york', 'city'), 2026.2782358261818), (('new', 'york', 'time'), 1988.9611565109817), (('envirosell', 'new', 'york'), 1948.6344010275116), (('new', 'york', 'state'), 1934.0946829286238), (('new', 'york', 'new'), 1918.1534597488114)]\n",
      "2009\n",
      "[(('unidentified', 'male'), 1536.1339905184982), (('jim', 'lehrer'), 1510.3308332477857), (('mankiewicz', 'voiceover'), 1380.2657822848687), (('unite', 'state'), 1097.6009806112102), (('amanpour-1voice', 'o'), 963.7690309676445)]\n",
      "[(('unidentified', 'male', 'translator'), 3062.3302760329925), (('break', 'jim', 'lehrer'), 2535.7659358821043), (('unidentified', 'male', 'yes'), 2462.2438331700037), (('yes', 'unidentified', 'male'), 2362.3551690150366), (('be', 'jim', 'lehrer'), 2359.9697691764427)]\n",
      "2010\n",
      "[(('e', 'mail'), 1638.4307997254743), (('jim', 'lehrer'), 1550.104621331647), (('new', 'york'), 1320.9807234990449), (('gwen', 'ifill'), 1203.8335702770355), (('unite', 'state'), 1167.6238561177624)]\n",
      "[(('port', 'au', 'prince'), 3321.3027692885535), (('tel', 'e', 'mail'), 2792.9528713362224), (('ma', 'e', 'mail'), 2741.3704635514946), (('cosecretary', 'e', 'mail'), 2592.6958314129524), (('fax', 'e', 'mail'), 2575.0909600927694)]\n",
      "2011\n",
      "[(('jeffrey', 'brown'), 2771.277740912567), (('judy', 'woodruff'), 2152.353588674151), (('gwen', 'ifill'), 1933.553551467969), (('unite', 'state'), 1470.341538487736), (('margaret', 'warner'), 1264.323731084904)]\n",
      "[(('be', 'jeffrey', 'brown'), 4247.589693597536), (('crosstalk', 'jeffrey', 'brown'), 4239.425069966942), (('jeffrey', 'brown', 'right'), 4232.1519940949), (('jeffrey', 'brown', 'yes'), 4225.036927723051), (('jeffrey', 'brown', 'newshour'), 4220.717553118032)]\n",
      "2012\n",
      "[(('judy', 'woodruff'), 4475.495798168895), (('jeffrey', 'brown'), 4379.4795333348675), (('ray', 'suarez'), 2368.613162567954), (('margaret', 'warner'), 1947.4049696444984), (('hari', 'sreenivasan'), 1781.3157299661648)]\n",
      "[(('be', 'judy', 'woodruff'), 6853.32946557576), (('thank', 'judy', 'woodruff'), 6835.159613764055), (('judy', 'woodruff', 'online'), 6808.0748163577555), (('judy', 'woodruff', 'newshour'), 6801.697275346537), (('judy', 'judy', 'woodruff'), 6784.801669220877)]\n",
      "2013\n",
      "[(('jeffrey', 'brown'), 4737.820689495818), (('judy', 'woodruff'), 4677.759333427152), (('unite', 'state'), 2820.069777775732), (('margaret', 'warner'), 2758.2600850356953), (('anthony', 'mason'), 2494.7024540831335)]\n",
      "[(('be', 'jeffrey', 'brown'), 7239.263092650839), (('jeffrey', 'brown', 'right'), 7235.554094268706), (('jeffrey', 'brown', 'be'), 7200.818426834733), (('thank', 'judy', 'woodruff'), 7193.34034317751), (('jeffrey', 'brown', 'okay'), 7191.1717059162165)]\n",
      "2014\n",
      "[(('judy', 'woodruff'), 6528.342921237328), (('terry', 'gross'), 4084.18041545874), (('unite', 'state'), 3970.4863718994447), (('gwen', 'ifill'), 3755.1569330524426), (('jeffrey', 'brown'), 2935.473414586916)]\n",
      "[(('thank', 'judy', 'woodruff'), 10023.776301808737), (('be', 'judy', 'woodruff'), 9939.5971248347), (('ifill', 'judy', 'woodruff'), 9858.06917551304), (('break', 'judy', 'woodruff'), 9849.962032515641), (('right', 'judy', 'woodruff'), 9846.727925993844)]\n",
      "2015\n",
      "[(('judy', 'woodruff'), 6619.150381508353), (('gwen', 'ifill'), 5604.424307622923), (('video', 'clip'), 4206.933549882992), (('unite', 'state'), 3936.428612981523), (('donald', 'trump'), 3771.660813591575)]\n",
      "[(('thank', 'judy', 'woodruff'), 10296.460706481586), (('be', 'judy', 'woodruff'), 10022.443644449904), (('judy', 'woodruff', 'quickly'), 10012.919120964703), (('pleasure', 'judy', 'woodruff'), 10012.126314530511), (('judy', 'woodruff', 'finally'), 10009.755185579696)]\n"
     ]
    }
   ],
   "source": [
    "# collocations\n",
    "for yr in years:\n",
    "    yearly = data[data['year']==yr]\n",
    "    words = yearly['normalized_words'].sum()\n",
    "    \n",
    "    data_bigrams = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    print(yr)\n",
    "    print(data_bigrams.score_ngrams(bigram_measures.likelihood_ratio)[:5])\n",
    "    \n",
    "    data_trigrams = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    print(data_trigrams.score_ngrams(trigram_measures.likelihood_ratio)[:5])\n",
    "    \n",
    "# other options include student_t, chi_sq, likelihood_ratio, pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pandas.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pandas.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_div_heatmap(corpora, fileids, diff_measure='KL'):\n",
    "    '''\n",
    "    Diff measure = KL, Chi2, KS or Wasserstein\n",
    "    '''\n",
    "    L = []\n",
    "    for p in corpora:\n",
    "        l = []\n",
    "        for q in corpora:\n",
    "            l.append(Divergence(p,q, difference = diff_measure))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHECK\n",
    "corpora = data['normalized_words'][:2].sum()\n",
    "# corpora = []\n",
    "# for index, row in data.iterrows():\n",
    "#     corpora.append(row['tokenized_words'])\n",
    "fileids = list(data['text_id'][:2])\n",
    "plot_div_heatmap(corpora, fileids,diff_measure='KL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCooccurrence(sentences, makeMatrix = False):\n",
    "    words = set()\n",
    "    for sent in sentences:\n",
    "        words |= set(sent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in sentences:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        return g\n",
    "\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)\n",
    "\n",
    "def posCooccurrence(sentences, *posType, makeMatrix = False):\n",
    "    words = set()\n",
    "    reducedSents = []\n",
    "    #Only using the first kind of POS for each word\n",
    "    wordsMap = {}\n",
    "    for sent in sentences:\n",
    "        s = [(w, t) for w, t in lucem_illud_2020.spacy_pos(sent) if t in posType]\n",
    "        for w, t in s:\n",
    "            if w not in wordsMap:\n",
    "                wordsMap[w] = t\n",
    "        reducedSent = [w for w, t in s]\n",
    "        words |= set(reducedSent)\n",
    "        reducedSents.append(reducedSent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in reducedSents:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        for w in g.nodes:\n",
    "            g.nodes[w]['bipartite'] = wordsMap[w]\n",
    "        return g\n",
    "\n",
    "def plot_word_graph(graph):\n",
    "    layout = nx.spring_layout(graph, weight='weight', iterations= 100)\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    nx.draw(graph, ax = ax, pos = layout, labels = {n:n for n in graph.nodes()},\n",
    "            width=.2, \n",
    "            alpha = .9, \n",
    "            node_size = 100,\n",
    "            node_color = \"xkcd:light red\",\n",
    "            edge_color='xkcd:black')\n",
    "\n",
    "def plot_word_centrality(g):\n",
    "    layout_nn = nx.spring_layout(g, weight='weight', iterations= 100)\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    centralities_nn = nx.eigenvector_centrality(g)\n",
    "    maxC = max(centralities_nn.items(), key = lambda x : x[1])[1]\n",
    "    #maxWeight = max((d['weight'] for n1, n2, d in g.edges(data = True)))\n",
    "    #minWeight = min((d['weight'] for n1, n2, d in g.edges(data = True)))\n",
    "    nx.draw(g, ax = ax, pos = layout_nn, labels = {n: n for n in g.nodes()},\n",
    "            #width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gNN.edges(data = True)], \n",
    "            alpha = .9, \n",
    "            node_color = [centralities_nn[n] / maxC for n in g.nodes],\n",
    "            node_size = [centralities_nn[n] / maxC * 100 for n in g.nodes],\n",
    "            font_size = 16,\n",
    "            font_color = 'xkcd:dark grey',\n",
    "            edge_color = 'xkcd:medium blue',\n",
    "            cmap = plt.get_cmap('plasma'),\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot word network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word network\n",
    "g = wordCooccurrence(data['normalized_sents'].sum())\n",
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_choice = 25\n",
    "# remove if less than 25\n",
    "g.remove_edges_from([(n1, n2) for n1, n2, d in g.edges(data = True) if d['weight'] <= weight_choice])\n",
    "#since we are changing the graph list() evaluates the isolates first\n",
    "g.remove_nodes_from(list(nx.isolates(g)))\n",
    "# keep just the giant connected component\n",
    "main_graph = max(connected_component_subgraphs(g), key=len)\n",
    "print(nx.info(main_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_graph(main_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sub-graph\n",
    "immigrant_neighbors = main_graph.neighbors('family')\n",
    "subgraph_immigrant = main_graph.subgraph(immigrant_neighbors)\n",
    "print(nx.info(subgraph_immigrant))\n",
    "\n",
    "plot_word_graph(subgraph_immigrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cliques\n",
    "print(', '.join(max(nx.clique.find_cliques(main_graph), key = lambda x: len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot word network by pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pos network\n",
    "gNV = posCooccurrence(data['normalized_sents'].sum(), 'NN', 'VB')\n",
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_threshold= 100\n",
    "weight_threshold = 2\n",
    "gNV.remove_nodes_from([n for n in gNV.nodes if len(set(gNV.neighbors(n))) <= degree_threshold]) \n",
    "print(nx.info(gNV))\n",
    "gNV.remove_edges_from([(n1, n2) for n1, n2, d in gNV.edges(data = True) if d['weight'] <= weight_threshold])\n",
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO CHECK\n",
    "plot_word_centrality(gNV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw egocentric network\n",
    "g_immigrant_NV = gNV.subgraph(['family'] + list(gNV.neighbors('family')))\n",
    "print(nx.info(g_immigrant_NV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_immigrant_NV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### centrality & global measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centralities\n",
    "\n",
    "centralities = nx.degree_centrality(main_graph)\n",
    "#centralities = nx.eigenvector_centrality(main_graph)\n",
    "#centralities = nx.closeness_centrality(main_graph)\n",
    "#centralities = nx.betweenness.betweenness_centrality(main_graph)\n",
    "plt.hist(list(centralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'degree'\n",
    "\n",
    "centrality_df = pandas.DataFrame.from_dict(centralities, orient='index', columns=[name])\n",
    "centrality_df.sort_values(by=name, ascending=False, inplace=True)\n",
    "#highest 10\n",
    "centrality_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global measures\n",
    "density = nx.density(main_graph)\n",
    "mean_degree_pernode = np.mean([v for w,v in nx.degree(main_graph)])\n",
    "diameter = nx.diameter(main_graph)\n",
    "print(\n",
    "\"The density of this graph is {}\\n\\\n",
    "Mean degree per node is {}\\n\\\n",
    "Diameter of graph is {}\".format(density, mean_degree_pernode, diameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF vectoriser\n",
    "data_vectoriser = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, max_features=1000, min_df=3, stop_words='english', norm='l2')\n",
    "dataVects = data_vectoriser.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_n(X, max=10):\n",
    "    clusters = []\n",
    "    s_avg = []\n",
    "    for i in range(2, max):\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=i, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        silhouette_avg = sklearn.metrics.silhouette_score(X, cluster_labels)\n",
    "        clusters.append(i)\n",
    "        s_avg.append(silhouette_avg)\n",
    "        print(\"For {} clusters, average silhouette score is {}\".format(i, silhouette_avg))\n",
    "    plt.plot(clusters, s_avg)\n",
    "    plt.show()\n",
    "\n",
    "X = dataVects.toarray()\n",
    "find_best_n(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = 5\n",
    "km = sklearn.cluster.KMeans(n_clusters=num_cluster, init='k-means++')\n",
    "km.fit(dataVects)\n",
    "data['kmeans_prediction'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(km, num_cluster, data_vectoriser):\n",
    "    terms = data_vectoriser.get_feature_names()\n",
    "    print(\"Top terms per cluster:\")\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    for i in range(num_cluster):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :20]:\n",
    "            print(' %s' % terms[ind])\n",
    "        print('\\n')\n",
    "\n",
    "get_top_words(km, num_cluster, data_vectoriser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSilhouette(n_clusters, X):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15,5))\n",
    "    \n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    clusterer = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    silhouette_avg = sklearn.metrics.silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = sklearn.metrics.silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        cmap = matplotlib.cm.get_cmap(\"nipy_spectral\")\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    cmap = matplotlib.cm.get_cmap(\"nipy_spectral\")\n",
    "    colors = cmap(float(i) / n_clusters)\n",
    "    PCA = sklearn.decomposition.PCA\n",
    "    pca = PCA(n_components = 2).fit(dataVects.toarray())\n",
    "    reduced_data = pca.transform(dataVects.toarray())\n",
    "    ax2.scatter(reduced_data[:, 0], reduced_data[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    projected_centers = pca.transform(centers)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(projected_centers[:, 0], projected_centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "    \n",
    "    for i, c in enumerate(projected_centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"PC 1\")    \n",
    "    ax2.set_ylabel(\"PC 2\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    print(\"For n_clusters = {}, The average silhouette_score is : {:.3f}\".format(n_clusters, silhouette_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSilhouette(num_cluster, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(dataVects, km, num_cluster, terms=data_vectoriser.get_feature_names()):\n",
    "    PCA = sklearn.decomposition.PCA\n",
    "    pca = PCA(n_components = 2).fit(dataVects.toarray())\n",
    "    reduced_data = pca.transform(dataVects.toarray())\n",
    "    # get distinguishing words to label\n",
    "    components = pca.components_\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    keyword_ids = list(set(order_centroids[:,:10].flatten())) #Get the ids of the most distinguishing words(features) from your kmeans model.\n",
    "    words = [terms[i] for i in keyword_ids]#Turn the ids into words.\n",
    "    x = components[:,keyword_ids][0,:] #Find the coordinates of those words in your biplot.\n",
    "    y = components[:,keyword_ids][1,:]\n",
    "    \n",
    "    cmap = matplotlib.cm.get_cmap(\"nipy_spectral\")\n",
    "    colors_p = [cmap(l/num_cluster) for l in km.labels_]\n",
    "    \n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_frame_on(False)\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], color=colors_p, alpha = 0.5)\n",
    "    for i, word in enumerate(words):\n",
    "        ax.annotate(word, (x[i],y[i]))\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title('Predicted Clusters\\n k = {}'.format(num_cluster))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(dataVects, km, num_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropMissing(wordLst, vocab):\n",
    "    return [w for w in wordLst if w in vocab]\n",
    "\n",
    "data['reduced_tokens'] = data['normalized_words'].apply(lambda x: dropMissing(x, data_vectoriser.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary\n",
    "dictionary = gensim.corpora.Dictionary(data['reduced_tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['reduced_tokens']]\n",
    "# serialize\n",
    "gensim.corpora.MmCorpus.serialize('data.mm', corpus)\n",
    "data_mm = gensim.corpora.MmCorpus('data.mm')\n",
    "# topic modelling\n",
    "topics=10\n",
    "data_lda = gensim.models.ldamodel.LdaModel(corpus=data_mm, id2word=dictionary, num_topics=topics, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics(data_lda, dictionary, data, title, n=10):\n",
    "    data_ldaDF = create_lda_df(data_lda, dictionary, data)\n",
    "    data_ldaDFV = data_ldaDF[:n][['topic_%d' %x for x in range(data_lda.num_topics)]]\n",
    "    data_ldaDFVisN = data_ldaDF[:10][['text_id']]\n",
    "    data_ldaDFVis = data_ldaDFV.values\n",
    "    data_ldaDFVisNames = data_ldaDFVisN.values\n",
    "    plot_topics_barchart(data_lda, data_ldaDFVis, data_ldaDFVisNames, title)\n",
    "    return data_ldaDF\n",
    "\n",
    "def create_lda_df(data_lda, dictionary, data):\n",
    "    # create a df of text and topics\n",
    "    data_ldaDF = pandas.DataFrame({\n",
    "                'text_id' : data['text_id'],\n",
    "                'title': data['title'],\n",
    "                'year': data['year'],\n",
    "                'topics' : [data_lda[dictionary.doc2bow(l)] for l in data['reduced_tokens']]\n",
    "        })\n",
    "\n",
    "    #Dict to temporally hold the probabilities\n",
    "    topicsProbDict = {i : [0] * len(data_ldaDF) for i in range(data_lda.num_topics)}\n",
    "\n",
    "    #Load them into the dict\n",
    "    for index, topicTuples in enumerate(data_ldaDF['topics']):\n",
    "        for topicNum, prob in topicTuples:\n",
    "            topicsProbDict[topicNum][index] = prob\n",
    "\n",
    "    #Update the DataFrame\n",
    "    for topicNum in range(data_lda.num_topics):\n",
    "        data_ldaDF['topic_{}'.format(topicNum)] = topicsProbDict[topicNum]\n",
    "    return data_ldaDF\n",
    "\n",
    "\n",
    "def plot_topics_barchart(senlda, ldaDFVis, ldaDFVisNames, title):\n",
    "    N = 10\n",
    "    ind = np.arange(N)\n",
    "    K = senlda.num_topics  # N documents, K topics\n",
    "    ind = np.arange(N)  # the x-axis locations for the novels\n",
    "    width = 0.5  # the width of the bars\n",
    "    plots = []\n",
    "    height_cumulative = np.zeros(N)\n",
    "\n",
    "    for k in range(K):\n",
    "        color = plt.cm.coolwarm(k/K, 1)\n",
    "        if k == 0:\n",
    "            p = plt.bar(ind, ldaDFVis[:, k], width, color=color)\n",
    "        else:\n",
    "            p = plt.bar(ind, ldaDFVis[:, k], width, bottom=height_cumulative, color=color)\n",
    "        height_cumulative += ldaDFVis[:, k]\n",
    "        plots.append(p)\n",
    "\n",
    "\n",
    "    plt.ylim((0, 1))  # proportions sum to 1, so the height of the stacked bars is 1\n",
    "    plt.ylabel('Topics')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xticks(ind+width/2, ldaDFVisNames, rotation='vertical')\n",
    "\n",
    "    plt.yticks(np.arange(0, 1, 10))\n",
    "    topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "    plt.legend([p[0] for p in plots], topic_labels, loc='center left', frameon=True,  bbox_to_anchor = (1, .5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"topics relevant to immigra in first 10 documents\"\n",
    "plot_topics(data_lda, dictionary, data, title, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lda.show_topic(5, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic topic modelling - long run time (overnight)\n",
    "#docs_per_year = list(data.groupby('year').size())\n",
    "#num_topics = 4\n",
    "#data_ldaseq = ldaseqmodel.LdaSeqModel(corpus=data_mm, id2word=dictionary, time_slice=docs_per_year, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ldaseq.save(\"data_ldaseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newspaper_ldaseq.print_topics(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic distribution divergence\n",
    "\n",
    "from gensim.matutils import kullback_leibler\n",
    "\n",
    "def plot_topic_divergence(data, years, num_topics=5):\n",
    "    topic_prob = get_topic_prob(data, years, num_topics)\n",
    "    L = []\n",
    "    for year_1 in topic_prob.keys():\n",
    "        p = topic_prob[year_1]\n",
    "        l = []\n",
    "        for year_2 in topic_prob.keys():\n",
    "            q = topic_prob[year_2]\n",
    "            l.append(kullback_leibler(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    div = pandas.DataFrame(M, columns = list(topic_prob.keys()), index = list(topic_prob.keys()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()\n",
    "    \n",
    "def get_topic_prob(data, years, num_topics=5):\n",
    "    topic_prob = {}\n",
    "    \n",
    "    byyear = get_topic_distribution(data, years, num_topics)\n",
    "    # Convert to probability\n",
    "    for yr in years:\n",
    "        j=0\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                index, prob = byyear[yr][j]\n",
    "            except IndexError:\n",
    "                index = False\n",
    "\n",
    "            if index == i:\n",
    "                j+=1\n",
    "                if yr in topic_prob:  \n",
    "                    topic_prob[yr].append(prob)\n",
    "                else:\n",
    "                    topic_prob[yr] = [prob]\n",
    "            else:\n",
    "                if yr in topic_prob:  \n",
    "                    topic_prob[yr].append(float(0))\n",
    "                else:\n",
    "                    topic_prob[yr] = [float(0)]\n",
    "    return topic_prob\n",
    "\n",
    "\n",
    "def get_topic_distribution(data, years, num_topics=5):\n",
    "    byyear = {}\n",
    "    # Get topic distribution for each year\n",
    "    for yr in years:\n",
    "        # get all text for each year\n",
    "        text_df = data[data['year']==yr][['text']]\n",
    "        text_df['tokenized_text'] = text_df['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "        text_df['normalized_tokens'] = text_df['tokenized_text'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))\n",
    "        # create dictionary\n",
    "        data_dictionary_byyear = gensim.corpora.Dictionary(text_df['normalized_tokens'])\n",
    "        data_corpus_byyear = [data_dictionary_byyear.doc2bow(text) for text in text_df['normalized_tokens']]\n",
    "        #lda\n",
    "        lda_byyear = gensim.models.ldamodel.LdaModel(corpus=data_corpus_byyear, id2word=data_dictionary_byyear, num_topics=num_topics, alpha='auto', eta='auto')\n",
    "\n",
    "        # place topic distribution in dictionary\n",
    "        all_text = []\n",
    "        for text in text_df['normalized_tokens']:\n",
    "            all_text.extend(text)\n",
    "        byyear[yr] = lda_byyear[data_dictionary_byyear.doc2bow(all_text)]\n",
    "\n",
    "        print('{} done'.format(yr))\n",
    "    return byyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(data['year'].unique())\n",
    "plot_topic_divergence(data, years, num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v = gensim.models.word2vec.Word2Vec(data['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_w2v.save('data_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v.most_similar(positive=['immigrants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(data_w2v, numWords):\n",
    "    targetWords = data_w2v.wv.index2word[:numWords]\n",
    "    wordsSubMatrix = []\n",
    "    for word in targetWords:\n",
    "        wordsSubMatrix.append(data_w2v[word])\n",
    "    wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "    pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "    reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "    #T-SNE is theoretically better, but you should experiment\n",
    "    tsneWords = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data)\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_frame_on(False)\n",
    "    plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "    for i, word in enumerate(targetWords):\n",
    "        ax.annotate(word, (tsneWords[:, 0][i],tsneWords[:, 1][i]), size =  20 * (numWords - i) / numWords)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(data_w2v, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_docs(data):\n",
    "    taggedDocs = []\n",
    "    for index, row in data.iterrows():\n",
    "        #Just doing a simple keyword assignment\n",
    "        docKeywords = [row['year']]\n",
    "        docKeywords.append(row['text_id'])\n",
    "        docKeywords.append(row['genre'])\n",
    "        docKeywords.append(row['title'])\n",
    "        docKeywords.append(row['word_count'])\n",
    "        taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "    return taggedDocs\n",
    "\n",
    "data['tagged_docs'] = tag_docs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d2v = gensim.models.doc2vec.Doc2Vec(data['tagged_docs'],size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_d2v.save(\"data_d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d2v.most_similar(positive = ['immigrants','illegal'], negative = ['legal'], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_by_year(data_d2v,years):\n",
    "    for yr in years:\n",
    "        print(yr)\n",
    "        print(data_d2v.most_similar( [ data_d2v.docvecs[yr] ], topn=5))\n",
    "        print()\n",
    "\n",
    "years = range(1990,2016)        \n",
    "most_similar_by_year(data_d2v, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = list(data['genre'].unique())       \n",
    "most_similar_by_year(data_d2v, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(data_d2v, targetDocs):\n",
    "    heatmapMatrixD = []\n",
    "    for tagOuter in targetDocs:\n",
    "        column = []\n",
    "        tagVec = data_d2v.docvecs[tagOuter].reshape(1, -1)\n",
    "        for tagInner in targetDocs:\n",
    "            column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, data_d2v.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "        heatmapMatrixD.append(column)\n",
    "    heatmapMatrixD = np.array(heatmapMatrixD)\n",
    "    fig, ax = plt.subplots()\n",
    "    hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "    cbar = plt.colorbar(hmap)\n",
    "\n",
    "    cbar.set_label('cosine similarity', rotation=270)\n",
    "    a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "    a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "    a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "    a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(data_d2v, list(data['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(data_d2v, list(data['genre'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)\n",
    "\n",
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rawEmbeddings, data_comparedEmbeddings = compareModels(data, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    print(word)\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)\n",
    "\n",
    "def plot_divergence(targetWord, comparedEmbeddings):\n",
    "    pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "    fig, ax = plt.subplots(figsize = (10, 7))\n",
    "    sns.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "    ax.set_xlabel(\"Starting year\")\n",
    "    ax.set_ylabel(\"Final year\")\n",
    "    ax.set_ylabel(\"Final year\")\n",
    "    ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('immigrants', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('immigration', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('mexico', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('border', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDivergence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDivergence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
    "\n",
    "data_wordDivergences = findMostDivergent(data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most divergence\n",
    "data_wordDivergences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least divergence\n",
    "data_wordDivergences[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1",
   "language": "python",
   "name": "3.8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
