{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim#For word2vec, etc\n",
    "from gensim.models import ldaseqmodel\n",
    "\n",
    "import lucem_illud_2020\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pandas.read_csv(\"/Users/rachelker/Documents/UChic MSCAPP/Curriculum/2019-20 Winter/Computational Content Analysis/Project/twitterpolicydiscourse/data/immigra_coca_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11743"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and normalize words\n",
    "data['tokenized_words'] = data['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "data['normalized_words'] = data['tokenized_words'].apply(lambda x: lucem_illud_2020.normalizeTokens(x, lemma=False))\n",
    "data['normalized_words_POS'] = [lucem_illud_2020.spacy_pos(t) for t in data['text']]\n",
    "\n",
    "\n",
    "# tokenize and normalize sentences\n",
    "data['tokenized_sents'] = data['text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "data['normalized_sents'] = data['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s, lemma=False) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>subgen</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_info</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>normalized_words</th>\n",
       "      <th>normalized_words_POS</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018849</td>\n",
       "      <td>\" Bums . \" That 's what Radio Havana called ...</td>\n",
       "      <td>2950</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>This is the land of opportunity.</td>\n",
       "      <td>Vol. 19 Issue 8, p98, 8p, 1 chart, 3c, 4bw\\r\\n</td>\n",
       "      <td>[Bums, That, 's, what, Radio, Havana, called, ...</td>\n",
       "      <td>[bums, radio, havana, called, cubans, fled, u....</td>\n",
       "      <td>[(  , _SP), (\", ``), (bums, NNS), (., .), (\", ...</td>\n",
       "      <td>[[Bums], [That, 's, what, Radio, Havana, calle...</td>\n",
       "      <td>[[bums], [radio, havana, called], [cubans, fle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018857</td>\n",
       "      <td>Section : SPENDING MONEY ranks the 15 greates...</td>\n",
       "      <td>3435</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>America's finest restaurant towns.</td>\n",
       "      <td>Vol. 19 Issue 7, p122, 13p, 2 charts, 2 illust...</td>\n",
       "      <td>[Section, SPENDING, MONEY, ranks, the, 15, gre...</td>\n",
       "      <td>[section, spending, money, ranks, greatest, ea...</td>\n",
       "      <td>[( , _SP), (section, NN), (:, :), (spending, V...</td>\n",
       "      <td>[[Section, SPENDING, MONEY, ranks, the, 15, gr...</td>\n",
       "      <td>[[section, spending, money, ranks, greatest, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018863</td>\n",
       "      <td>Section : WORK IN THE ' 90S The hottest jobs ...</td>\n",
       "      <td>5087</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>15 fast-track careers.</td>\n",
       "      <td>Vol. 19 Issue 6, p108, 12p, 1 chart, 7 illustr...</td>\n",
       "      <td>[Section, WORK, IN, THE, 90S, The, hottest, jo...</td>\n",
       "      <td>[section, work, 90s, hottest, jobs, decade, fa...</td>\n",
       "      <td>[( , _SP), (section, NN), (:, :), (work, NN), ...</td>\n",
       "      <td>[[Section, WORK, IN, THE, 90S], [The, hottest,...</td>\n",
       "      <td>[[section, work, 90s], [hottest, jobs, decade,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018864</td>\n",
       "      <td>Communism 's collapse could make the Old Wor...</td>\n",
       "      <td>2325</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>Euro-Strategy</td>\n",
       "      <td>Vol. 19 Issue 6, p128, 7p, 1 illustration, 3c\\r\\n</td>\n",
       "      <td>[Communism, 's, collapse, could, make, the, Ol...</td>\n",
       "      <td>[communism, collapse, old, world, site, entici...</td>\n",
       "      <td>[(  , _SP), (communism, NN), ('s, POS), (colla...</td>\n",
       "      <td>[[Communism, 's, collapse, could, make, the, O...</td>\n",
       "      <td>[[communism, collapse, old, world, site, entic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018979</td>\n",
       "      <td>Section : NEWS FROM MOTHER  THIS MAY SOUND li...</td>\n",
       "      <td>1375</td>\n",
       "      <td>1990</td>\n",
       "      <td>MAG</td>\n",
       "      <td>130.0</td>\n",
       "      <td>MotherEarth</td>\n",
       "      <td>Heating up.</td>\n",
       "      <td>p4, 3p, 1c, 2bw\\r\\n</td>\n",
       "      <td>[Section, NEWS, FROM, MOTHER, THIS, MAY, SOUND...</td>\n",
       "      <td>[section, news, mother, sound, like, clucking,...</td>\n",
       "      <td>[( , _SP), (section, NN), (:, :), (news, NN), ...</td>\n",
       "      <td>[[Section, NEWS, FROM, MOTHER], [THIS, MAY, SO...</td>\n",
       "      <td>[[section, news, mother], [sound, like, clucki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text  word_count  \\\n",
       "0  2018849    \" Bums . \" That 's what Radio Havana called ...        2950   \n",
       "1  2018857   Section : SPENDING MONEY ranks the 15 greates...        3435   \n",
       "2  2018863   Section : WORK IN THE ' 90S The hottest jobs ...        5087   \n",
       "3  2018864    Communism 's collapse could make the Old Wor...        2325   \n",
       "4  2018979   Section : NEWS FROM MOTHER  THIS MAY SOUND li...        1375   \n",
       "\n",
       "   year genre  subgen       source                               title  \\\n",
       "0  1990   MAG   124.0        Money    This is the land of opportunity.   \n",
       "1  1990   MAG   124.0        Money  America's finest restaurant towns.   \n",
       "2  1990   MAG   124.0        Money              15 fast-track careers.   \n",
       "3  1990   MAG   124.0        Money                       Euro-Strategy   \n",
       "4  1990   MAG   130.0  MotherEarth                         Heating up.   \n",
       "\n",
       "                                    publication_info  \\\n",
       "0     Vol. 19 Issue 8, p98, 8p, 1 chart, 3c, 4bw\\r\\n   \n",
       "1  Vol. 19 Issue 7, p122, 13p, 2 charts, 2 illust...   \n",
       "2  Vol. 19 Issue 6, p108, 12p, 1 chart, 7 illustr...   \n",
       "3  Vol. 19 Issue 6, p128, 7p, 1 illustration, 3c\\r\\n   \n",
       "4                                p4, 3p, 1c, 2bw\\r\\n   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [Bums, That, 's, what, Radio, Havana, called, ...   \n",
       "1  [Section, SPENDING, MONEY, ranks, the, 15, gre...   \n",
       "2  [Section, WORK, IN, THE, 90S, The, hottest, jo...   \n",
       "3  [Communism, 's, collapse, could, make, the, Ol...   \n",
       "4  [Section, NEWS, FROM, MOTHER, THIS, MAY, SOUND...   \n",
       "\n",
       "                                    normalized_words  \\\n",
       "0  [bums, radio, havana, called, cubans, fled, u....   \n",
       "1  [section, spending, money, ranks, greatest, ea...   \n",
       "2  [section, work, 90s, hottest, jobs, decade, fa...   \n",
       "3  [communism, collapse, old, world, site, entici...   \n",
       "4  [section, news, mother, sound, like, clucking,...   \n",
       "\n",
       "                                normalized_words_POS  \\\n",
       "0  [(  , _SP), (\", ``), (bums, NNS), (., .), (\", ...   \n",
       "1  [( , _SP), (section, NN), (:, :), (spending, V...   \n",
       "2  [( , _SP), (section, NN), (:, :), (work, NN), ...   \n",
       "3  [(  , _SP), (communism, NN), ('s, POS), (colla...   \n",
       "4  [( , _SP), (section, NN), (:, :), (news, NN), ...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [[Bums], [That, 's, what, Radio, Havana, calle...   \n",
       "1  [[Section, SPENDING, MONEY, ranks, the, 15, gr...   \n",
       "2  [[Section, WORK, IN, THE, 90S], [The, hottest,...   \n",
       "3  [[Communism, 's, collapse, could, make, the, O...   \n",
       "4  [[Section, NEWS, FROM, MOTHER], [THIS, MAY, SO...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [[bums], [radio, havana, called], [cubans, fle...  \n",
       "1  [[section, spending, money, ranks, greatest, e...  \n",
       "2  [[section, work, 90s], [hottest, jobs, decade,...  \n",
       "3  [[communism, collapse, old, world, site, entic...  \n",
       "4  [[section, news, mother], [sound, like, clucki...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word/Phrase Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get context\n",
    "post_text = nltk.Text(data['tokenized_words'].sum())\n",
    "index = nltk.text.ConcordanceIndex(post_text) \n",
    "index.print_concordance('immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.print_concordance('immigrants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.print_concordance('immigrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common context\n",
    "post_text.common_contexts(['immigrants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexical Dispersion Plot\n",
    "sns.reset_orig() #Seaborn messes with this plot, disabling it\n",
    "post_text.dispersion_plot(['immigration', 'immigrant', 'immigrants'])\n",
    "sns.set() #Re-enabling seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Distribution\n",
    "fdist = nltk.FreqDist([w for w in data['normalized_words'].sum()])\n",
    "# Frequency distribution condition on word lengths\n",
    "freqdist = nltk.ConditionalFreqDist(((len(w), w) for w in data['normalized_words'].sum() if fdist[w]>100))\n",
    "# frequency plot for words of n character\n",
    "n=5\n",
    "plt.title(\"Frequency plot of words with {} characters and have frequencies of more than 100 in the blog corpus\".format(n))\n",
    "freqdist[n].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common Noun\n",
    "freqdist_POStoWord = nltk.ConditionalFreqDist((p, w) for w, p in data['normalized_words_POS'].sum())\n",
    "freqdist_POStoWord['NN'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common Adj\n",
    "freqdist_POStoWord['JJ'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common Verbs\n",
    "freqdist_POStoWord['VB'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud\n",
    "wc = wordcloud.WordCloud(background_color=\"white\", max_words=500, width= 1000, height = 1000, mode ='RGBA', scale=.5).generate(' '.join(data['normalized_words'].sum()))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "#plt.savefig(\"wc.pdf\", format = 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collocations\n",
    "data_bigrams = nltk.collocations.BigramCollocationFinder.from_words(data['normalized_words'].sum())\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "data_bigrams.score_ngrams(bigram_measures.likelihood_ratio)[:20]\n",
    "# other options include student_t, chi_sq, likelihood_ratio, pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bigrams = nltk.collocations.TrigramCollocationFinder.from_words(data['normalized_words'].sum())\n",
    "bigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "data_bigrams.score_ngrams(bigram_measures.likelihood_ratio)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pandas.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pandas.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_div_heatmap(corpora, fileids, diff_measure='KL'):\n",
    "    '''\n",
    "    Diff measure = KL, Chi2, KS or Wasserstein\n",
    "    '''\n",
    "    L = []\n",
    "    for p in corpora:\n",
    "        l = []\n",
    "        for q in corpora:\n",
    "            l.append(Divergence(p,q, difference = diff_measure))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHECK\n",
    "corpora = data['normalized_words'][:2].sum()\n",
    "# corpora = []\n",
    "# for index, row in data.iterrows():\n",
    "#     corpora.append(row['tokenized_words'])\n",
    "fileids = list(data['text_id'][:2])\n",
    "plot_div_heatmap(corpora, fileids,diff_measure='KL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCooccurrence(sentences, makeMatrix = False):\n",
    "    words = set()\n",
    "    for sent in sentences:\n",
    "        words |= set(sent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in sentences:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        return g\n",
    "\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)\n",
    "\n",
    "def posCooccurrence(sentences, *posType, makeMatrix = False):\n",
    "    words = set()\n",
    "    reducedSents = []\n",
    "    #Only using the first kind of POS for each word\n",
    "    wordsMap = {}\n",
    "    for sent in sentences:\n",
    "        s = [(w, t) for w, t in lucem_illud_2020.spacy_pos(sent) if t in posType]\n",
    "        for w, t in s:\n",
    "            if w not in wordsMap:\n",
    "                wordsMap[w] = t\n",
    "        reducedSent = [w for w, t in s]\n",
    "        words |= set(reducedSent)\n",
    "        reducedSents.append(reducedSent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in reducedSents:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        for w in g.nodes:\n",
    "            g.nodes[w]['bipartite'] = wordsMap[w]\n",
    "        return g\n",
    "\n",
    "def plot_word_graph(graph):\n",
    "    layout = nx.spring_layout(graph, weight='weight', iterations= 100)\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    nx.draw(graph, ax = ax, pos = layout, labels = {n:n for n in graph.nodes()},\n",
    "            width=.2, \n",
    "            alpha = .9, \n",
    "            node_size = 100,\n",
    "            node_color = \"xkcd:light red\",\n",
    "            edge_color='xkcd:black')\n",
    "\n",
    "def plot_word_centrality(g):\n",
    "    layout_nn = nx.spring_layout(g, weight='weight', iterations= 100)\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    centralities_nn = nx.eigenvector_centrality(g)\n",
    "    maxC = max(centralities_nn.items(), key = lambda x : x[1])[1]\n",
    "    maxWeight = max((d['weight'] for n1, n2, d in g.edges(data = True)))\n",
    "    minWeight = min((d['weight'] for n1, n2, d in g.edges(data = True)))\n",
    "    nx.draw(g, ax = ax, pos = layout_nn, labels = {n: n for n in g.nodes()},\n",
    "            #width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gNN.edges(data = True)], \n",
    "            alpha = .9, \n",
    "            node_color = [centralities_nn[n] / maxC for n in g.nodes],\n",
    "            node_size = [centralities_nn[n] / maxC * 100 for n in g.nodes],\n",
    "            font_size = 16,\n",
    "            font_color = 'xkcd:dark grey',\n",
    "            edge_color = 'xkcd:medium blue',\n",
    "            cmap = plt.get_cmap('plasma'),\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot word network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word network\n",
    "g = wordCooccurrence(data['normalized_sents'].sum())\n",
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_choice = 25\n",
    "# remove if less than 25\n",
    "g.remove_edges_from([(n1, n2) for n1, n2, d in g.edges(data = True) if d['weight'] <= weight_choice])\n",
    "#since we are changing the graph list() evaluates the isolates first\n",
    "g.remove_nodes_from(list(nx.isolates(g)))\n",
    "# keep just the giant connected component\n",
    "main_graph = max(connected_component_subgraphs(g), key=len)\n",
    "print(nx.info(main_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_graph(main_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sub-graph\n",
    "immigrant_neighbors = main_graph.neighbors('family')\n",
    "subgraph_immigrant = main_graph.subgraph(immigrant_neighbors)\n",
    "print(nx.info(subgraph_immigrant))\n",
    "\n",
    "plot_word_graph(subgraph_immigrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cliques\n",
    "print(', '.join(max(nx.clique.find_cliques(main_graph), key = lambda x: len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot word network by pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pos network\n",
    "gNV = posCooccurrence(data['normalized_sents'].sum(), 'NN', 'VB')\n",
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_threshold= 100\n",
    "weight_threshold = 2\n",
    "gNV.remove_nodes_from([n for n in gNV.nodes if len(set(gNV.neighbors(n))) <= degree_threshold]) \n",
    "print(nx.info(gNV))\n",
    "gNV.remove_edges_from([(n1, n2) for n1, n2, d in gNV.edges(data = True) if d['weight'] <= weight_threshold])\n",
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO CHECK\n",
    "plot_word_centrality(gNV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw egocentric network\n",
    "g_immigrant_NV = gNV.subgraph(['family'] + list(gNV.neighbors('family')))\n",
    "print(nx.info(g_immigrant_NV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_immigrant_NV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### centrality & global measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centralities\n",
    "\n",
    "centralities = nx.degree_centrality(main_graph)\n",
    "#centralities = nx.eigenvector_centrality(main_graph)\n",
    "#centralities = nx.closeness_centrality(main_graph)\n",
    "#centralities = nx.betweenness.betweenness_centrality(main_graph)\n",
    "plt.hist(list(centralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'degree'\n",
    "\n",
    "centrality_df = pandas.DataFrame.from_dict(centralities, orient='index', columns=[name])\n",
    "centrality_df.sort_values(by=name, ascending=False, inplace=True)\n",
    "#highest 10\n",
    "centrality_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global measures\n",
    "density = nx.density(main_graph)\n",
    "mean_degree_pernode = np.mean([v for w,v in nx.degree(main_graph)])\n",
    "diameter = nx.diameter(main_graph)\n",
    "print(\n",
    "\"The density of this graph is {}\\n\\\n",
    "Mean degree per node is {}\\n\\\n",
    "Diameter of graph is {}\".format(density, mean_degree_pernode, diameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF vectoriser\n",
    "data_vectoriser = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, max_features=1000, min_df=3, stop_words='english', norm='l2')\n",
    "dataVects = data_vectoriser.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_n(X, max=10):\n",
    "    clusters = []\n",
    "    s_avg = []\n",
    "    for i in range(2, max):\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=i, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        silhouette_avg = sklearn.metrics.silhouette_score(X, cluster_labels)\n",
    "        clusters.append(i)\n",
    "        s_avg.append(silhouette_avg)\n",
    "        print(\"For {} clusters, average silhouette score is {}\".format(i, silhouette_avg))\n",
    "    plt.plot(clusters, s_avg)\n",
    "    plt.show()\n",
    "\n",
    "X = dataVects.toarray()\n",
    "find_best_n(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = 5\n",
    "km = sklearn.cluster.KMeans(n_clusters=num_cluster, init='k-means++')\n",
    "km.fit(dataVects)\n",
    "data['kmeans_prediction'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(km, num_cluster, data_vectoriser):\n",
    "    terms = data_vectoriser.get_feature_names()\n",
    "    print(\"Top terms per cluster:\")\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    for i in range(num_cluster):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :20]:\n",
    "            print(' %s' % terms[ind])\n",
    "        print('\\n')\n",
    "\n",
    "get_top_words(km, num_cluster, data_vectoriser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSilhouette(n_clusters, X):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15,5))\n",
    "    \n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    clusterer = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    silhouette_avg = sklearn.metrics.silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = sklearn.metrics.silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        cmap = matplotlib.cm.get_cmap(\"nipy_spectral\")\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    cmap = matplotlib.cm.get_cmap(\"nipy_spectral\")\n",
    "    colors = cmap(float(i) / n_clusters)\n",
    "    PCA = sklearn.decomposition.PCA\n",
    "    pca = PCA(n_components = 2).fit(dataVects.toarray())\n",
    "    reduced_data = pca.transform(dataVects.toarray())\n",
    "    ax2.scatter(reduced_data[:, 0], reduced_data[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    projected_centers = pca.transform(centers)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(projected_centers[:, 0], projected_centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "    \n",
    "    for i, c in enumerate(projected_centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"PC 1\")    \n",
    "    ax2.set_ylabel(\"PC 2\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    print(\"For n_clusters = {}, The average silhouette_score is : {:.3f}\".format(n_clusters, silhouette_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSilhouette(num_cluster, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(dataVects, km, num_cluster, terms=data_vectoriser.get_feature_names()):\n",
    "    PCA = sklearn.decomposition.PCA\n",
    "    pca = PCA(n_components = 2).fit(dataVects.toarray())\n",
    "    reduced_data = pca.transform(dataVects.toarray())\n",
    "    # get distinguishing words to label\n",
    "    components = pca.components_\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    keyword_ids = list(set(order_centroids[:,:10].flatten())) #Get the ids of the most distinguishing words(features) from your kmeans model.\n",
    "    words = [terms[i] for i in keyword_ids]#Turn the ids into words.\n",
    "    x = components[:,keyword_ids][0,:] #Find the coordinates of those words in your biplot.\n",
    "    y = components[:,keyword_ids][1,:]\n",
    "    \n",
    "    cmap = matplotlib.cm.get_cmap(\"nipy_spectral\")\n",
    "    colors_p = [cmap(l/num_cluster) for l in km.labels_]\n",
    "    \n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_frame_on(False)\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], color=colors_p, alpha = 0.5)\n",
    "    for i, word in enumerate(words):\n",
    "        ax.annotate(word, (x[i],y[i]))\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title('Predicted Clusters\\n k = {}'.format(num_cluster))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(dataVects, km, num_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropMissing(wordLst, vocab):\n",
    "    return [w for w in wordLst if w in vocab]\n",
    "\n",
    "data['reduced_tokens'] = data['normalized_words'].apply(lambda x: dropMissing(x, data_vectoriser.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary\n",
    "dictionary = gensim.corpora.Dictionary(data['reduced_tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['reduced_tokens']]\n",
    "# serialize\n",
    "gensim.corpora.MmCorpus.serialize('data.mm', corpus)\n",
    "data_mm = gensim.corpora.MmCorpus('data.mm')\n",
    "# topic modelling\n",
    "topics=10\n",
    "data_lda = gensim.models.ldamodel.LdaModel(corpus=data_mm, id2word=dictionary, num_topics=topics, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics(data_lda, dictionary, data, title, n=10):\n",
    "    data_ldaDF = create_lda_df(data_lda, dictionary, data)\n",
    "    data_ldaDFV = data_ldaDF[:n][['topic_%d' %x for x in range(data_lda.num_topics)]]\n",
    "    data_ldaDFVisN = data_ldaDF[:10][['text_id']]\n",
    "    data_ldaDFVis = data_ldaDFV.values\n",
    "    data_ldaDFVisNames = data_ldaDFVisN.values\n",
    "    plot_topics_barchart(data_lda, data_ldaDFVis, data_ldaDFVisNames, title)\n",
    "    return data_ldaDF\n",
    "\n",
    "def create_lda_df(data_lda, dictionary, data):\n",
    "    # create a df of text and topics\n",
    "    data_ldaDF = pandas.DataFrame({\n",
    "                'text_id' : data['text_id'],\n",
    "                'title': data['title'],\n",
    "                'year': data['year'],\n",
    "                'topics' : [data_lda[dictionary.doc2bow(l)] for l in data['reduced_tokens']]\n",
    "        })\n",
    "\n",
    "    #Dict to temporally hold the probabilities\n",
    "    topicsProbDict = {i : [0] * len(data_ldaDF) for i in range(data_lda.num_topics)}\n",
    "\n",
    "    #Load them into the dict\n",
    "    for index, topicTuples in enumerate(data_ldaDF['topics']):\n",
    "        for topicNum, prob in topicTuples:\n",
    "            topicsProbDict[topicNum][index] = prob\n",
    "\n",
    "    #Update the DataFrame\n",
    "    for topicNum in range(data_lda.num_topics):\n",
    "        data_ldaDF['topic_{}'.format(topicNum)] = topicsProbDict[topicNum]\n",
    "    return data_ldaDF\n",
    "\n",
    "\n",
    "def plot_topics_barchart(senlda, ldaDFVis, ldaDFVisNames, title):\n",
    "    N = 10\n",
    "    ind = np.arange(N)\n",
    "    K = senlda.num_topics  # N documents, K topics\n",
    "    ind = np.arange(N)  # the x-axis locations for the novels\n",
    "    width = 0.5  # the width of the bars\n",
    "    plots = []\n",
    "    height_cumulative = np.zeros(N)\n",
    "\n",
    "    for k in range(K):\n",
    "        color = plt.cm.coolwarm(k/K, 1)\n",
    "        if k == 0:\n",
    "            p = plt.bar(ind, ldaDFVis[:, k], width, color=color)\n",
    "        else:\n",
    "            p = plt.bar(ind, ldaDFVis[:, k], width, bottom=height_cumulative, color=color)\n",
    "        height_cumulative += ldaDFVis[:, k]\n",
    "        plots.append(p)\n",
    "\n",
    "\n",
    "    plt.ylim((0, 1))  # proportions sum to 1, so the height of the stacked bars is 1\n",
    "    plt.ylabel('Topics')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xticks(ind+width/2, ldaDFVisNames, rotation='vertical')\n",
    "\n",
    "    plt.yticks(np.arange(0, 1, 10))\n",
    "    topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "    plt.legend([p[0] for p in plots], topic_labels, loc='center left', frameon=True,  bbox_to_anchor = (1, .5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"topics relevant to immigra in first 10 documents\"\n",
    "plot_topics(data_lda, dictionary, data, title, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lda.show_topic(5, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic topic modelling - long run time (overnight)\n",
    "#docs_per_year = list(data.groupby('year').size())\n",
    "#num_topics = 4\n",
    "#data_ldaseq = ldaseqmodel.LdaSeqModel(corpus=data_mm, id2word=dictionary, time_slice=docs_per_year, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ldaseq.save(\"data_ldaseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newspaper_ldaseq.print_topics(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic distribution divergence\n",
    "\n",
    "from gensim.matutils import kullback_leibler\n",
    "\n",
    "def plot_topic_divergence(data, years, num_topics=5):\n",
    "    topic_prob = get_topic_prob(data, years, num_topics)\n",
    "    L = []\n",
    "    for year_1 in topic_prob.keys():\n",
    "        p = topic_prob[year_1]\n",
    "        l = []\n",
    "        for year_2 in topic_prob.keys():\n",
    "            q = topic_prob[year_2]\n",
    "            l.append(kullback_leibler(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    div = pandas.DataFrame(M, columns = list(topic_prob.keys()), index = list(topic_prob.keys()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()\n",
    "    \n",
    "def get_topic_prob(data, years, num_topics=5):\n",
    "    topic_prob = {}\n",
    "    \n",
    "    byyear = get_topic_distribution(data, years, num_topics)\n",
    "    # Convert to probability\n",
    "    for yr in years:\n",
    "        j=0\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                index, prob = byyear[yr][j]\n",
    "            except IndexError:\n",
    "                index = False\n",
    "\n",
    "            if index == i:\n",
    "                j+=1\n",
    "                if yr in topic_prob:  \n",
    "                    topic_prob[yr].append(prob)\n",
    "                else:\n",
    "                    topic_prob[yr] = [prob]\n",
    "            else:\n",
    "                if yr in topic_prob:  \n",
    "                    topic_prob[yr].append(float(0))\n",
    "                else:\n",
    "                    topic_prob[yr] = [float(0)]\n",
    "    return topic_prob\n",
    "\n",
    "\n",
    "def get_topic_distribution(data, years, num_topics=5):\n",
    "    byyear = {}\n",
    "    # Get topic distribution for each year\n",
    "    for yr in years:\n",
    "        # get all text for each year\n",
    "        text_df = data[data['year']==yr][['text']]\n",
    "        text_df['tokenized_text'] = text_df['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "        text_df['normalized_tokens'] = text_df['tokenized_text'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))\n",
    "        # create dictionary\n",
    "        data_dictionary_byyear = gensim.corpora.Dictionary(text_df['normalized_tokens'])\n",
    "        data_corpus_byyear = [data_dictionary_byyear.doc2bow(text) for text in text_df['normalized_tokens']]\n",
    "        #lda\n",
    "        lda_byyear = gensim.models.ldamodel.LdaModel(corpus=data_corpus_byyear, id2word=data_dictionary_byyear, num_topics=num_topics, alpha='auto', eta='auto')\n",
    "\n",
    "        # place topic distribution in dictionary\n",
    "        all_text = []\n",
    "        for text in text_df['normalized_tokens']:\n",
    "            all_text.extend(text)\n",
    "        byyear[yr] = lda_byyear[data_dictionary_byyear.doc2bow(all_text)]\n",
    "\n",
    "        print('{} done'.format(yr))\n",
    "    return byyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = data['year'].unique()\n",
    "plot_topic_divergence(data, years, num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v = gensim.models.word2vec.Word2Vec(data['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_w2v.save('data_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v.most_similar(positive=['immigrants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(data_w2v, numWords):\n",
    "    targetWords = data_w2v.wv.index2word[:numWords]\n",
    "    wordsSubMatrix = []\n",
    "    for word in targetWords:\n",
    "        wordsSubMatrix.append(data_w2v[word])\n",
    "    wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "    pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "    reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "    #T-SNE is theoretically better, but you should experiment\n",
    "    tsneWords = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data)\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_frame_on(False)\n",
    "    plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "    for i, word in enumerate(targetWords):\n",
    "        ax.annotate(word, (tsneWords[:, 0][i],tsneWords[:, 1][i]), size =  20 * (numWords - i) / numWords)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(data_w2v, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_docs(data):\n",
    "    taggedDocs = []\n",
    "    for index, row in data.iterrows():\n",
    "        #Just doing a simple keyword assignment\n",
    "        docKeywords = [row['year']]\n",
    "        docKeywords.append(row['text_id'])\n",
    "        docKeywords.append(row['genre'])\n",
    "        docKeywords.append(row['title'])\n",
    "        docKeywords.append(row['word_count'])\n",
    "        taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "    return taggedDocs\n",
    "\n",
    "data['tagged_docs'] = tag_docs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d2v = gensim.models.doc2vec.Doc2Vec(data['tagged_docs'],size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_d2v.save(\"data_d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d2v.most_similar(positive = ['immigrants','illegal'], negative = ['legal'], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_by_year(data_d2v,years):\n",
    "    for yr in years:\n",
    "        print(yr)\n",
    "        print(data_d2v.most_similar( [ data_d2v.docvecs[yr] ], topn=5))\n",
    "        print()\n",
    "\n",
    "years = range(1990,2016)        \n",
    "most_similar_by_year(data_d2v, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = list(data['genre'].unique())       \n",
    "most_similar_by_year(data_d2v, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(data_d2v, targetDocs):\n",
    "    heatmapMatrixD = []\n",
    "    for tagOuter in targetDocs:\n",
    "        column = []\n",
    "        tagVec = data_d2v.docvecs[tagOuter].reshape(1, -1)\n",
    "        for tagInner in targetDocs:\n",
    "            column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, data_d2v.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "        heatmapMatrixD.append(column)\n",
    "    heatmapMatrixD = np.array(heatmapMatrixD)\n",
    "    fig, ax = plt.subplots()\n",
    "    hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "    cbar = plt.colorbar(hmap)\n",
    "\n",
    "    cbar.set_label('cosine similarity', rotation=270)\n",
    "    a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "    a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "    a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "    a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(data_d2v, list(data['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(data_d2v, list(data['genre'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)\n",
    "\n",
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rawEmbeddings, data_comparedEmbeddings = compareModels(data, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    print(word)\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)\n",
    "\n",
    "def plot_divergence(targetWord, comparedEmbeddings):\n",
    "    pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "    fig, ax = plt.subplots(figsize = (10, 7))\n",
    "    sns.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "    ax.set_xlabel(\"Starting year\")\n",
    "    ax.set_ylabel(\"Final year\")\n",
    "    ax.set_ylabel(\"Final year\")\n",
    "    ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('immigrants', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('immigration', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('mexico', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence('border', data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDivergence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDivergence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
    "\n",
    "data_wordDivergences = findMostDivergent(data_comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most divergence\n",
    "data_wordDivergences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least divergence\n",
    "data_wordDivergences[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1",
   "language": "python",
   "name": "3.8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
